{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f761251-f5aa-48dd-ae57-4225a045031b",
   "metadata": {},
   "source": [
    "# Assignment 4 -  Recurrent Neural Networks(RNN) and English Text Character Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6186aaaf-4f3b-40da-976e-db55629f78a4",
   "metadata": {},
   "source": [
    "## Libraries and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cf43349-eb6f-4df8-bdf9-a97cf1533bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd64f725-3a21-4781-9436-ee357217733c",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a67508f-7c64-4e04-8302-71e1b8782cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_path = \"./data/goblet_book.txt\"\n",
    "with open(book_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    book_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0c6347c-e353-402f-9736-34824f11cabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HARRY POTTER AND THE GOBLET OF FIRE\n",
      "\n",
      "CHAPTER ONE - THE RIDDLE HOUSE\n",
      "\n",
      "\tThe villagers of Little Hangle\n"
     ]
    }
   ],
   "source": [
    "print(f\"{book_data[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d57a49a-6521-4517-9664-3f46becbe6c2",
   "metadata": {},
   "source": [
    "## Exercise 1 - Implement and train vanilla RNN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28f62f3-3913-469f-8f24-7ea1054aedfb",
   "metadata": {},
   "source": [
    "### 1.1 - Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f57ef89-f227-4fea-9e14-62c94868b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chars = list(set(book_data))\n",
    "char_to_id = {char: i for i, char in enumerate(unique_chars)}\n",
    "id_to_char = {i: char for i, char in enumerate(unique_chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "773dfc38-b6e5-4921-bdcc-f3b1898bcc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char -> id: {'(': 0, 'c': 1, ';': 2, '6': 3, 'f': 4, 'j': 5, 'X': 6, 'x': 7, '?': 8, 'u': 9, 'y': 10, '•': 11, 'G': 12, '-': 13, 'E': 14, 'Y': 15, '2': 16, 'D': 17, 'F': 18, '!': 19, '0': 20, ':': 21, 'p': 22, 'd': 23, 'o': 24, 'V': 25, ' ': 26, 'i': 27, '_': 28, 'h': 29, 'Z': 30, 'w': 31, 'k': 32, 'O': 33, '1': 34, '\\t': 35, 'C': 36, 'U': 37, 'R': 38, 'z': 39, 'e': 40, 'ü': 41, 'M': 42, 'n': 43, 'W': 44, 'a': 45, ',': 46, 'B': 47, 'r': 48, '9': 49, 's': 50, 't': 51, ')': 52, 'T': 53, 'J': 54, 'K': 55, 'l': 56, 'S': 57, '.': 58, '^': 59, '}': 60, 'q': 61, 'm': 62, 'L': 63, 'N': 64, 'P': 65, '7': 66, 'A': 67, '4': 68, '3': 69, 'v': 70, '/': 71, 'g': 72, '\\n': 73, 'I': 74, 'H': 75, '\"': 76, 'Q': 77, 'b': 78, \"'\": 79}\n"
     ]
    }
   ],
   "source": [
    "print(f\"char -> id: {char_to_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a05cf1e-936b-4db2-97e2-e539387324fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char <- id: {0: '(', 1: 'c', 2: ';', 3: '6', 4: 'f', 5: 'j', 6: 'X', 7: 'x', 8: '?', 9: 'u', 10: 'y', 11: '•', 12: 'G', 13: '-', 14: 'E', 15: 'Y', 16: '2', 17: 'D', 18: 'F', 19: '!', 20: '0', 21: ':', 22: 'p', 23: 'd', 24: 'o', 25: 'V', 26: ' ', 27: 'i', 28: '_', 29: 'h', 30: 'Z', 31: 'w', 32: 'k', 33: 'O', 34: '1', 35: '\\t', 36: 'C', 37: 'U', 38: 'R', 39: 'z', 40: 'e', 41: 'ü', 42: 'M', 43: 'n', 44: 'W', 45: 'a', 46: ',', 47: 'B', 48: 'r', 49: '9', 50: 's', 51: 't', 52: ')', 53: 'T', 54: 'J', 55: 'K', 56: 'l', 57: 'S', 58: '.', 59: '^', 60: '}', 61: 'q', 62: 'm', 63: 'L', 64: 'N', 65: 'P', 66: '7', 67: 'A', 68: '4', 69: '3', 70: 'v', 71: '/', 72: 'g', 73: '\\n', 74: 'I', 75: 'H', 76: '\"', 77: 'Q', 78: 'b', 79: \"'\"}\n"
     ]
    }
   ],
   "source": [
    "print(f\"char <- id: {id_to_char}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23858b1b-8409-40f0-af4f-c5f371d59821",
   "metadata": {},
   "source": [
    "### 1.2 - Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2556ed6a-c317-4482-8889-62668e13244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_network(hidden_dim, output_dim, rng):\n",
    "    init_net = {}\n",
    "    init_net[\"b\"] = np.zeros((hidden_dim, 1))\n",
    "    init_net[\"c\"] = np.zeros((output_dim, 1))\n",
    "\n",
    "    init_net[\"U\"] = (1 / np.sqrt(2 * output_dim)) * rng.standard_normal(\n",
    "        size=(hidden_dim, output_dim)  # mxk\n",
    "    )\n",
    "    init_net[\"W\"] = (1 / np.sqrt(2 * hidden_dim)) * rng.standard_normal(\n",
    "        size=(hidden_dim, hidden_dim)  # mxm\n",
    "    )\n",
    "    init_net[\"V\"] = (1 / np.sqrt(hidden_dim)) * rng.standard_normal(\n",
    "        size=(output_dim, hidden_dim)  # kxm\n",
    "    )\n",
    "    return init_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fff37d38-fd32-4760-ba62-a480cf591f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_network():\n",
    "    eta = 0.001\n",
    "    seq_length = 25\n",
    "    m = 100\n",
    "    K = len(unique_chars)\n",
    "    seed = 42\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    init_net = initialize_network(m, K, rng)\n",
    "    return init_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ca0b2a5-ff29-4b35-ab31-f58c17ef737b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize network...\n",
      "init_network[U] shape -> (100, 80)\n",
      "init_network[W] shape -> (100, 100)\n",
      "init_network[V] shape -> (80, 100)\n",
      "init_network[b] shape -> (100, 1)\n",
      "init_network[c] shape -> (80, 1)\n"
     ]
    }
   ],
   "source": [
    "debug_init_net = init_network()\n",
    "print(\"initialize network...\")\n",
    "print(f\"init_network[U] shape -> {debug_init_net['U'].shape}\")\n",
    "print(f\"init_network[W] shape -> {debug_init_net['W'].shape}\")\n",
    "print(f\"init_network[V] shape -> {debug_init_net['V'].shape}\")\n",
    "print(f\"init_network[b] shape -> {debug_init_net['b'].shape}\")\n",
    "print(f\"init_network[c] shape -> {debug_init_net['c'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5ae111-efab-4ca5-b269-37a774e77d01",
   "metadata": {},
   "source": [
    "### 1.3 - Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8066cc65-b94d-4dce-9930-6783912819d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Activation functions.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# softmax function - not numerically stable according to stanford cs231n notes\n",
    "def softmax(s):\n",
    "    return np.exp(s) / np.sum(np.exp(s), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "776fb8e4-35fe-4e82-94ac-4d41cd807c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_text(network, h0, x0, n, rng):\n",
    "    K = network[\"V\"].shape[0]\n",
    "    Y = np.zeros((K, n))\n",
    "\n",
    "    h = h0.copy()\n",
    "    x = x0.copy()\n",
    "\n",
    "    for t in range(n):\n",
    "        a = network[\"W\"] @ h + network[\"U\"] @ x + network[\"b\"]\n",
    "        h = np.tanh(a)\n",
    "        o = network[\"V\"] @ h + network[\"c\"]\n",
    "        p = softmax(o)\n",
    "\n",
    "        cp = np.cumsum(p, axis=0)\n",
    "        a = rng.uniform(size=1)\n",
    "        ii = np.argmax(cp - a > 0)\n",
    "\n",
    "        Y[ii, t] = 1\n",
    "\n",
    "        x = np.zeros((K, 1))\n",
    "        x[ii, 0] = 1\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8cda593-d645-4ed3-890b-d6f23e8f89f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_chars(chars, char_to_id, K):\n",
    "    X = np.zeros((K, len(chars)))\n",
    "    for i, char in enumerate(chars):\n",
    "        if char in char_to_id:\n",
    "            X[char_to_id[char], i] = 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64881c9e-a380-48b6-9f32-74144fdd7e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_chars(Y, ind_to_char):\n",
    "    indices = np.argmax(Y, axis=0)\n",
    "    text = \"\".join([ind_to_char[index] for index in indices])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cde4847-66fc-437f-8bba-62d45f42eaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(network, X, Y, h0):\n",
    "    K, seq_length = X.shape\n",
    "    m = network[\"W\"].shape[0]\n",
    "\n",
    "    h = np.zeros((m, seq_length + 1))\n",
    "    p = np.zeros((K, seq_length))\n",
    "\n",
    "    h[:, 0:1] = h0\n",
    "    loss = 0\n",
    "\n",
    "    for t in range(seq_length):\n",
    "        x_t = X[:, t : t + 1]\n",
    "        h_prev = h[:, t : t + 1]\n",
    "        a_t = network[\"W\"] @ h_prev + network[\"U\"] @ x_t + network[\"b\"]\n",
    "        h[:, t + 1 : t + 2] = np.tanh(a_t)\n",
    "        o_t = network[\"V\"] @ h[:, t + 1 : t + 2] + network[\"c\"]\n",
    "        p_t = softmax(o_t)\n",
    "        p[:, t : t + 1] = p_t\n",
    "\n",
    "        y_t = Y[:, t : t + 1]\n",
    "        loss += -np.sum(y_t * np.log(p_t + 1e-20))\n",
    "\n",
    "    loss = loss / seq_length\n",
    "\n",
    "    return loss, h, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18609437-260f-4cb0-aee2-332134c1e97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(network, X, Y, h, p):\n",
    "    K, seq_length = X.shape\n",
    "    m = network[\"W\"].shape[0]\n",
    "    grads = {}\n",
    "    grads[\"dU\"] = np.zeros_like(network[\"U\"])  # m × K\n",
    "    grads[\"dW\"] = np.zeros_like(network[\"W\"])  # m × m\n",
    "    grads[\"dV\"] = np.zeros_like(network[\"V\"])  # K × m\n",
    "    grads[\"db\"] = np.zeros_like(network[\"b\"])  # m × 1\n",
    "    grads[\"dc\"] = np.zeros_like(network[\"c\"])  # K × 1\n",
    "\n",
    "    # initialize gradient for next time step\n",
    "    dh_next = np.zeros((m, 1))\n",
    "\n",
    "    for t in reversed(range(seq_length)):\n",
    "        y_t = Y[:, t : t + 1]\n",
    "        p_t = p[:, t : t + 1]\n",
    "        h_t = h[:, t + 1 : t + 2]\n",
    "        h_prev = h[:, t : t + 1]\n",
    "        x_t = X[:, t : t + 1]\n",
    "\n",
    "        # gradient of output\n",
    "        d_out = p_t - y_t\n",
    "\n",
    "        # gradient of v\n",
    "        grads[\"dV\"] += d_out @ h_t.T\n",
    "\n",
    "        # gradient of c\n",
    "        grads[\"dc\"] += d_out\n",
    "\n",
    "        # gradient of hidden\n",
    "        d_h = network[\"V\"].T @ d_out + dh_next\n",
    "\n",
    "        # gradient of a\n",
    "        d_a = d_h * (1 - h_t**2)\n",
    "\n",
    "        # gradient of W\n",
    "        grads[\"dW\"] += d_a @ h_prev.T\n",
    "\n",
    "        # gradient of U\n",
    "        grads[\"dU\"] += d_a @ x_t.T\n",
    "\n",
    "        # gradient of B\n",
    "        grads[\"db\"] += d_a\n",
    "\n",
    "        # gradient for next iteration\n",
    "        dh_next = network[\"W\"].T @ d_a\n",
    "\n",
    "    for key in grads:\n",
    "        grads[key] /= seq_length\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8982125e-24cb-4587-8b29-0a1c0044facd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assumes X has size d x tau, h0 has size m x 1, etc\n",
    "def ComputeGradsWithTorch(X, y, h0, RNN):\n",
    "    tau = X.shape[1]\n",
    "\n",
    "    Xt = torch.from_numpy(X)\n",
    "    ht = torch.from_numpy(h0)\n",
    "\n",
    "    torch_network = {}\n",
    "    for kk in RNN.keys():\n",
    "        torch_network[kk] = torch.tensor(RNN[kk], requires_grad=True)\n",
    "\n",
    "    ## give informative names to these torch classes\n",
    "    apply_tanh = torch.nn.Tanh()\n",
    "    apply_softmax = torch.nn.Softmax(dim=0)\n",
    "\n",
    "    # create an empty tensor to store the hidden vector at each timestep\n",
    "    Hs = torch.empty(h0.shape[0], X.shape[1], dtype=torch.float64)\n",
    "\n",
    "    hprev = ht\n",
    "    for t in range(tau):\n",
    "        #### BEGIN your code ######\n",
    "\n",
    "        # Code to apply the RNN to hprev and Xt[:, t:t+1] to compute the hidden scores \"Hs\" at timestep t\n",
    "        x_t = Xt[:, t : t + 1]  # kx1\n",
    "\n",
    "        # (ie equations (1,2) in the assignment instructions)\n",
    "        a_t = (\n",
    "            torch.matmul(torch_network[\"W\"], hprev)\n",
    "            + torch.matmul(torch_network[\"U\"], x_t)\n",
    "            + torch_network[\"b\"]\n",
    "        )\n",
    "\n",
    "        h_t = apply_tanh(a_t)\n",
    "\n",
    "        # Store results in Hs\n",
    "        Hs[:, t : t + 1] = h_t\n",
    "\n",
    "        # Don't forget to update hprev!\n",
    "        hprev = h_t\n",
    "\n",
    "        #### END of your code ######\n",
    "\n",
    "    Os = torch.matmul(torch_network[\"V\"], Hs) + torch_network[\"c\"]\n",
    "    P = apply_softmax(Os)\n",
    "\n",
    "    # compute the loss\n",
    "\n",
    "    loss = torch.mean(-torch.log(P[y, np.arange(tau)]))\n",
    "\n",
    "    # compute the backward pass relative to the loss and the named parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # extract the computed gradients and make them numpy arrays\n",
    "    grads = {}\n",
    "    for kk in RNN.keys():\n",
    "        grads[kk] = torch_network[kk].grad.numpy()\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70721ae8-a359-4a1d-9302-9e9a1b260540",
   "metadata": {},
   "source": [
    "### 1.4 - Neural Networks go brrrr.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8675519e-ffb2-4d6c-9f14-afa29a51d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_network():\n",
    "    rng = np.random.default_rng(42)\n",
    "    seq_length = 25\n",
    "    m = 10\n",
    "    X_chars = book_data[0:seq_length]\n",
    "    Y_chars = book_data[1 : seq_length + 1]\n",
    "    K = len(unique_chars)\n",
    "    X = encode_chars(X_chars, char_to_id, K)\n",
    "    Y = encode_chars(Y_chars, char_to_id, K)\n",
    "    h0 = np.zeros((m, 1))\n",
    "\n",
    "    init_net = initialize_network(m, K, rng)\n",
    "    loss, h, p = forward(init_net, X, Y, h0)\n",
    "    print(\n",
    "        f\"forward pass -> Loss: {loss:.4f} | h_shape: {h.shape} | p_shape: {p.shape}\"\n",
    "    )\n",
    "\n",
    "    y_indices = np.argmax(encode_chars(Y_chars, char_to_id, K), axis=0)\n",
    "    torch_grads = ComputeGradsWithTorch(X, y_indices, h0, init_net)\n",
    "    grads = backward(init_net, X, Y, h, p)\n",
    "\n",
    "    return torch_grads, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc01f7ac-e4a8-4a1b-a494-de2b9de5d31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolute_error_check():\n",
    "    eps = 1e-6\n",
    "    torch_grads, my_grads = small_network()\n",
    "    abs_error_U = np.abs(my_grads[\"dU\"] - torch_grads[\"U\"])\n",
    "    abs_error_W = np.abs(my_grads[\"dW\"] - torch_grads[\"W\"])\n",
    "    abs_error_V = np.abs(my_grads[\"dV\"] - torch_grads[\"V\"])\n",
    "    abs_error_b = np.abs(my_grads[\"db\"] - torch_grads[\"b\"])\n",
    "    abs_error_c = np.abs(my_grads[\"dc\"] - torch_grads[\"c\"])\n",
    "    print(f\"U gradient check -> {np.all(abs_error_U < eps)}\")\n",
    "    print(f\"W gradient check -> {np.all(abs_error_W < eps)}\")\n",
    "    print(f\"V gradient check -> {np.all(abs_error_V < eps)}\")\n",
    "    print(f\"b gradient check -> {np.all(abs_error_b < eps)}\")\n",
    "    print(f\"c gradient check -> {np.all(abs_error_c < eps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "481ab120-d580-4acc-ad78-5408023e2c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward pass -> Loss: 4.3681 | h_shape: (10, 26) | p_shape: (80, 25)\n",
      "U gradient check -> True\n",
      "W gradient check -> True\n",
      "V gradient check -> True\n",
      "b gradient check -> True\n",
      "c gradient check -> True\n"
     ]
    }
   ],
   "source": [
    "absolute_error_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df5f2852-028f-4b2e-b840-10ca71108fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss_history, iter_history, filename=None):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(iter_history, loss_history)\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Smooth Loss\")\n",
    "    plt.title(\"Smooth Loss - 4 Epochs\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    if filename:\n",
    "        plt.savefig(f\"{filename}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04a80ee6-0a55-4ef2-a9fe-9b350b2f2745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_text_history(text_samples, filename=\"text_evolution.txt\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for sample in text_samples:\n",
    "            f.write(\n",
    "                f\"Iteration: {sample['iteration']} | Smooth Loss: {sample['loss']:.4f}\\n\"\n",
    "            )\n",
    "            f.write(f\"{sample['text']}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2c1258b-bb21-4e84-809f-f7ec94a87515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_final_text(network, char_to_id, id_to_char, rng, length=1000):\n",
    "    K = len(char_to_id)\n",
    "    hidden_dim = network[\"W\"].shape[0]\n",
    "    h0 = np.zeros((hidden_dim, 1))\n",
    "    x0 = np.zeros((K, 1))\n",
    "    x0[0, 0] = 1\n",
    "    Y_synth = synthesize_text(network, h0, x0, length, rng)\n",
    "    return decode_chars(Y_synth, id_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4753cfc0-f90e-4068-bb23-6987eeba9bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(\n",
    "    book_data,\n",
    "    char_to_id,\n",
    "    id_to_char,\n",
    "    network,\n",
    "    rng,\n",
    "    eta=0.001,\n",
    "    seq_length=25,\n",
    "    num_epochs=2,\n",
    "):\n",
    "    adam_params = {}\n",
    "    for key in network.keys():\n",
    "        adam_params[key] = {\n",
    "            \"m\": np.zeros_like(network[key]),\n",
    "            \"v\": np.zeros_like(network[key]),\n",
    "            \"t\": 0,\n",
    "        }\n",
    "\n",
    "    # adam hyperparameters\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 1e-8\n",
    "\n",
    "    e = 0\n",
    "    smooth_loss = None\n",
    "    hprev = np.zeros((network[\"W\"].shape[0], 1))\n",
    "    update = 0\n",
    "    K = len(unique_chars)\n",
    "\n",
    "    loss_history = []\n",
    "    iter_history = []\n",
    "    text_samples = []\n",
    "\n",
    "    best_smooth_loss = float(\"inf\")\n",
    "    best_network = None\n",
    "    best_update = 0\n",
    "\n",
    "    print(\"TRAIN THE RNN..............\")\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch: {epoch + 1}\")\n",
    "        while e <= len(book_data) - seq_length - 1:\n",
    "            X_chars = book_data[e : e + seq_length]\n",
    "            Y_chars = book_data[e + 1 : e + seq_length + 1]\n",
    "\n",
    "            X = encode_chars(X_chars, char_to_id, K)\n",
    "            Y = encode_chars(Y_chars, char_to_id, K)\n",
    "\n",
    "            loss, h, p = forward(network, X, Y, hprev)\n",
    "\n",
    "            if smooth_loss is None:\n",
    "                smooth_loss = loss\n",
    "            else:\n",
    "                smooth_loss = 0.999 * smooth_loss + 0.001 * loss\n",
    "\n",
    "            if smooth_loss < best_smooth_loss:\n",
    "                best_smooth_loss = smooth_loss\n",
    "                best_network = copy.deepcopy(network)\n",
    "                best_update = update\n",
    "                print(\n",
    "                    f\"Best model at iteration: {update} | smooth loss: {smooth_loss:.5f}\"\n",
    "                )\n",
    "\n",
    "            grads = backward(network, X, Y, h, p)\n",
    "\n",
    "            for key in network.keys():\n",
    "                adam_params[key][\"t\"] += 1\n",
    "                t = adam_params[key][\"t\"]\n",
    "                adam_params[key][\"m\"] = (\n",
    "                    beta1 * adam_params[key][\"m\"]\n",
    "                    + (1 - beta1) * grads[\"d\" + key]\n",
    "                )\n",
    "                adam_params[key][\"v\"] = beta2 * adam_params[key][\"v\"] + (\n",
    "                    1 - beta2\n",
    "                ) * (grads[\"d\" + key] ** 2)\n",
    "                m_hat = adam_params[key][\"m\"] / (1 - beta1**t)\n",
    "                v_hat = adam_params[key][\"v\"] / (1 - beta2**t)\n",
    "                network[key] = network[key] - eta * m_hat / (\n",
    "                    np.sqrt(v_hat) + epsilon\n",
    "                )\n",
    "\n",
    "            hprev = h[:, -1:]\n",
    "\n",
    "            if update % 100 == 0:\n",
    "                loss_history.append(smooth_loss)\n",
    "                iter_history.append(update)\n",
    "\n",
    "            if update == 0 or update % 1000 == 0:\n",
    "                print(f\"iter = {update}, smooth loss={smooth_loss}\")\n",
    "                Y_synth = synthesize_text(network, hprev, X[:, 0:1], 200, rng)\n",
    "                generated_text = decode_chars(Y_synth, id_to_char)\n",
    "                text_samples.append(\n",
    "                    {\n",
    "                        \"iteration\": update,\n",
    "                        \"text\": generated_text,\n",
    "                        \"loss\": smooth_loss,\n",
    "                    }\n",
    "                )\n",
    "                print(generated_text)\n",
    "                print()\n",
    "\n",
    "            e += seq_length\n",
    "            update += 1\n",
    "\n",
    "        print(f\"Completed epoch {epoch + 1}\")\n",
    "        e = 0\n",
    "        hprev = np.zeros((network[\"W\"].shape[0], 1))\n",
    "\n",
    "    print(\n",
    "        f\"\\nBest model achieved smooth loss: {best_smooth_loss:.5f} at iteration: {best_update}\"\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        network,\n",
    "        smooth_loss,\n",
    "        loss_history,\n",
    "        iter_history,\n",
    "        text_samples,\n",
    "        best_network,\n",
    "        best_smooth_loss,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6abfb3e8-60bc-45f1-ab9c-72d1c33e36bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN THE RNN..............\n",
      "Epoch: 1\n",
      "Best model at iteration: 0 | smooth loss: 4.37449\n",
      "iter = 0, smooth loss=4.374490016935686\n",
      "/M-/.RCJ 'mtJ'(cmDKüVFm-G\tut'^Bw'vPFyE0XvEF/QFpLQt1Tm?\t4lG}T:iV'Y7Mvb\"4k:Vti1Xq?iBM/TLoy^•7Hp}1oOt'ol !sqSH?ndPfEm_J'UMb^YLemBFN(/PQ61Eh_r1QpkKpSSSJP'fzP(n)YwmjyPNBO?N!\n",
      "qXt\n",
      "3r-J)B:gzd!pCFüYp?ikw.RK;\tx\n",
      "\n",
      "Best model at iteration: 1 | smooth loss: 4.37448\n",
      "Best model at iteration: 2 | smooth loss: 4.37447\n",
      "Best model at iteration: 5 | smooth loss: 4.37446\n",
      "Best model at iteration: 6 | smooth loss: 4.37445\n",
      "Best model at iteration: 7 | smooth loss: 4.37439\n",
      "Best model at iteration: 8 | smooth loss: 4.37427\n",
      "Best model at iteration: 9 | smooth loss: 4.37415\n",
      "Best model at iteration: 10 | smooth loss: 4.37391\n",
      "Best model at iteration: 11 | smooth loss: 4.37364\n",
      "Best model at iteration: 12 | smooth loss: 4.37339\n",
      "Best model at iteration: 13 | smooth loss: 4.37306\n",
      "Best model at iteration: 14 | smooth loss: 4.37241\n",
      "Best model at iteration: 15 | smooth loss: 4.37200\n",
      "Best model at iteration: 16 | smooth loss: 4.37131\n",
      "Best model at iteration: 17 | smooth loss: 4.37063\n",
      "Best model at iteration: 18 | smooth loss: 4.36963\n",
      "Best model at iteration: 19 | smooth loss: 4.36901\n",
      "Best model at iteration: 20 | smooth loss: 4.36852\n",
      "Best model at iteration: 21 | smooth loss: 4.36746\n",
      "Best model at iteration: 22 | smooth loss: 4.36659\n",
      "Best model at iteration: 23 | smooth loss: 4.36566\n",
      "Best model at iteration: 24 | smooth loss: 4.36468\n",
      "Best model at iteration: 25 | smooth loss: 4.36364\n",
      "Best model at iteration: 26 | smooth loss: 4.36243\n",
      "Best model at iteration: 27 | smooth loss: 4.36113\n",
      "Best model at iteration: 28 | smooth loss: 4.35983\n",
      "Best model at iteration: 29 | smooth loss: 4.35855\n",
      "Best model at iteration: 30 | smooth loss: 4.35750\n",
      "Best model at iteration: 31 | smooth loss: 4.35636\n",
      "Best model at iteration: 32 | smooth loss: 4.35516\n",
      "Best model at iteration: 33 | smooth loss: 4.35384\n",
      "Best model at iteration: 34 | smooth loss: 4.35270\n",
      "Best model at iteration: 35 | smooth loss: 4.35145\n",
      "Best model at iteration: 36 | smooth loss: 4.35028\n",
      "Best model at iteration: 37 | smooth loss: 4.34888\n",
      "Best model at iteration: 38 | smooth loss: 4.34780\n",
      "Best model at iteration: 39 | smooth loss: 4.34668\n",
      "Best model at iteration: 40 | smooth loss: 4.34543\n",
      "Best model at iteration: 41 | smooth loss: 4.34408\n",
      "Best model at iteration: 42 | smooth loss: 4.34269\n",
      "Best model at iteration: 43 | smooth loss: 4.34138\n",
      "Best model at iteration: 44 | smooth loss: 4.33979\n",
      "Best model at iteration: 45 | smooth loss: 4.33845\n",
      "Best model at iteration: 46 | smooth loss: 4.33717\n",
      "Best model at iteration: 47 | smooth loss: 4.33581\n",
      "Best model at iteration: 48 | smooth loss: 4.33434\n",
      "Best model at iteration: 49 | smooth loss: 4.33293\n",
      "Best model at iteration: 50 | smooth loss: 4.33224\n",
      "Best model at iteration: 51 | smooth loss: 4.33092\n",
      "Best model at iteration: 52 | smooth loss: 4.32983\n",
      "Best model at iteration: 53 | smooth loss: 4.32902\n",
      "Best model at iteration: 54 | smooth loss: 4.32767\n",
      "Best model at iteration: 55 | smooth loss: 4.32631\n",
      "Best model at iteration: 56 | smooth loss: 4.32467\n",
      "Best model at iteration: 57 | smooth loss: 4.32348\n",
      "Best model at iteration: 58 | smooth loss: 4.32272\n",
      "Best model at iteration: 59 | smooth loss: 4.32134\n",
      "Best model at iteration: 60 | smooth loss: 4.31993\n",
      "Best model at iteration: 61 | smooth loss: 4.31867\n",
      "Best model at iteration: 62 | smooth loss: 4.31725\n",
      "Best model at iteration: 63 | smooth loss: 4.31660\n",
      "Best model at iteration: 64 | smooth loss: 4.31533\n",
      "Best model at iteration: 65 | smooth loss: 4.31404\n",
      "Best model at iteration: 66 | smooth loss: 4.31314\n",
      "Best model at iteration: 67 | smooth loss: 4.31192\n",
      "Best model at iteration: 68 | smooth loss: 4.31077\n",
      "Best model at iteration: 69 | smooth loss: 4.30925\n",
      "Best model at iteration: 70 | smooth loss: 4.30814\n",
      "Best model at iteration: 71 | smooth loss: 4.30687\n",
      "Best model at iteration: 72 | smooth loss: 4.30543\n",
      "Best model at iteration: 73 | smooth loss: 4.30395\n",
      "Best model at iteration: 74 | smooth loss: 4.30251\n",
      "Best model at iteration: 75 | smooth loss: 4.30176\n",
      "Best model at iteration: 76 | smooth loss: 4.30043\n",
      "Best model at iteration: 77 | smooth loss: 4.29908\n",
      "Best model at iteration: 78 | smooth loss: 4.29770\n",
      "Best model at iteration: 79 | smooth loss: 4.29626\n",
      "Best model at iteration: 80 | smooth loss: 4.29490\n",
      "Best model at iteration: 81 | smooth loss: 4.29348\n",
      "Best model at iteration: 82 | smooth loss: 4.29215\n",
      "Best model at iteration: 83 | smooth loss: 4.29115\n",
      "Best model at iteration: 84 | smooth loss: 4.28965\n",
      "Best model at iteration: 85 | smooth loss: 4.28828\n",
      "Best model at iteration: 86 | smooth loss: 4.28687\n",
      "Best model at iteration: 87 | smooth loss: 4.28598\n",
      "Best model at iteration: 88 | smooth loss: 4.28545\n",
      "Best model at iteration: 89 | smooth loss: 4.28440\n",
      "Best model at iteration: 90 | smooth loss: 4.28397\n",
      "Best model at iteration: 91 | smooth loss: 4.28278\n",
      "Best model at iteration: 92 | smooth loss: 4.28148\n",
      "Best model at iteration: 93 | smooth loss: 4.28005\n",
      "Best model at iteration: 94 | smooth loss: 4.27891\n",
      "Best model at iteration: 95 | smooth loss: 4.27781\n",
      "Best model at iteration: 96 | smooth loss: 4.27648\n",
      "Best model at iteration: 97 | smooth loss: 4.27505\n",
      "Best model at iteration: 98 | smooth loss: 4.27373\n",
      "Best model at iteration: 99 | smooth loss: 4.27239\n",
      "Best model at iteration: 100 | smooth loss: 4.27135\n",
      "Best model at iteration: 101 | smooth loss: 4.26992\n",
      "Best model at iteration: 102 | smooth loss: 4.26850\n",
      "Best model at iteration: 103 | smooth loss: 4.26778\n",
      "Best model at iteration: 104 | smooth loss: 4.26643\n",
      "Best model at iteration: 105 | smooth loss: 4.26510\n",
      "Best model at iteration: 106 | smooth loss: 4.26386\n",
      "Best model at iteration: 107 | smooth loss: 4.26293\n",
      "Best model at iteration: 108 | smooth loss: 4.26233\n",
      "Best model at iteration: 109 | smooth loss: 4.26142\n",
      "Best model at iteration: 110 | smooth loss: 4.25992\n",
      "Best model at iteration: 111 | smooth loss: 4.25875\n",
      "Best model at iteration: 112 | smooth loss: 4.25842\n",
      "Best model at iteration: 113 | smooth loss: 4.25706\n",
      "Best model at iteration: 114 | smooth loss: 4.25572\n",
      "Best model at iteration: 115 | smooth loss: 4.25456\n",
      "Best model at iteration: 116 | smooth loss: 4.25332\n",
      "Best model at iteration: 117 | smooth loss: 4.25259\n",
      "Best model at iteration: 118 | smooth loss: 4.25145\n",
      "Best model at iteration: 119 | smooth loss: 4.25018\n",
      "Best model at iteration: 120 | smooth loss: 4.24895\n",
      "Best model at iteration: 121 | smooth loss: 4.24768\n",
      "Best model at iteration: 122 | smooth loss: 4.24647\n",
      "Best model at iteration: 123 | smooth loss: 4.24551\n",
      "Best model at iteration: 124 | smooth loss: 4.24442\n",
      "Best model at iteration: 125 | smooth loss: 4.24351\n",
      "Best model at iteration: 126 | smooth loss: 4.24213\n",
      "Best model at iteration: 127 | smooth loss: 4.24075\n",
      "Best model at iteration: 128 | smooth loss: 4.24004\n",
      "Best model at iteration: 129 | smooth loss: 4.23898\n",
      "Best model at iteration: 130 | smooth loss: 4.23821\n",
      "Best model at iteration: 131 | smooth loss: 4.23663\n",
      "Best model at iteration: 132 | smooth loss: 4.23567\n",
      "Best model at iteration: 133 | smooth loss: 4.23417\n",
      "Best model at iteration: 134 | smooth loss: 4.23352\n",
      "Best model at iteration: 135 | smooth loss: 4.23225\n",
      "Best model at iteration: 136 | smooth loss: 4.23147\n",
      "Best model at iteration: 137 | smooth loss: 4.23016\n",
      "Best model at iteration: 138 | smooth loss: 4.22901\n",
      "Best model at iteration: 139 | smooth loss: 4.22834\n",
      "Best model at iteration: 140 | smooth loss: 4.22702\n",
      "Best model at iteration: 141 | smooth loss: 4.22627\n",
      "Best model at iteration: 142 | smooth loss: 4.22529\n",
      "Best model at iteration: 143 | smooth loss: 4.22418\n",
      "Best model at iteration: 144 | smooth loss: 4.22337\n",
      "Best model at iteration: 145 | smooth loss: 4.22221\n",
      "Best model at iteration: 146 | smooth loss: 4.22103\n",
      "Best model at iteration: 147 | smooth loss: 4.21998\n",
      "Best model at iteration: 148 | smooth loss: 4.21908\n",
      "Best model at iteration: 149 | smooth loss: 4.21770\n",
      "Best model at iteration: 150 | smooth loss: 4.21658\n",
      "Best model at iteration: 151 | smooth loss: 4.21518\n",
      "Best model at iteration: 152 | smooth loss: 4.21403\n",
      "Best model at iteration: 153 | smooth loss: 4.21297\n",
      "Best model at iteration: 154 | smooth loss: 4.21146\n",
      "Best model at iteration: 155 | smooth loss: 4.20989\n",
      "Best model at iteration: 156 | smooth loss: 4.20848\n",
      "Best model at iteration: 157 | smooth loss: 4.20698\n",
      "Best model at iteration: 158 | smooth loss: 4.20583\n",
      "Best model at iteration: 159 | smooth loss: 4.20431\n",
      "Best model at iteration: 160 | smooth loss: 4.20316\n",
      "Best model at iteration: 161 | smooth loss: 4.20189\n",
      "Best model at iteration: 162 | smooth loss: 4.20058\n",
      "Best model at iteration: 163 | smooth loss: 4.19935\n",
      "Best model at iteration: 164 | smooth loss: 4.19811\n",
      "Best model at iteration: 165 | smooth loss: 4.19738\n",
      "Best model at iteration: 166 | smooth loss: 4.19617\n",
      "Best model at iteration: 167 | smooth loss: 4.19499\n",
      "Best model at iteration: 168 | smooth loss: 4.19381\n",
      "Best model at iteration: 169 | smooth loss: 4.19258\n",
      "Best model at iteration: 170 | smooth loss: 4.19170\n",
      "Best model at iteration: 171 | smooth loss: 4.19010\n",
      "Best model at iteration: 172 | smooth loss: 4.18912\n",
      "Best model at iteration: 173 | smooth loss: 4.18766\n",
      "Best model at iteration: 174 | smooth loss: 4.18622\n",
      "Best model at iteration: 175 | smooth loss: 4.18469\n",
      "Best model at iteration: 176 | smooth loss: 4.18352\n",
      "Best model at iteration: 177 | smooth loss: 4.18253\n",
      "Best model at iteration: 178 | smooth loss: 4.18135\n",
      "Best model at iteration: 179 | smooth loss: 4.18031\n",
      "Best model at iteration: 180 | smooth loss: 4.17933\n",
      "Best model at iteration: 181 | smooth loss: 4.17810\n",
      "Best model at iteration: 182 | smooth loss: 4.17721\n",
      "Best model at iteration: 183 | smooth loss: 4.17570\n",
      "Best model at iteration: 184 | smooth loss: 4.17461\n",
      "Best model at iteration: 185 | smooth loss: 4.17316\n",
      "Best model at iteration: 186 | smooth loss: 4.17179\n",
      "Best model at iteration: 187 | smooth loss: 4.17049\n",
      "Best model at iteration: 188 | smooth loss: 4.16919\n",
      "Best model at iteration: 189 | smooth loss: 4.16811\n",
      "Best model at iteration: 190 | smooth loss: 4.16660\n",
      "Best model at iteration: 191 | smooth loss: 4.16527\n",
      "Best model at iteration: 192 | smooth loss: 4.16397\n",
      "Best model at iteration: 193 | smooth loss: 4.16257\n",
      "Best model at iteration: 194 | smooth loss: 4.16124\n",
      "Best model at iteration: 195 | smooth loss: 4.16001\n",
      "Best model at iteration: 196 | smooth loss: 4.15893\n",
      "Best model at iteration: 197 | smooth loss: 4.15738\n",
      "Best model at iteration: 198 | smooth loss: 4.15588\n",
      "Best model at iteration: 199 | smooth loss: 4.15447\n",
      "Best model at iteration: 200 | smooth loss: 4.15324\n",
      "Best model at iteration: 201 | smooth loss: 4.15170\n",
      "Best model at iteration: 202 | smooth loss: 4.15089\n",
      "Best model at iteration: 203 | smooth loss: 4.14962\n",
      "Best model at iteration: 204 | smooth loss: 4.14868\n",
      "Best model at iteration: 205 | smooth loss: 4.14750\n",
      "Best model at iteration: 206 | smooth loss: 4.14641\n",
      "Best model at iteration: 207 | smooth loss: 4.14559\n",
      "Best model at iteration: 208 | smooth loss: 4.14430\n",
      "Best model at iteration: 209 | smooth loss: 4.14286\n",
      "Best model at iteration: 210 | smooth loss: 4.14227\n",
      "Best model at iteration: 211 | smooth loss: 4.14116\n",
      "Best model at iteration: 212 | smooth loss: 4.13994\n",
      "Best model at iteration: 213 | smooth loss: 4.13870\n",
      "Best model at iteration: 214 | smooth loss: 4.13756\n",
      "Best model at iteration: 215 | smooth loss: 4.13626\n",
      "Best model at iteration: 216 | smooth loss: 4.13509\n",
      "Best model at iteration: 217 | smooth loss: 4.13380\n",
      "Best model at iteration: 218 | smooth loss: 4.13302\n",
      "Best model at iteration: 219 | smooth loss: 4.13176\n",
      "Best model at iteration: 220 | smooth loss: 4.13005\n",
      "Best model at iteration: 221 | smooth loss: 4.12899\n",
      "Best model at iteration: 222 | smooth loss: 4.12748\n",
      "Best model at iteration: 223 | smooth loss: 4.12628\n",
      "Best model at iteration: 224 | smooth loss: 4.12540\n",
      "Best model at iteration: 225 | smooth loss: 4.12430\n",
      "Best model at iteration: 226 | smooth loss: 4.12295\n",
      "Best model at iteration: 227 | smooth loss: 4.12145\n",
      "Best model at iteration: 228 | smooth loss: 4.12021\n",
      "Best model at iteration: 229 | smooth loss: 4.11878\n",
      "Best model at iteration: 230 | smooth loss: 4.11751\n",
      "Best model at iteration: 231 | smooth loss: 4.11678\n",
      "Best model at iteration: 232 | smooth loss: 4.11536\n",
      "Best model at iteration: 233 | smooth loss: 4.11381\n",
      "Best model at iteration: 234 | smooth loss: 4.11233\n",
      "Best model at iteration: 235 | smooth loss: 4.11108\n",
      "Best model at iteration: 236 | smooth loss: 4.10944\n",
      "Best model at iteration: 237 | smooth loss: 4.10819\n",
      "Best model at iteration: 238 | smooth loss: 4.10728\n",
      "Best model at iteration: 239 | smooth loss: 4.10593\n",
      "Best model at iteration: 240 | smooth loss: 4.10457\n",
      "Best model at iteration: 241 | smooth loss: 4.10340\n",
      "Best model at iteration: 242 | smooth loss: 4.10206\n",
      "Best model at iteration: 243 | smooth loss: 4.10092\n",
      "Best model at iteration: 244 | smooth loss: 4.09999\n",
      "Best model at iteration: 245 | smooth loss: 4.09874\n",
      "Best model at iteration: 246 | smooth loss: 4.09759\n",
      "Best model at iteration: 247 | smooth loss: 4.09635\n",
      "Best model at iteration: 248 | smooth loss: 4.09495\n",
      "Best model at iteration: 249 | smooth loss: 4.09346\n",
      "Best model at iteration: 250 | smooth loss: 4.09187\n",
      "Best model at iteration: 251 | smooth loss: 4.09076\n",
      "Best model at iteration: 252 | smooth loss: 4.08945\n",
      "Best model at iteration: 253 | smooth loss: 4.08826\n",
      "Best model at iteration: 254 | smooth loss: 4.08690\n",
      "Best model at iteration: 255 | smooth loss: 4.08558\n",
      "Best model at iteration: 256 | smooth loss: 4.08434\n",
      "Best model at iteration: 257 | smooth loss: 4.08306\n",
      "Best model at iteration: 258 | smooth loss: 4.08160\n",
      "Best model at iteration: 259 | smooth loss: 4.08032\n",
      "Best model at iteration: 260 | smooth loss: 4.07898\n",
      "Best model at iteration: 261 | smooth loss: 4.07764\n",
      "Best model at iteration: 262 | smooth loss: 4.07662\n",
      "Best model at iteration: 263 | smooth loss: 4.07515\n",
      "Best model at iteration: 264 | smooth loss: 4.07358\n",
      "Best model at iteration: 265 | smooth loss: 4.07226\n",
      "Best model at iteration: 266 | smooth loss: 4.07084\n",
      "Best model at iteration: 267 | smooth loss: 4.06951\n",
      "Best model at iteration: 268 | smooth loss: 4.06827\n",
      "Best model at iteration: 269 | smooth loss: 4.06677\n",
      "Best model at iteration: 270 | smooth loss: 4.06557\n",
      "Best model at iteration: 271 | smooth loss: 4.06435\n",
      "Best model at iteration: 272 | smooth loss: 4.06335\n",
      "Best model at iteration: 273 | smooth loss: 4.06208\n",
      "Best model at iteration: 274 | smooth loss: 4.06078\n",
      "Best model at iteration: 275 | smooth loss: 4.05947\n",
      "Best model at iteration: 276 | smooth loss: 4.05806\n",
      "Best model at iteration: 277 | smooth loss: 4.05680\n",
      "Best model at iteration: 278 | smooth loss: 4.05574\n",
      "Best model at iteration: 279 | smooth loss: 4.05451\n",
      "Best model at iteration: 280 | smooth loss: 4.05331\n",
      "Best model at iteration: 281 | smooth loss: 4.05182\n",
      "Best model at iteration: 282 | smooth loss: 4.05040\n",
      "Best model at iteration: 283 | smooth loss: 4.04867\n",
      "Best model at iteration: 284 | smooth loss: 4.04725\n",
      "Best model at iteration: 285 | smooth loss: 4.04653\n",
      "Best model at iteration: 286 | smooth loss: 4.04524\n",
      "Best model at iteration: 287 | smooth loss: 4.04406\n",
      "Best model at iteration: 288 | smooth loss: 4.04249\n",
      "Best model at iteration: 289 | smooth loss: 4.04111\n",
      "Best model at iteration: 290 | smooth loss: 4.03985\n",
      "Best model at iteration: 291 | smooth loss: 4.03826\n",
      "Best model at iteration: 292 | smooth loss: 4.03731\n",
      "Best model at iteration: 293 | smooth loss: 4.03583\n",
      "Best model at iteration: 294 | smooth loss: 4.03463\n",
      "Best model at iteration: 295 | smooth loss: 4.03313\n",
      "Best model at iteration: 296 | smooth loss: 4.03170\n",
      "Best model at iteration: 297 | smooth loss: 4.03016\n",
      "Best model at iteration: 298 | smooth loss: 4.02899\n",
      "Best model at iteration: 299 | smooth loss: 4.02802\n",
      "Best model at iteration: 300 | smooth loss: 4.02655\n",
      "Best model at iteration: 301 | smooth loss: 4.02526\n",
      "Best model at iteration: 302 | smooth loss: 4.02368\n",
      "Best model at iteration: 303 | smooth loss: 4.02264\n",
      "Best model at iteration: 304 | smooth loss: 4.02147\n",
      "Best model at iteration: 305 | smooth loss: 4.02001\n",
      "Best model at iteration: 306 | smooth loss: 4.01899\n",
      "Best model at iteration: 307 | smooth loss: 4.01748\n",
      "Best model at iteration: 308 | smooth loss: 4.01619\n",
      "Best model at iteration: 309 | smooth loss: 4.01467\n",
      "Best model at iteration: 310 | smooth loss: 4.01334\n",
      "Best model at iteration: 311 | smooth loss: 4.01217\n",
      "Best model at iteration: 312 | smooth loss: 4.01068\n",
      "Best model at iteration: 313 | smooth loss: 4.00959\n",
      "Best model at iteration: 314 | smooth loss: 4.00810\n",
      "Best model at iteration: 315 | smooth loss: 4.00675\n",
      "Best model at iteration: 316 | smooth loss: 4.00554\n",
      "Best model at iteration: 317 | smooth loss: 4.00436\n",
      "Best model at iteration: 318 | smooth loss: 4.00322\n",
      "Best model at iteration: 319 | smooth loss: 4.00179\n",
      "Best model at iteration: 320 | smooth loss: 4.00057\n",
      "Best model at iteration: 321 | smooth loss: 3.99959\n",
      "Best model at iteration: 322 | smooth loss: 3.99800\n",
      "Best model at iteration: 323 | smooth loss: 3.99679\n",
      "Best model at iteration: 324 | smooth loss: 3.99539\n",
      "Best model at iteration: 325 | smooth loss: 3.99414\n",
      "Best model at iteration: 326 | smooth loss: 3.99277\n",
      "Best model at iteration: 327 | smooth loss: 3.99136\n",
      "Best model at iteration: 328 | smooth loss: 3.98972\n",
      "Best model at iteration: 329 | smooth loss: 3.98816\n",
      "Best model at iteration: 330 | smooth loss: 3.98704\n",
      "Best model at iteration: 331 | smooth loss: 3.98595\n",
      "Best model at iteration: 332 | smooth loss: 3.98447\n",
      "Best model at iteration: 333 | smooth loss: 3.98271\n",
      "Best model at iteration: 334 | smooth loss: 3.98182\n",
      "Best model at iteration: 335 | smooth loss: 3.98057\n",
      "Best model at iteration: 336 | smooth loss: 3.97922\n",
      "Best model at iteration: 337 | smooth loss: 3.97788\n",
      "Best model at iteration: 338 | smooth loss: 3.97700\n",
      "Best model at iteration: 339 | smooth loss: 3.97570\n",
      "Best model at iteration: 340 | smooth loss: 3.97423\n",
      "Best model at iteration: 341 | smooth loss: 3.97250\n",
      "Best model at iteration: 342 | smooth loss: 3.97118\n",
      "Best model at iteration: 343 | smooth loss: 3.96984\n",
      "Best model at iteration: 344 | smooth loss: 3.96860\n",
      "Best model at iteration: 345 | smooth loss: 3.96719\n",
      "Best model at iteration: 346 | smooth loss: 3.96607\n",
      "Best model at iteration: 347 | smooth loss: 3.96459\n",
      "Best model at iteration: 348 | smooth loss: 3.96336\n",
      "Best model at iteration: 349 | smooth loss: 3.96195\n",
      "Best model at iteration: 350 | smooth loss: 3.96074\n",
      "Best model at iteration: 351 | smooth loss: 3.95900\n",
      "Best model at iteration: 352 | smooth loss: 3.95766\n",
      "Best model at iteration: 353 | smooth loss: 3.95611\n",
      "Best model at iteration: 354 | smooth loss: 3.95477\n",
      "Best model at iteration: 355 | smooth loss: 3.95350\n",
      "Best model at iteration: 356 | smooth loss: 3.95184\n",
      "Best model at iteration: 357 | smooth loss: 3.95081\n",
      "Best model at iteration: 358 | smooth loss: 3.94969\n",
      "Best model at iteration: 359 | smooth loss: 3.94801\n",
      "Best model at iteration: 360 | smooth loss: 3.94688\n",
      "Best model at iteration: 361 | smooth loss: 3.94544\n",
      "Best model at iteration: 362 | smooth loss: 3.94423\n",
      "Best model at iteration: 363 | smooth loss: 3.94297\n",
      "Best model at iteration: 364 | smooth loss: 3.94181\n",
      "Best model at iteration: 365 | smooth loss: 3.94076\n",
      "Best model at iteration: 366 | smooth loss: 3.93953\n",
      "Best model at iteration: 367 | smooth loss: 3.93838\n",
      "Best model at iteration: 368 | smooth loss: 3.93681\n",
      "Best model at iteration: 369 | smooth loss: 3.93545\n",
      "Best model at iteration: 370 | smooth loss: 3.93445\n",
      "Best model at iteration: 371 | smooth loss: 3.93284\n",
      "Best model at iteration: 372 | smooth loss: 3.93190\n",
      "Best model at iteration: 373 | smooth loss: 3.93065\n",
      "Best model at iteration: 374 | smooth loss: 3.92937\n",
      "Best model at iteration: 375 | smooth loss: 3.92828\n",
      "Best model at iteration: 376 | smooth loss: 3.92690\n",
      "Best model at iteration: 377 | smooth loss: 3.92618\n",
      "Best model at iteration: 378 | smooth loss: 3.92472\n",
      "Best model at iteration: 379 | smooth loss: 3.92389\n",
      "Best model at iteration: 380 | smooth loss: 3.92343\n",
      "Best model at iteration: 381 | smooth loss: 3.92241\n",
      "Best model at iteration: 382 | smooth loss: 3.92119\n",
      "Best model at iteration: 383 | smooth loss: 3.92014\n",
      "Best model at iteration: 384 | smooth loss: 3.91909\n",
      "Best model at iteration: 385 | smooth loss: 3.91797\n",
      "Best model at iteration: 386 | smooth loss: 3.91680\n",
      "Best model at iteration: 387 | smooth loss: 3.91532\n",
      "Best model at iteration: 388 | smooth loss: 3.91390\n",
      "Best model at iteration: 389 | smooth loss: 3.91318\n",
      "Best model at iteration: 390 | smooth loss: 3.91173\n",
      "Best model at iteration: 391 | smooth loss: 3.91096\n",
      "Best model at iteration: 392 | smooth loss: 3.90943\n",
      "Best model at iteration: 393 | smooth loss: 3.90793\n",
      "Best model at iteration: 394 | smooth loss: 3.90638\n",
      "Best model at iteration: 395 | smooth loss: 3.90529\n",
      "Best model at iteration: 396 | smooth loss: 3.90403\n",
      "Best model at iteration: 397 | smooth loss: 3.90245\n",
      "Best model at iteration: 398 | smooth loss: 3.90130\n",
      "Best model at iteration: 399 | smooth loss: 3.89994\n",
      "Best model at iteration: 400 | smooth loss: 3.89885\n",
      "Best model at iteration: 401 | smooth loss: 3.89769\n",
      "Best model at iteration: 402 | smooth loss: 3.89616\n",
      "Best model at iteration: 403 | smooth loss: 3.89478\n",
      "Best model at iteration: 404 | smooth loss: 3.89353\n",
      "Best model at iteration: 405 | smooth loss: 3.89194\n",
      "Best model at iteration: 406 | smooth loss: 3.89041\n",
      "Best model at iteration: 407 | smooth loss: 3.88897\n",
      "Best model at iteration: 408 | smooth loss: 3.88832\n",
      "Best model at iteration: 409 | smooth loss: 3.88725\n",
      "Best model at iteration: 410 | smooth loss: 3.88707\n",
      "Best model at iteration: 411 | smooth loss: 3.88618\n",
      "Best model at iteration: 412 | smooth loss: 3.88540\n",
      "Best model at iteration: 413 | smooth loss: 3.88434\n",
      "Best model at iteration: 414 | smooth loss: 3.88379\n",
      "Best model at iteration: 415 | smooth loss: 3.88267\n",
      "Best model at iteration: 416 | smooth loss: 3.88181\n",
      "Best model at iteration: 417 | smooth loss: 3.88058\n",
      "Best model at iteration: 418 | smooth loss: 3.87914\n",
      "Best model at iteration: 419 | smooth loss: 3.87796\n",
      "Best model at iteration: 420 | smooth loss: 3.87746\n",
      "Best model at iteration: 421 | smooth loss: 3.87626\n",
      "Best model at iteration: 422 | smooth loss: 3.87503\n",
      "Best model at iteration: 423 | smooth loss: 3.87360\n",
      "Best model at iteration: 424 | smooth loss: 3.87240\n",
      "Best model at iteration: 425 | smooth loss: 3.87058\n",
      "Best model at iteration: 426 | smooth loss: 3.87000\n",
      "Best model at iteration: 427 | smooth loss: 3.86910\n",
      "Best model at iteration: 428 | smooth loss: 3.86757\n",
      "Best model at iteration: 429 | smooth loss: 3.86666\n",
      "Best model at iteration: 430 | smooth loss: 3.86566\n",
      "Best model at iteration: 431 | smooth loss: 3.86440\n",
      "Best model at iteration: 432 | smooth loss: 3.86336\n",
      "Best model at iteration: 433 | smooth loss: 3.86216\n",
      "Best model at iteration: 434 | smooth loss: 3.86103\n",
      "Best model at iteration: 435 | smooth loss: 3.85999\n",
      "Best model at iteration: 436 | smooth loss: 3.85953\n",
      "Best model at iteration: 437 | smooth loss: 3.85827\n",
      "Best model at iteration: 438 | smooth loss: 3.85687\n",
      "Best model at iteration: 439 | smooth loss: 3.85599\n",
      "Best model at iteration: 440 | smooth loss: 3.85517\n",
      "Best model at iteration: 441 | smooth loss: 3.85371\n",
      "Best model at iteration: 442 | smooth loss: 3.85277\n",
      "Best model at iteration: 443 | smooth loss: 3.85172\n",
      "Best model at iteration: 444 | smooth loss: 3.85120\n",
      "Best model at iteration: 445 | smooth loss: 3.85028\n",
      "Best model at iteration: 446 | smooth loss: 3.84913\n",
      "Best model at iteration: 447 | smooth loss: 3.84816\n",
      "Best model at iteration: 448 | smooth loss: 3.84735\n",
      "Best model at iteration: 449 | smooth loss: 3.84642\n",
      "Best model at iteration: 450 | smooth loss: 3.84532\n",
      "Best model at iteration: 451 | smooth loss: 3.84424\n",
      "Best model at iteration: 452 | smooth loss: 3.84321\n",
      "Best model at iteration: 453 | smooth loss: 3.84238\n",
      "Best model at iteration: 454 | smooth loss: 3.84100\n",
      "Best model at iteration: 455 | smooth loss: 3.83965\n",
      "Best model at iteration: 456 | smooth loss: 3.83852\n",
      "Best model at iteration: 457 | smooth loss: 3.83759\n",
      "Best model at iteration: 458 | smooth loss: 3.83619\n",
      "Best model at iteration: 459 | smooth loss: 3.83527\n",
      "Best model at iteration: 460 | smooth loss: 3.83411\n",
      "Best model at iteration: 461 | smooth loss: 3.83330\n",
      "Best model at iteration: 462 | smooth loss: 3.83216\n",
      "Best model at iteration: 463 | smooth loss: 3.83115\n",
      "Best model at iteration: 464 | smooth loss: 3.82999\n",
      "Best model at iteration: 465 | smooth loss: 3.82908\n",
      "Best model at iteration: 466 | smooth loss: 3.82774\n",
      "Best model at iteration: 467 | smooth loss: 3.82661\n",
      "Best model at iteration: 468 | smooth loss: 3.82525\n",
      "Best model at iteration: 469 | smooth loss: 3.82508\n",
      "Best model at iteration: 470 | smooth loss: 3.82457\n",
      "Best model at iteration: 471 | smooth loss: 3.82351\n",
      "Best model at iteration: 472 | smooth loss: 3.82217\n",
      "Best model at iteration: 473 | smooth loss: 3.82068\n",
      "Best model at iteration: 474 | smooth loss: 3.81951\n",
      "Best model at iteration: 475 | smooth loss: 3.81863\n",
      "Best model at iteration: 476 | smooth loss: 3.81733\n",
      "Best model at iteration: 477 | smooth loss: 3.81610\n",
      "Best model at iteration: 478 | smooth loss: 3.81478\n",
      "Best model at iteration: 479 | smooth loss: 3.81340\n",
      "Best model at iteration: 480 | smooth loss: 3.81281\n",
      "Best model at iteration: 481 | smooth loss: 3.81169\n",
      "Best model at iteration: 482 | smooth loss: 3.81097\n",
      "Best model at iteration: 483 | smooth loss: 3.81026\n",
      "Best model at iteration: 484 | smooth loss: 3.80919\n",
      "Best model at iteration: 485 | smooth loss: 3.80779\n",
      "Best model at iteration: 486 | smooth loss: 3.80700\n",
      "Best model at iteration: 487 | smooth loss: 3.80572\n",
      "Best model at iteration: 488 | smooth loss: 3.80430\n",
      "Best model at iteration: 489 | smooth loss: 3.80326\n",
      "Best model at iteration: 490 | smooth loss: 3.80204\n",
      "Best model at iteration: 491 | smooth loss: 3.80063\n",
      "Best model at iteration: 492 | smooth loss: 3.79939\n",
      "Best model at iteration: 493 | smooth loss: 3.79818\n",
      "Best model at iteration: 494 | smooth loss: 3.79741\n",
      "Best model at iteration: 495 | smooth loss: 3.79637\n",
      "Best model at iteration: 496 | smooth loss: 3.79550\n",
      "Best model at iteration: 497 | smooth loss: 3.79480\n",
      "Best model at iteration: 498 | smooth loss: 3.79326\n",
      "Best model at iteration: 499 | smooth loss: 3.79278\n",
      "Best model at iteration: 500 | smooth loss: 3.79155\n",
      "Best model at iteration: 501 | smooth loss: 3.79017\n",
      "Best model at iteration: 502 | smooth loss: 3.78909\n",
      "Best model at iteration: 503 | smooth loss: 3.78840\n",
      "Best model at iteration: 504 | smooth loss: 3.78687\n",
      "Best model at iteration: 505 | smooth loss: 3.78576\n",
      "Best model at iteration: 506 | smooth loss: 3.78430\n",
      "Best model at iteration: 507 | smooth loss: 3.78339\n",
      "Best model at iteration: 508 | smooth loss: 3.78218\n",
      "Best model at iteration: 509 | smooth loss: 3.78081\n",
      "Best model at iteration: 510 | smooth loss: 3.78014\n",
      "Best model at iteration: 511 | smooth loss: 3.77897\n",
      "Best model at iteration: 512 | smooth loss: 3.77786\n",
      "Best model at iteration: 513 | smooth loss: 3.77698\n",
      "Best model at iteration: 514 | smooth loss: 3.77628\n",
      "Best model at iteration: 515 | smooth loss: 3.77498\n",
      "Best model at iteration: 516 | smooth loss: 3.77376\n",
      "Best model at iteration: 517 | smooth loss: 3.77321\n",
      "Best model at iteration: 518 | smooth loss: 3.77212\n",
      "Best model at iteration: 519 | smooth loss: 3.77052\n",
      "Best model at iteration: 520 | smooth loss: 3.76981\n",
      "Best model at iteration: 521 | smooth loss: 3.76890\n",
      "Best model at iteration: 522 | smooth loss: 3.76770\n",
      "Best model at iteration: 523 | smooth loss: 3.76681\n",
      "Best model at iteration: 524 | smooth loss: 3.76585\n",
      "Best model at iteration: 525 | smooth loss: 3.76471\n",
      "Best model at iteration: 526 | smooth loss: 3.76365\n",
      "Best model at iteration: 527 | smooth loss: 3.76285\n",
      "Best model at iteration: 528 | smooth loss: 3.76156\n",
      "Best model at iteration: 529 | smooth loss: 3.76103\n",
      "Best model at iteration: 530 | smooth loss: 3.76005\n",
      "Best model at iteration: 531 | smooth loss: 3.75900\n",
      "Best model at iteration: 532 | smooth loss: 3.75803\n",
      "Best model at iteration: 533 | smooth loss: 3.75680\n",
      "Best model at iteration: 534 | smooth loss: 3.75559\n",
      "Best model at iteration: 535 | smooth loss: 3.75423\n",
      "Best model at iteration: 536 | smooth loss: 3.75350\n",
      "Best model at iteration: 537 | smooth loss: 3.75267\n",
      "Best model at iteration: 538 | smooth loss: 3.75147\n",
      "Best model at iteration: 539 | smooth loss: 3.75061\n",
      "Best model at iteration: 540 | smooth loss: 3.74908\n",
      "Best model at iteration: 541 | smooth loss: 3.74814\n",
      "Best model at iteration: 542 | smooth loss: 3.74724\n",
      "Best model at iteration: 543 | smooth loss: 3.74608\n",
      "Best model at iteration: 544 | smooth loss: 3.74493\n",
      "Best model at iteration: 545 | smooth loss: 3.74385\n",
      "Best model at iteration: 546 | smooth loss: 3.74280\n",
      "Best model at iteration: 547 | smooth loss: 3.74165\n",
      "Best model at iteration: 548 | smooth loss: 3.74089\n",
      "Best model at iteration: 549 | smooth loss: 3.74032\n",
      "Best model at iteration: 550 | smooth loss: 3.73872\n",
      "Best model at iteration: 551 | smooth loss: 3.73762\n",
      "Best model at iteration: 552 | smooth loss: 3.73637\n",
      "Best model at iteration: 553 | smooth loss: 3.73525\n",
      "Best model at iteration: 554 | smooth loss: 3.73403\n",
      "Best model at iteration: 555 | smooth loss: 3.73301\n",
      "Best model at iteration: 556 | smooth loss: 3.73208\n",
      "Best model at iteration: 557 | smooth loss: 3.73153\n",
      "Best model at iteration: 558 | smooth loss: 3.73055\n",
      "Best model at iteration: 559 | smooth loss: 3.72961\n",
      "Best model at iteration: 560 | smooth loss: 3.72825\n",
      "Best model at iteration: 561 | smooth loss: 3.72723\n",
      "Best model at iteration: 562 | smooth loss: 3.72604\n",
      "Best model at iteration: 563 | smooth loss: 3.72465\n",
      "Best model at iteration: 564 | smooth loss: 3.72380\n",
      "Best model at iteration: 565 | smooth loss: 3.72287\n",
      "Best model at iteration: 566 | smooth loss: 3.72210\n",
      "Best model at iteration: 567 | smooth loss: 3.72105\n",
      "Best model at iteration: 568 | smooth loss: 3.72004\n",
      "Best model at iteration: 569 | smooth loss: 3.71921\n",
      "Best model at iteration: 570 | smooth loss: 3.71766\n",
      "Best model at iteration: 571 | smooth loss: 3.71647\n",
      "Best model at iteration: 572 | smooth loss: 3.71524\n",
      "Best model at iteration: 573 | smooth loss: 3.71391\n",
      "Best model at iteration: 574 | smooth loss: 3.71306\n",
      "Best model at iteration: 575 | smooth loss: 3.71190\n",
      "Best model at iteration: 576 | smooth loss: 3.71079\n",
      "Best model at iteration: 577 | smooth loss: 3.70975\n",
      "Best model at iteration: 578 | smooth loss: 3.70855\n",
      "Best model at iteration: 579 | smooth loss: 3.70743\n",
      "Best model at iteration: 580 | smooth loss: 3.70627\n",
      "Best model at iteration: 581 | smooth loss: 3.70542\n",
      "Best model at iteration: 582 | smooth loss: 3.70426\n",
      "Best model at iteration: 583 | smooth loss: 3.70307\n",
      "Best model at iteration: 584 | smooth loss: 3.70222\n",
      "Best model at iteration: 585 | smooth loss: 3.70085\n",
      "Best model at iteration: 586 | smooth loss: 3.69989\n",
      "Best model at iteration: 587 | smooth loss: 3.69871\n",
      "Best model at iteration: 588 | smooth loss: 3.69759\n",
      "Best model at iteration: 589 | smooth loss: 3.69651\n",
      "Best model at iteration: 590 | smooth loss: 3.69589\n",
      "Best model at iteration: 591 | smooth loss: 3.69524\n",
      "Best model at iteration: 592 | smooth loss: 3.69459\n",
      "Best model at iteration: 593 | smooth loss: 3.69347\n",
      "Best model at iteration: 594 | smooth loss: 3.69273\n",
      "Best model at iteration: 595 | smooth loss: 3.69136\n",
      "Best model at iteration: 596 | smooth loss: 3.69035\n",
      "Best model at iteration: 597 | smooth loss: 3.69004\n",
      "Best model at iteration: 598 | smooth loss: 3.68901\n",
      "Best model at iteration: 599 | smooth loss: 3.68779\n",
      "Best model at iteration: 600 | smooth loss: 3.68705\n",
      "Best model at iteration: 601 | smooth loss: 3.68582\n",
      "Best model at iteration: 602 | smooth loss: 3.68481\n",
      "Best model at iteration: 603 | smooth loss: 3.68344\n",
      "Best model at iteration: 604 | smooth loss: 3.68267\n",
      "Best model at iteration: 605 | smooth loss: 3.68164\n",
      "Best model at iteration: 606 | smooth loss: 3.68070\n",
      "Best model at iteration: 607 | smooth loss: 3.67977\n",
      "Best model at iteration: 608 | smooth loss: 3.67857\n",
      "Best model at iteration: 609 | smooth loss: 3.67772\n",
      "Best model at iteration: 610 | smooth loss: 3.67711\n",
      "Best model at iteration: 611 | smooth loss: 3.67573\n",
      "Best model at iteration: 612 | smooth loss: 3.67474\n",
      "Best model at iteration: 613 | smooth loss: 3.67371\n",
      "Best model at iteration: 614 | smooth loss: 3.67244\n",
      "Best model at iteration: 615 | smooth loss: 3.67138\n",
      "Best model at iteration: 616 | smooth loss: 3.67032\n",
      "Best model at iteration: 617 | smooth loss: 3.66965\n",
      "Best model at iteration: 618 | smooth loss: 3.66863\n",
      "Best model at iteration: 619 | smooth loss: 3.66727\n",
      "Best model at iteration: 620 | smooth loss: 3.66626\n",
      "Best model at iteration: 621 | smooth loss: 3.66536\n",
      "Best model at iteration: 622 | smooth loss: 3.66456\n",
      "Best model at iteration: 623 | smooth loss: 3.66357\n",
      "Best model at iteration: 624 | smooth loss: 3.66260\n",
      "Best model at iteration: 625 | smooth loss: 3.66165\n",
      "Best model at iteration: 626 | smooth loss: 3.66070\n",
      "Best model at iteration: 627 | smooth loss: 3.65903\n",
      "Best model at iteration: 628 | smooth loss: 3.65841\n",
      "Best model at iteration: 629 | smooth loss: 3.65724\n",
      "Best model at iteration: 630 | smooth loss: 3.65579\n",
      "Best model at iteration: 631 | smooth loss: 3.65512\n",
      "Best model at iteration: 632 | smooth loss: 3.65433\n",
      "Best model at iteration: 633 | smooth loss: 3.65276\n",
      "Best model at iteration: 634 | smooth loss: 3.65196\n",
      "Best model at iteration: 635 | smooth loss: 3.65081\n",
      "Best model at iteration: 636 | smooth loss: 3.64999\n",
      "Best model at iteration: 637 | smooth loss: 3.64877\n",
      "Best model at iteration: 638 | smooth loss: 3.64745\n",
      "Best model at iteration: 639 | smooth loss: 3.64643\n",
      "Best model at iteration: 640 | smooth loss: 3.64494\n",
      "Best model at iteration: 641 | smooth loss: 3.64379\n",
      "Best model at iteration: 642 | smooth loss: 3.64266\n",
      "Best model at iteration: 643 | smooth loss: 3.64180\n",
      "Best model at iteration: 644 | smooth loss: 3.64035\n",
      "Best model at iteration: 645 | smooth loss: 3.63946\n",
      "Best model at iteration: 646 | smooth loss: 3.63841\n",
      "Best model at iteration: 647 | smooth loss: 3.63751\n",
      "Best model at iteration: 648 | smooth loss: 3.63638\n",
      "Best model at iteration: 649 | smooth loss: 3.63546\n",
      "Best model at iteration: 650 | smooth loss: 3.63424\n",
      "Best model at iteration: 651 | smooth loss: 3.63276\n",
      "Best model at iteration: 652 | smooth loss: 3.63138\n",
      "Best model at iteration: 653 | smooth loss: 3.63018\n",
      "Best model at iteration: 654 | smooth loss: 3.62912\n",
      "Best model at iteration: 655 | smooth loss: 3.62789\n",
      "Best model at iteration: 656 | smooth loss: 3.62679\n",
      "Best model at iteration: 657 | smooth loss: 3.62555\n",
      "Best model at iteration: 658 | smooth loss: 3.62497\n",
      "Best model at iteration: 659 | smooth loss: 3.62418\n",
      "Best model at iteration: 660 | smooth loss: 3.62306\n",
      "Best model at iteration: 661 | smooth loss: 3.62229\n",
      "Best model at iteration: 662 | smooth loss: 3.62096\n",
      "Best model at iteration: 663 | smooth loss: 3.62005\n",
      "Best model at iteration: 664 | smooth loss: 3.61864\n",
      "Best model at iteration: 665 | smooth loss: 3.61807\n",
      "Best model at iteration: 666 | smooth loss: 3.61655\n",
      "Best model at iteration: 667 | smooth loss: 3.61570\n",
      "Best model at iteration: 668 | smooth loss: 3.61493\n",
      "Best model at iteration: 669 | smooth loss: 3.61442\n",
      "Best model at iteration: 670 | smooth loss: 3.61332\n",
      "Best model at iteration: 671 | smooth loss: 3.61207\n",
      "Best model at iteration: 672 | smooth loss: 3.61123\n",
      "Best model at iteration: 673 | smooth loss: 3.61054\n",
      "Best model at iteration: 674 | smooth loss: 3.60935\n",
      "Best model at iteration: 675 | smooth loss: 3.60787\n",
      "Best model at iteration: 676 | smooth loss: 3.60699\n",
      "Best model at iteration: 677 | smooth loss: 3.60603\n",
      "Best model at iteration: 678 | smooth loss: 3.60500\n",
      "Best model at iteration: 679 | smooth loss: 3.60379\n",
      "Best model at iteration: 680 | smooth loss: 3.60319\n",
      "Best model at iteration: 681 | smooth loss: 3.60225\n",
      "Best model at iteration: 682 | smooth loss: 3.60131\n",
      "Best model at iteration: 683 | smooth loss: 3.60021\n",
      "Best model at iteration: 684 | smooth loss: 3.59896\n",
      "Best model at iteration: 685 | smooth loss: 3.59758\n",
      "Best model at iteration: 686 | smooth loss: 3.59680\n",
      "Best model at iteration: 687 | smooth loss: 3.59589\n",
      "Best model at iteration: 688 | smooth loss: 3.59456\n",
      "Best model at iteration: 689 | smooth loss: 3.59370\n",
      "Best model at iteration: 690 | smooth loss: 3.59303\n",
      "Best model at iteration: 691 | smooth loss: 3.59221\n",
      "Best model at iteration: 692 | smooth loss: 3.59109\n",
      "Best model at iteration: 693 | smooth loss: 3.58977\n",
      "Best model at iteration: 694 | smooth loss: 3.58852\n",
      "Best model at iteration: 695 | smooth loss: 3.58752\n",
      "Best model at iteration: 696 | smooth loss: 3.58651\n",
      "Best model at iteration: 697 | smooth loss: 3.58545\n",
      "Best model at iteration: 698 | smooth loss: 3.58472\n",
      "Best model at iteration: 699 | smooth loss: 3.58424\n",
      "Best model at iteration: 700 | smooth loss: 3.58360\n",
      "Best model at iteration: 701 | smooth loss: 3.58289\n",
      "Best model at iteration: 702 | smooth loss: 3.58178\n",
      "Best model at iteration: 703 | smooth loss: 3.58055\n",
      "Best model at iteration: 704 | smooth loss: 3.57929\n",
      "Best model at iteration: 705 | smooth loss: 3.57869\n",
      "Best model at iteration: 706 | smooth loss: 3.57742\n",
      "Best model at iteration: 707 | smooth loss: 3.57659\n",
      "Best model at iteration: 708 | smooth loss: 3.57526\n",
      "Best model at iteration: 709 | smooth loss: 3.57426\n",
      "Best model at iteration: 710 | smooth loss: 3.57328\n",
      "Best model at iteration: 711 | smooth loss: 3.57179\n",
      "Best model at iteration: 712 | smooth loss: 3.57070\n",
      "Best model at iteration: 713 | smooth loss: 3.56955\n",
      "Best model at iteration: 714 | smooth loss: 3.56832\n",
      "Best model at iteration: 715 | smooth loss: 3.56741\n",
      "Best model at iteration: 716 | smooth loss: 3.56639\n",
      "Best model at iteration: 717 | smooth loss: 3.56536\n",
      "Best model at iteration: 718 | smooth loss: 3.56403\n",
      "Best model at iteration: 719 | smooth loss: 3.56334\n",
      "Best model at iteration: 720 | smooth loss: 3.56199\n",
      "Best model at iteration: 721 | smooth loss: 3.56108\n",
      "Best model at iteration: 722 | smooth loss: 3.56015\n",
      "Best model at iteration: 723 | smooth loss: 3.55861\n",
      "Best model at iteration: 724 | smooth loss: 3.55748\n",
      "Best model at iteration: 725 | smooth loss: 3.55607\n",
      "Best model at iteration: 726 | smooth loss: 3.55489\n",
      "Best model at iteration: 727 | smooth loss: 3.55415\n",
      "Best model at iteration: 728 | smooth loss: 3.55282\n",
      "Best model at iteration: 729 | smooth loss: 3.55170\n",
      "Best model at iteration: 730 | smooth loss: 3.55049\n",
      "Best model at iteration: 731 | smooth loss: 3.54930\n",
      "Best model at iteration: 732 | smooth loss: 3.54828\n",
      "Best model at iteration: 733 | smooth loss: 3.54752\n",
      "Best model at iteration: 734 | smooth loss: 3.54711\n",
      "Best model at iteration: 735 | smooth loss: 3.54610\n",
      "Best model at iteration: 736 | smooth loss: 3.54489\n",
      "Best model at iteration: 737 | smooth loss: 3.54369\n",
      "Best model at iteration: 738 | smooth loss: 3.54305\n",
      "Best model at iteration: 739 | smooth loss: 3.54234\n",
      "Best model at iteration: 740 | smooth loss: 3.54119\n",
      "Best model at iteration: 741 | smooth loss: 3.54001\n",
      "Best model at iteration: 742 | smooth loss: 3.53905\n",
      "Best model at iteration: 743 | smooth loss: 3.53801\n",
      "Best model at iteration: 744 | smooth loss: 3.53667\n",
      "Best model at iteration: 745 | smooth loss: 3.53555\n",
      "Best model at iteration: 746 | smooth loss: 3.53428\n",
      "Best model at iteration: 747 | smooth loss: 3.53308\n",
      "Best model at iteration: 748 | smooth loss: 3.53198\n",
      "Best model at iteration: 749 | smooth loss: 3.53092\n",
      "Best model at iteration: 750 | smooth loss: 3.52968\n",
      "Best model at iteration: 751 | smooth loss: 3.52882\n",
      "Best model at iteration: 752 | smooth loss: 3.52776\n",
      "Best model at iteration: 753 | smooth loss: 3.52729\n",
      "Best model at iteration: 754 | smooth loss: 3.52567\n",
      "Best model at iteration: 755 | smooth loss: 3.52434\n",
      "Best model at iteration: 756 | smooth loss: 3.52335\n",
      "Best model at iteration: 757 | smooth loss: 3.52230\n",
      "Best model at iteration: 758 | smooth loss: 3.52144\n",
      "Best model at iteration: 759 | smooth loss: 3.52019\n",
      "Best model at iteration: 760 | smooth loss: 3.51939\n",
      "Best model at iteration: 761 | smooth loss: 3.51866\n",
      "Best model at iteration: 762 | smooth loss: 3.51801\n",
      "Best model at iteration: 763 | smooth loss: 3.51682\n",
      "Best model at iteration: 764 | smooth loss: 3.51609\n",
      "Best model at iteration: 765 | smooth loss: 3.51507\n",
      "Best model at iteration: 766 | smooth loss: 3.51379\n",
      "Best model at iteration: 767 | smooth loss: 3.51273\n",
      "Best model at iteration: 768 | smooth loss: 3.51162\n",
      "Best model at iteration: 769 | smooth loss: 3.51095\n",
      "Best model at iteration: 770 | smooth loss: 3.50951\n",
      "Best model at iteration: 771 | smooth loss: 3.50833\n",
      "Best model at iteration: 772 | smooth loss: 3.50733\n",
      "Best model at iteration: 773 | smooth loss: 3.50581\n",
      "Best model at iteration: 774 | smooth loss: 3.50512\n",
      "Best model at iteration: 775 | smooth loss: 3.50391\n",
      "Best model at iteration: 776 | smooth loss: 3.50275\n",
      "Best model at iteration: 777 | smooth loss: 3.50156\n",
      "Best model at iteration: 778 | smooth loss: 3.50130\n",
      "Best model at iteration: 779 | smooth loss: 3.50020\n",
      "Best model at iteration: 780 | smooth loss: 3.49895\n",
      "Best model at iteration: 781 | smooth loss: 3.49766\n",
      "Best model at iteration: 782 | smooth loss: 3.49620\n",
      "Best model at iteration: 783 | smooth loss: 3.49485\n",
      "Best model at iteration: 784 | smooth loss: 3.49454\n",
      "Best model at iteration: 785 | smooth loss: 3.49331\n",
      "Best model at iteration: 786 | smooth loss: 3.49213\n",
      "Best model at iteration: 787 | smooth loss: 3.49115\n",
      "Best model at iteration: 788 | smooth loss: 3.48936\n",
      "Best model at iteration: 789 | smooth loss: 3.48829\n",
      "Best model at iteration: 790 | smooth loss: 3.48687\n",
      "Best model at iteration: 791 | smooth loss: 3.48561\n",
      "Best model at iteration: 792 | smooth loss: 3.48407\n",
      "Best model at iteration: 793 | smooth loss: 3.48335\n",
      "Best model at iteration: 794 | smooth loss: 3.48300\n",
      "Best model at iteration: 795 | smooth loss: 3.48181\n",
      "Best model at iteration: 796 | smooth loss: 3.48100\n",
      "Best model at iteration: 797 | smooth loss: 3.47961\n",
      "Best model at iteration: 798 | smooth loss: 3.47822\n",
      "Best model at iteration: 799 | smooth loss: 3.47686\n",
      "Best model at iteration: 800 | smooth loss: 3.47607\n",
      "Best model at iteration: 801 | smooth loss: 3.47490\n",
      "Best model at iteration: 802 | smooth loss: 3.47409\n",
      "Best model at iteration: 803 | smooth loss: 3.47285\n",
      "Best model at iteration: 804 | smooth loss: 3.47140\n",
      "Best model at iteration: 805 | smooth loss: 3.47026\n",
      "Best model at iteration: 806 | smooth loss: 3.46958\n",
      "Best model at iteration: 807 | smooth loss: 3.46906\n",
      "Best model at iteration: 808 | smooth loss: 3.46798\n",
      "Best model at iteration: 809 | smooth loss: 3.46700\n",
      "Best model at iteration: 810 | smooth loss: 3.46654\n",
      "Best model at iteration: 811 | smooth loss: 3.46585\n",
      "Best model at iteration: 812 | smooth loss: 3.46478\n",
      "Best model at iteration: 813 | smooth loss: 3.46349\n",
      "Best model at iteration: 814 | smooth loss: 3.46261\n",
      "Best model at iteration: 815 | smooth loss: 3.46158\n",
      "Best model at iteration: 816 | smooth loss: 3.46034\n",
      "Best model at iteration: 817 | smooth loss: 3.45956\n",
      "Best model at iteration: 818 | smooth loss: 3.45855\n",
      "Best model at iteration: 819 | smooth loss: 3.45746\n",
      "Best model at iteration: 820 | smooth loss: 3.45600\n",
      "Best model at iteration: 821 | smooth loss: 3.45502\n",
      "Best model at iteration: 822 | smooth loss: 3.45389\n",
      "Best model at iteration: 823 | smooth loss: 3.45300\n",
      "Best model at iteration: 824 | smooth loss: 3.45199\n",
      "Best model at iteration: 825 | smooth loss: 3.45100\n",
      "Best model at iteration: 826 | smooth loss: 3.45024\n",
      "Best model at iteration: 827 | smooth loss: 3.44911\n",
      "Best model at iteration: 828 | smooth loss: 3.44783\n",
      "Best model at iteration: 829 | smooth loss: 3.44690\n",
      "Best model at iteration: 830 | smooth loss: 3.44559\n",
      "Best model at iteration: 831 | smooth loss: 3.44471\n",
      "Best model at iteration: 832 | smooth loss: 3.44349\n",
      "Best model at iteration: 833 | smooth loss: 3.44276\n",
      "Best model at iteration: 834 | smooth loss: 3.44141\n",
      "Best model at iteration: 835 | smooth loss: 3.44029\n",
      "Best model at iteration: 836 | smooth loss: 3.43920\n",
      "Best model at iteration: 837 | smooth loss: 3.43836\n",
      "Best model at iteration: 838 | smooth loss: 3.43756\n",
      "Best model at iteration: 839 | smooth loss: 3.43639\n",
      "Best model at iteration: 840 | smooth loss: 3.43568\n",
      "Best model at iteration: 841 | smooth loss: 3.43468\n",
      "Best model at iteration: 842 | smooth loss: 3.43364\n",
      "Best model at iteration: 843 | smooth loss: 3.43233\n",
      "Best model at iteration: 844 | smooth loss: 3.43112\n",
      "Best model at iteration: 845 | smooth loss: 3.43011\n",
      "Best model at iteration: 846 | smooth loss: 3.42900\n",
      "Best model at iteration: 847 | smooth loss: 3.42831\n",
      "Best model at iteration: 848 | smooth loss: 3.42694\n",
      "Best model at iteration: 849 | smooth loss: 3.42571\n",
      "Best model at iteration: 850 | smooth loss: 3.42459\n",
      "Best model at iteration: 851 | smooth loss: 3.42355\n",
      "Best model at iteration: 852 | smooth loss: 3.42234\n",
      "Best model at iteration: 853 | smooth loss: 3.42130\n",
      "Best model at iteration: 854 | smooth loss: 3.42014\n",
      "Best model at iteration: 855 | smooth loss: 3.41954\n",
      "Best model at iteration: 856 | smooth loss: 3.41806\n",
      "Best model at iteration: 857 | smooth loss: 3.41713\n",
      "Best model at iteration: 858 | smooth loss: 3.41663\n",
      "Best model at iteration: 859 | smooth loss: 3.41511\n",
      "Best model at iteration: 860 | smooth loss: 3.41352\n",
      "Best model at iteration: 861 | smooth loss: 3.41221\n",
      "Best model at iteration: 862 | smooth loss: 3.41124\n",
      "Best model at iteration: 863 | smooth loss: 3.41066\n",
      "Best model at iteration: 864 | smooth loss: 3.40931\n",
      "Best model at iteration: 865 | smooth loss: 3.40866\n",
      "Best model at iteration: 866 | smooth loss: 3.40743\n",
      "Best model at iteration: 867 | smooth loss: 3.40620\n",
      "Best model at iteration: 868 | smooth loss: 3.40559\n",
      "Best model at iteration: 869 | smooth loss: 3.40473\n",
      "Best model at iteration: 870 | smooth loss: 3.40361\n",
      "Best model at iteration: 871 | smooth loss: 3.40318\n",
      "Best model at iteration: 872 | smooth loss: 3.40240\n",
      "Best model at iteration: 873 | smooth loss: 3.40137\n",
      "Best model at iteration: 874 | smooth loss: 3.40055\n",
      "Best model at iteration: 875 | smooth loss: 3.39977\n",
      "Best model at iteration: 876 | smooth loss: 3.39920\n",
      "Best model at iteration: 877 | smooth loss: 3.39819\n",
      "Best model at iteration: 878 | smooth loss: 3.39779\n",
      "Best model at iteration: 879 | smooth loss: 3.39725\n",
      "Best model at iteration: 880 | smooth loss: 3.39621\n",
      "Best model at iteration: 881 | smooth loss: 3.39535\n",
      "Best model at iteration: 882 | smooth loss: 3.39408\n",
      "Best model at iteration: 883 | smooth loss: 3.39401\n",
      "Best model at iteration: 884 | smooth loss: 3.39262\n",
      "Best model at iteration: 885 | smooth loss: 3.39152\n",
      "Best model at iteration: 886 | smooth loss: 3.39077\n",
      "Best model at iteration: 887 | smooth loss: 3.39006\n",
      "Best model at iteration: 888 | smooth loss: 3.38997\n",
      "Best model at iteration: 889 | smooth loss: 3.38896\n",
      "Best model at iteration: 890 | smooth loss: 3.38800\n",
      "Best model at iteration: 891 | smooth loss: 3.38711\n",
      "Best model at iteration: 892 | smooth loss: 3.38643\n",
      "Best model at iteration: 893 | smooth loss: 3.38546\n",
      "Best model at iteration: 894 | smooth loss: 3.38454\n",
      "Best model at iteration: 895 | smooth loss: 3.38375\n",
      "Best model at iteration: 896 | smooth loss: 3.38280\n",
      "Best model at iteration: 897 | smooth loss: 3.38189\n",
      "Best model at iteration: 898 | smooth loss: 3.38072\n",
      "Best model at iteration: 899 | smooth loss: 3.37976\n",
      "Best model at iteration: 900 | smooth loss: 3.37864\n",
      "Best model at iteration: 901 | smooth loss: 3.37801\n",
      "Best model at iteration: 902 | smooth loss: 3.37716\n",
      "Best model at iteration: 903 | smooth loss: 3.37634\n",
      "Best model at iteration: 904 | smooth loss: 3.37543\n",
      "Best model at iteration: 905 | smooth loss: 3.37429\n",
      "Best model at iteration: 906 | smooth loss: 3.37351\n",
      "Best model at iteration: 907 | smooth loss: 3.37266\n",
      "Best model at iteration: 908 | smooth loss: 3.37125\n",
      "Best model at iteration: 909 | smooth loss: 3.37021\n",
      "Best model at iteration: 910 | smooth loss: 3.36885\n",
      "Best model at iteration: 911 | smooth loss: 3.36790\n",
      "Best model at iteration: 912 | smooth loss: 3.36678\n",
      "Best model at iteration: 913 | smooth loss: 3.36572\n",
      "Best model at iteration: 914 | smooth loss: 3.36458\n",
      "Best model at iteration: 915 | smooth loss: 3.36415\n",
      "Best model at iteration: 916 | smooth loss: 3.36324\n",
      "Best model at iteration: 917 | smooth loss: 3.36193\n",
      "Best model at iteration: 918 | smooth loss: 3.36139\n",
      "Best model at iteration: 919 | smooth loss: 3.35986\n",
      "Best model at iteration: 920 | smooth loss: 3.35842\n",
      "Best model at iteration: 921 | smooth loss: 3.35737\n",
      "Best model at iteration: 922 | smooth loss: 3.35618\n",
      "Best model at iteration: 923 | smooth loss: 3.35542\n",
      "Best model at iteration: 924 | smooth loss: 3.35428\n",
      "Best model at iteration: 925 | smooth loss: 3.35348\n",
      "Best model at iteration: 926 | smooth loss: 3.35238\n",
      "Best model at iteration: 927 | smooth loss: 3.35089\n",
      "Best model at iteration: 928 | smooth loss: 3.34944\n",
      "Best model at iteration: 929 | smooth loss: 3.34836\n",
      "Best model at iteration: 930 | smooth loss: 3.34772\n",
      "Best model at iteration: 931 | smooth loss: 3.34678\n",
      "Best model at iteration: 932 | smooth loss: 3.34607\n",
      "Best model at iteration: 933 | smooth loss: 3.34493\n",
      "Best model at iteration: 934 | smooth loss: 3.34419\n",
      "Best model at iteration: 935 | smooth loss: 3.34321\n",
      "Best model at iteration: 936 | smooth loss: 3.34245\n",
      "Best model at iteration: 940 | smooth loss: 3.34206\n",
      "Best model at iteration: 941 | smooth loss: 3.34137\n",
      "Best model at iteration: 942 | smooth loss: 3.34080\n",
      "Best model at iteration: 943 | smooth loss: 3.33983\n",
      "Best model at iteration: 944 | smooth loss: 3.33875\n",
      "Best model at iteration: 945 | smooth loss: 3.33760\n",
      "Best model at iteration: 946 | smooth loss: 3.33720\n",
      "Best model at iteration: 947 | smooth loss: 3.33616\n",
      "Best model at iteration: 948 | smooth loss: 3.33496\n",
      "Best model at iteration: 949 | smooth loss: 3.33436\n",
      "Best model at iteration: 950 | smooth loss: 3.33359\n",
      "Best model at iteration: 951 | smooth loss: 3.33266\n",
      "Best model at iteration: 952 | smooth loss: 3.33149\n",
      "Best model at iteration: 953 | smooth loss: 3.33008\n",
      "Best model at iteration: 954 | smooth loss: 3.32921\n",
      "Best model at iteration: 955 | smooth loss: 3.32804\n",
      "Best model at iteration: 956 | smooth loss: 3.32715\n",
      "Best model at iteration: 957 | smooth loss: 3.32614\n",
      "Best model at iteration: 958 | smooth loss: 3.32566\n",
      "Best model at iteration: 959 | smooth loss: 3.32496\n",
      "Best model at iteration: 960 | smooth loss: 3.32365\n",
      "Best model at iteration: 961 | smooth loss: 3.32252\n",
      "Best model at iteration: 962 | smooth loss: 3.32160\n",
      "Best model at iteration: 963 | smooth loss: 3.32064\n",
      "Best model at iteration: 964 | smooth loss: 3.31956\n",
      "Best model at iteration: 965 | smooth loss: 3.31868\n",
      "Best model at iteration: 966 | smooth loss: 3.31757\n",
      "Best model at iteration: 967 | smooth loss: 3.31686\n",
      "Best model at iteration: 968 | smooth loss: 3.31594\n",
      "Best model at iteration: 969 | smooth loss: 3.31474\n",
      "Best model at iteration: 970 | smooth loss: 3.31363\n",
      "Best model at iteration: 971 | smooth loss: 3.31228\n",
      "Best model at iteration: 972 | smooth loss: 3.31157\n",
      "Best model at iteration: 973 | smooth loss: 3.31067\n",
      "Best model at iteration: 974 | smooth loss: 3.31054\n",
      "Best model at iteration: 975 | smooth loss: 3.30958\n",
      "Best model at iteration: 976 | smooth loss: 3.30861\n",
      "Best model at iteration: 977 | smooth loss: 3.30806\n",
      "Best model at iteration: 978 | smooth loss: 3.30742\n",
      "Best model at iteration: 979 | smooth loss: 3.30650\n",
      "Best model at iteration: 980 | smooth loss: 3.30533\n",
      "Best model at iteration: 981 | smooth loss: 3.30453\n",
      "Best model at iteration: 982 | smooth loss: 3.30358\n",
      "Best model at iteration: 983 | smooth loss: 3.30266\n",
      "Best model at iteration: 984 | smooth loss: 3.30190\n",
      "Best model at iteration: 985 | smooth loss: 3.30097\n",
      "Best model at iteration: 986 | smooth loss: 3.29992\n",
      "Best model at iteration: 987 | smooth loss: 3.29957\n",
      "Best model at iteration: 988 | smooth loss: 3.29884\n",
      "Best model at iteration: 989 | smooth loss: 3.29787\n",
      "Best model at iteration: 990 | smooth loss: 3.29702\n",
      "Best model at iteration: 991 | smooth loss: 3.29606\n",
      "Best model at iteration: 992 | smooth loss: 3.29566\n",
      "Best model at iteration: 993 | smooth loss: 3.29502\n",
      "Best model at iteration: 994 | smooth loss: 3.29435\n",
      "Best model at iteration: 995 | smooth loss: 3.29358\n",
      "Best model at iteration: 996 | smooth loss: 3.29279\n",
      "Best model at iteration: 997 | smooth loss: 3.29168\n",
      "Best model at iteration: 998 | smooth loss: 3.29094\n",
      "Best model at iteration: 999 | smooth loss: 3.29004\n",
      "Best model at iteration: 1000 | smooth loss: 3.28888\n",
      "iter = 1000, smooth loss=3.2888840560009993\n",
      "e, and. fDbt caice heancx. ho wisi), mtore,of uet he lreio\n",
      "\tURy by oog,.\n",
      "\tAnekeis anker, hud la mithe formatfelhed fato...\"\" Ih orghe moarcvvertoule, soo wathe R. lf ase...\tR, hedceltbe sran,d di. pTt\n",
      "\n",
      "Best model at iteration: 1001 | smooth loss: 3.28804\n",
      "Best model at iteration: 1002 | smooth loss: 3.28732\n",
      "Best model at iteration: 1003 | smooth loss: 3.28639\n",
      "Best model at iteration: 1004 | smooth loss: 3.28587\n",
      "Best model at iteration: 1005 | smooth loss: 3.28494\n",
      "Best model at iteration: 1006 | smooth loss: 3.28424\n",
      "Best model at iteration: 1007 | smooth loss: 3.28332\n",
      "Best model at iteration: 1008 | smooth loss: 3.28216\n",
      "Best model at iteration: 1009 | smooth loss: 3.28097\n",
      "Best model at iteration: 1010 | smooth loss: 3.28009\n",
      "Best model at iteration: 1011 | smooth loss: 3.27935\n",
      "Best model at iteration: 1012 | smooth loss: 3.27795\n",
      "Best model at iteration: 1013 | smooth loss: 3.27705\n",
      "Best model at iteration: 1014 | smooth loss: 3.27601\n",
      "Best model at iteration: 1015 | smooth loss: 3.27500\n",
      "Best model at iteration: 1016 | smooth loss: 3.27400\n",
      "Best model at iteration: 1017 | smooth loss: 3.27285\n",
      "Best model at iteration: 1018 | smooth loss: 3.27195\n",
      "Best model at iteration: 1019 | smooth loss: 3.27121\n",
      "Best model at iteration: 1020 | smooth loss: 3.27020\n",
      "Best model at iteration: 1021 | smooth loss: 3.26929\n",
      "Best model at iteration: 1022 | smooth loss: 3.26807\n",
      "Best model at iteration: 1023 | smooth loss: 3.26727\n",
      "Best model at iteration: 1024 | smooth loss: 3.26628\n",
      "Best model at iteration: 1025 | smooth loss: 3.26531\n",
      "Best model at iteration: 1026 | smooth loss: 3.26465\n",
      "Best model at iteration: 1027 | smooth loss: 3.26401\n",
      "Best model at iteration: 1028 | smooth loss: 3.26283\n",
      "Best model at iteration: 1029 | smooth loss: 3.26207\n",
      "Best model at iteration: 1030 | smooth loss: 3.26075\n",
      "Best model at iteration: 1031 | smooth loss: 3.25986\n",
      "Best model at iteration: 1032 | smooth loss: 3.25893\n",
      "Best model at iteration: 1033 | smooth loss: 3.25783\n",
      "Best model at iteration: 1034 | smooth loss: 3.25673\n",
      "Best model at iteration: 1035 | smooth loss: 3.25575\n",
      "Best model at iteration: 1036 | smooth loss: 3.25541\n",
      "Best model at iteration: 1037 | smooth loss: 3.25425\n",
      "Best model at iteration: 1038 | smooth loss: 3.25322\n",
      "Best model at iteration: 1039 | smooth loss: 3.25206\n",
      "Best model at iteration: 1040 | smooth loss: 3.25115\n",
      "Best model at iteration: 1041 | smooth loss: 3.25014\n",
      "Best model at iteration: 1042 | smooth loss: 3.24910\n",
      "Best model at iteration: 1043 | smooth loss: 3.24876\n",
      "Best model at iteration: 1044 | smooth loss: 3.24792\n",
      "Best model at iteration: 1045 | smooth loss: 3.24729\n",
      "Best model at iteration: 1046 | smooth loss: 3.24620\n",
      "Best model at iteration: 1047 | smooth loss: 3.24516\n",
      "Best model at iteration: 1048 | smooth loss: 3.24479\n",
      "Best model at iteration: 1049 | smooth loss: 3.24390\n",
      "Best model at iteration: 1050 | smooth loss: 3.24361\n",
      "Best model at iteration: 1051 | smooth loss: 3.24264\n",
      "Best model at iteration: 1052 | smooth loss: 3.24145\n",
      "Best model at iteration: 1053 | smooth loss: 3.24084\n",
      "Best model at iteration: 1054 | smooth loss: 3.24012\n",
      "Best model at iteration: 1055 | smooth loss: 3.23996\n",
      "Best model at iteration: 1056 | smooth loss: 3.23910\n",
      "Best model at iteration: 1057 | smooth loss: 3.23827\n",
      "Best model at iteration: 1058 | smooth loss: 3.23759\n",
      "Best model at iteration: 1059 | smooth loss: 3.23668\n",
      "Best model at iteration: 1060 | smooth loss: 3.23605\n",
      "Best model at iteration: 1061 | smooth loss: 3.23473\n",
      "Best model at iteration: 1062 | smooth loss: 3.23408\n",
      "Best model at iteration: 1063 | smooth loss: 3.23329\n",
      "Best model at iteration: 1064 | smooth loss: 3.23212\n",
      "Best model at iteration: 1065 | smooth loss: 3.23163\n",
      "Best model at iteration: 1066 | smooth loss: 3.23048\n",
      "Best model at iteration: 1067 | smooth loss: 3.22920\n",
      "Best model at iteration: 1068 | smooth loss: 3.22831\n",
      "Best model at iteration: 1069 | smooth loss: 3.22740\n",
      "Best model at iteration: 1070 | smooth loss: 3.22696\n",
      "Best model at iteration: 1071 | smooth loss: 3.22578\n",
      "Best model at iteration: 1073 | smooth loss: 3.22471\n",
      "Best model at iteration: 1075 | smooth loss: 3.22429\n",
      "Best model at iteration: 1076 | smooth loss: 3.22326\n",
      "Best model at iteration: 1077 | smooth loss: 3.22265\n",
      "Best model at iteration: 1078 | smooth loss: 3.22152\n",
      "Best model at iteration: 1079 | smooth loss: 3.22062\n",
      "Best model at iteration: 1080 | smooth loss: 3.21992\n",
      "Best model at iteration: 1081 | smooth loss: 3.21872\n",
      "Best model at iteration: 1082 | smooth loss: 3.21778\n",
      "Best model at iteration: 1083 | smooth loss: 3.21705\n",
      "Best model at iteration: 1084 | smooth loss: 3.21695\n",
      "Best model at iteration: 1085 | smooth loss: 3.21670\n",
      "Best model at iteration: 1086 | smooth loss: 3.21596\n",
      "Best model at iteration: 1087 | smooth loss: 3.21506\n",
      "Best model at iteration: 1088 | smooth loss: 3.21452\n",
      "Best model at iteration: 1089 | smooth loss: 3.21352\n",
      "Best model at iteration: 1090 | smooth loss: 3.21263\n",
      "Best model at iteration: 1091 | smooth loss: 3.21149\n",
      "Best model at iteration: 1092 | smooth loss: 3.21083\n",
      "Best model at iteration: 1093 | smooth loss: 3.21008\n",
      "Best model at iteration: 1094 | smooth loss: 3.20968\n",
      "Best model at iteration: 1095 | smooth loss: 3.20909\n",
      "Best model at iteration: 1096 | smooth loss: 3.20769\n",
      "Best model at iteration: 1097 | smooth loss: 3.20693\n",
      "Best model at iteration: 1098 | smooth loss: 3.20623\n",
      "Best model at iteration: 1099 | smooth loss: 3.20503\n",
      "Best model at iteration: 1100 | smooth loss: 3.20419\n",
      "Best model at iteration: 1101 | smooth loss: 3.20309\n",
      "Best model at iteration: 1102 | smooth loss: 3.20217\n",
      "Best model at iteration: 1103 | smooth loss: 3.20111\n",
      "Best model at iteration: 1104 | smooth loss: 3.19997\n",
      "Best model at iteration: 1105 | smooth loss: 3.19977\n",
      "Best model at iteration: 1106 | smooth loss: 3.19867\n",
      "Best model at iteration: 1107 | smooth loss: 3.19829\n",
      "Best model at iteration: 1108 | smooth loss: 3.19767\n",
      "Best model at iteration: 1109 | smooth loss: 3.19702\n",
      "Best model at iteration: 1110 | smooth loss: 3.19624\n",
      "Best model at iteration: 1111 | smooth loss: 3.19567\n",
      "Best model at iteration: 1112 | smooth loss: 3.19485\n",
      "Best model at iteration: 1113 | smooth loss: 3.19429\n",
      "Best model at iteration: 1114 | smooth loss: 3.19388\n",
      "Best model at iteration: 1115 | smooth loss: 3.19345\n",
      "Best model at iteration: 1116 | smooth loss: 3.19328\n",
      "Best model at iteration: 1117 | smooth loss: 3.19270\n",
      "Best model at iteration: 1118 | smooth loss: 3.19190\n",
      "Best model at iteration: 1119 | smooth loss: 3.19107\n",
      "Best model at iteration: 1120 | smooth loss: 3.18977\n",
      "Best model at iteration: 1121 | smooth loss: 3.18854\n",
      "Best model at iteration: 1122 | smooth loss: 3.18760\n",
      "Best model at iteration: 1123 | smooth loss: 3.18690\n",
      "Best model at iteration: 1124 | smooth loss: 3.18679\n",
      "Best model at iteration: 1125 | smooth loss: 3.18608\n",
      "Best model at iteration: 1126 | smooth loss: 3.18558\n",
      "Best model at iteration: 1127 | smooth loss: 3.18515\n",
      "Best model at iteration: 1128 | smooth loss: 3.18464\n",
      "Best model at iteration: 1129 | smooth loss: 3.18382\n",
      "Best model at iteration: 1130 | smooth loss: 3.18273\n",
      "Best model at iteration: 1131 | smooth loss: 3.18189\n",
      "Best model at iteration: 1132 | smooth loss: 3.18097\n",
      "Best model at iteration: 1133 | smooth loss: 3.18032\n",
      "Best model at iteration: 1134 | smooth loss: 3.17962\n",
      "Best model at iteration: 1135 | smooth loss: 3.17863\n",
      "Best model at iteration: 1136 | smooth loss: 3.17831\n",
      "Best model at iteration: 1137 | smooth loss: 3.17769\n",
      "Best model at iteration: 1138 | smooth loss: 3.17681\n",
      "Best model at iteration: 1139 | smooth loss: 3.17627\n",
      "Best model at iteration: 1140 | smooth loss: 3.17524\n",
      "Best model at iteration: 1141 | smooth loss: 3.17409\n",
      "Best model at iteration: 1142 | smooth loss: 3.17397\n",
      "Best model at iteration: 1143 | smooth loss: 3.17383\n",
      "Best model at iteration: 1144 | smooth loss: 3.17274\n",
      "Best model at iteration: 1145 | smooth loss: 3.17210\n",
      "Best model at iteration: 1146 | smooth loss: 3.17147\n",
      "Best model at iteration: 1147 | smooth loss: 3.17056\n",
      "Best model at iteration: 1148 | smooth loss: 3.16989\n",
      "Best model at iteration: 1149 | smooth loss: 3.16903\n",
      "Best model at iteration: 1150 | smooth loss: 3.16795\n",
      "Best model at iteration: 1151 | smooth loss: 3.16764\n",
      "Best model at iteration: 1152 | smooth loss: 3.16733\n",
      "Best model at iteration: 1153 | smooth loss: 3.16667\n",
      "Best model at iteration: 1154 | smooth loss: 3.16581\n",
      "Best model at iteration: 1155 | smooth loss: 3.16516\n",
      "Best model at iteration: 1156 | smooth loss: 3.16447\n",
      "Best model at iteration: 1157 | smooth loss: 3.16341\n",
      "Best model at iteration: 1158 | smooth loss: 3.16250\n",
      "Best model at iteration: 1159 | smooth loss: 3.16173\n",
      "Best model at iteration: 1160 | smooth loss: 3.16110\n",
      "Best model at iteration: 1161 | smooth loss: 3.16069\n",
      "Best model at iteration: 1162 | smooth loss: 3.15965\n",
      "Best model at iteration: 1163 | smooth loss: 3.15879\n",
      "Best model at iteration: 1164 | smooth loss: 3.15834\n",
      "Best model at iteration: 1165 | smooth loss: 3.15818\n",
      "Best model at iteration: 1166 | smooth loss: 3.15792\n",
      "Best model at iteration: 1167 | smooth loss: 3.15723\n",
      "Best model at iteration: 1168 | smooth loss: 3.15642\n",
      "Best model at iteration: 1169 | smooth loss: 3.15548\n",
      "Best model at iteration: 1170 | smooth loss: 3.15519\n",
      "Best model at iteration: 1171 | smooth loss: 3.15417\n",
      "Best model at iteration: 1172 | smooth loss: 3.15342\n",
      "Best model at iteration: 1173 | smooth loss: 3.15230\n",
      "Best model at iteration: 1174 | smooth loss: 3.15101\n",
      "Best model at iteration: 1175 | smooth loss: 3.15028\n",
      "Best model at iteration: 1176 | smooth loss: 3.14919\n",
      "Best model at iteration: 1177 | smooth loss: 3.14810\n",
      "Best model at iteration: 1178 | smooth loss: 3.14706\n",
      "Best model at iteration: 1179 | smooth loss: 3.14621\n",
      "Best model at iteration: 1180 | smooth loss: 3.14507\n",
      "Best model at iteration: 1181 | smooth loss: 3.14386\n",
      "Best model at iteration: 1182 | smooth loss: 3.14286\n",
      "Best model at iteration: 1183 | smooth loss: 3.14221\n",
      "Best model at iteration: 1184 | smooth loss: 3.14144\n",
      "Best model at iteration: 1185 | smooth loss: 3.14064\n",
      "Best model at iteration: 1186 | smooth loss: 3.13936\n",
      "Best model at iteration: 1187 | smooth loss: 3.13853\n",
      "Best model at iteration: 1188 | smooth loss: 3.13765\n",
      "Best model at iteration: 1189 | smooth loss: 3.13704\n",
      "Best model at iteration: 1190 | smooth loss: 3.13591\n",
      "Best model at iteration: 1191 | smooth loss: 3.13498\n",
      "Best model at iteration: 1192 | smooth loss: 3.13406\n",
      "Best model at iteration: 1193 | smooth loss: 3.13344\n",
      "Best model at iteration: 1194 | smooth loss: 3.13234\n",
      "Best model at iteration: 1195 | smooth loss: 3.13132\n",
      "Best model at iteration: 1196 | smooth loss: 3.13055\n",
      "Best model at iteration: 1197 | smooth loss: 3.12953\n",
      "Best model at iteration: 1198 | smooth loss: 3.12909\n",
      "Best model at iteration: 1199 | smooth loss: 3.12871\n",
      "Best model at iteration: 1200 | smooth loss: 3.12785\n",
      "Best model at iteration: 1201 | smooth loss: 3.12748\n",
      "Best model at iteration: 1202 | smooth loss: 3.12640\n",
      "Best model at iteration: 1203 | smooth loss: 3.12522\n",
      "Best model at iteration: 1204 | smooth loss: 3.12394\n",
      "Best model at iteration: 1205 | smooth loss: 3.12283\n",
      "Best model at iteration: 1206 | smooth loss: 3.12175\n",
      "Best model at iteration: 1207 | smooth loss: 3.12083\n",
      "Best model at iteration: 1208 | smooth loss: 3.12047\n",
      "Best model at iteration: 1209 | smooth loss: 3.11964\n",
      "Best model at iteration: 1210 | smooth loss: 3.11878\n",
      "Best model at iteration: 1211 | smooth loss: 3.11800\n",
      "Best model at iteration: 1212 | smooth loss: 3.11705\n",
      "Best model at iteration: 1213 | smooth loss: 3.11605\n",
      "Best model at iteration: 1214 | smooth loss: 3.11537\n",
      "Best model at iteration: 1215 | smooth loss: 3.11415\n",
      "Best model at iteration: 1216 | smooth loss: 3.11345\n",
      "Best model at iteration: 1217 | smooth loss: 3.11270\n",
      "Best model at iteration: 1218 | smooth loss: 3.11161\n",
      "Best model at iteration: 1219 | smooth loss: 3.11062\n",
      "Best model at iteration: 1220 | smooth loss: 3.10992\n",
      "Best model at iteration: 1221 | smooth loss: 3.10948\n",
      "Best model at iteration: 1222 | smooth loss: 3.10861\n",
      "Best model at iteration: 1223 | smooth loss: 3.10780\n",
      "Best model at iteration: 1224 | smooth loss: 3.10714\n",
      "Best model at iteration: 1225 | smooth loss: 3.10598\n",
      "Best model at iteration: 1226 | smooth loss: 3.10505\n",
      "Best model at iteration: 1227 | smooth loss: 3.10426\n",
      "Best model at iteration: 1228 | smooth loss: 3.10323\n",
      "Best model at iteration: 1229 | smooth loss: 3.10251\n",
      "Best model at iteration: 1230 | smooth loss: 3.10155\n",
      "Best model at iteration: 1231 | smooth loss: 3.10073\n",
      "Best model at iteration: 1232 | smooth loss: 3.09990\n",
      "Best model at iteration: 1233 | smooth loss: 3.09933\n",
      "Best model at iteration: 1234 | smooth loss: 3.09838\n",
      "Best model at iteration: 1235 | smooth loss: 3.09741\n",
      "Best model at iteration: 1236 | smooth loss: 3.09629\n",
      "Best model at iteration: 1237 | smooth loss: 3.09540\n",
      "Best model at iteration: 1238 | smooth loss: 3.09429\n",
      "Best model at iteration: 1239 | smooth loss: 3.09358\n",
      "Best model at iteration: 1240 | smooth loss: 3.09251\n",
      "Best model at iteration: 1241 | smooth loss: 3.09161\n",
      "Best model at iteration: 1242 | smooth loss: 3.09058\n",
      "Best model at iteration: 1243 | smooth loss: 3.09021\n",
      "Best model at iteration: 1244 | smooth loss: 3.08917\n",
      "Best model at iteration: 1245 | smooth loss: 3.08799\n",
      "Best model at iteration: 1246 | smooth loss: 3.08739\n",
      "Best model at iteration: 1247 | smooth loss: 3.08639\n",
      "Best model at iteration: 1248 | smooth loss: 3.08551\n",
      "Best model at iteration: 1249 | smooth loss: 3.08474\n",
      "Best model at iteration: 1250 | smooth loss: 3.08370\n",
      "Best model at iteration: 1251 | smooth loss: 3.08314\n",
      "Best model at iteration: 1252 | smooth loss: 3.08244\n",
      "Best model at iteration: 1253 | smooth loss: 3.08176\n",
      "Best model at iteration: 1254 | smooth loss: 3.08058\n",
      "Best model at iteration: 1255 | smooth loss: 3.07977\n",
      "Best model at iteration: 1256 | smooth loss: 3.07922\n",
      "Best model at iteration: 1257 | smooth loss: 3.07822\n",
      "Best model at iteration: 1258 | smooth loss: 3.07764\n",
      "Best model at iteration: 1259 | smooth loss: 3.07673\n",
      "Best model at iteration: 1260 | smooth loss: 3.07537\n",
      "Best model at iteration: 1261 | smooth loss: 3.07463\n",
      "Best model at iteration: 1262 | smooth loss: 3.07448\n",
      "Best model at iteration: 1263 | smooth loss: 3.07338\n",
      "Best model at iteration: 1264 | smooth loss: 3.07302\n",
      "Best model at iteration: 1265 | smooth loss: 3.07226\n",
      "Best model at iteration: 1266 | smooth loss: 3.07164\n",
      "Best model at iteration: 1267 | smooth loss: 3.07149\n",
      "Best model at iteration: 1268 | smooth loss: 3.07123\n",
      "Best model at iteration: 1270 | smooth loss: 3.07099\n",
      "Best model at iteration: 1271 | smooth loss: 3.07012\n",
      "Best model at iteration: 1272 | smooth loss: 3.06934\n",
      "Best model at iteration: 1273 | smooth loss: 3.06890\n",
      "Best model at iteration: 1275 | smooth loss: 3.06834\n",
      "Best model at iteration: 1276 | smooth loss: 3.06752\n",
      "Best model at iteration: 1277 | smooth loss: 3.06701\n",
      "Best model at iteration: 1278 | smooth loss: 3.06589\n",
      "Best model at iteration: 1279 | smooth loss: 3.06515\n",
      "Best model at iteration: 1281 | smooth loss: 3.06474\n",
      "Best model at iteration: 1282 | smooth loss: 3.06399\n",
      "Best model at iteration: 1283 | smooth loss: 3.06285\n",
      "Best model at iteration: 1284 | smooth loss: 3.06230\n",
      "Best model at iteration: 1285 | smooth loss: 3.06176\n",
      "Best model at iteration: 1286 | smooth loss: 3.06138\n",
      "Best model at iteration: 1287 | smooth loss: 3.06079\n",
      "Best model at iteration: 1288 | smooth loss: 3.05987\n",
      "Best model at iteration: 1289 | smooth loss: 3.05955\n",
      "Best model at iteration: 1290 | smooth loss: 3.05945\n",
      "Best model at iteration: 1291 | smooth loss: 3.05854\n",
      "Best model at iteration: 1292 | smooth loss: 3.05761\n",
      "Best model at iteration: 1293 | smooth loss: 3.05729\n",
      "Best model at iteration: 1294 | smooth loss: 3.05642\n",
      "Best model at iteration: 1295 | smooth loss: 3.05597\n",
      "Best model at iteration: 1296 | smooth loss: 3.05522\n",
      "Best model at iteration: 1297 | smooth loss: 3.05462\n",
      "Best model at iteration: 1298 | smooth loss: 3.05408\n",
      "Best model at iteration: 1299 | smooth loss: 3.05338\n",
      "Best model at iteration: 1300 | smooth loss: 3.05257\n",
      "Best model at iteration: 1301 | smooth loss: 3.05235\n",
      "Best model at iteration: 1302 | smooth loss: 3.05176\n",
      "Best model at iteration: 1303 | smooth loss: 3.05100\n",
      "Best model at iteration: 1304 | smooth loss: 3.05029\n",
      "Best model at iteration: 1305 | smooth loss: 3.04916\n",
      "Best model at iteration: 1306 | smooth loss: 3.04845\n",
      "Best model at iteration: 1307 | smooth loss: 3.04773\n",
      "Best model at iteration: 1308 | smooth loss: 3.04719\n",
      "Best model at iteration: 1309 | smooth loss: 3.04663\n",
      "Best model at iteration: 1310 | smooth loss: 3.04594\n",
      "Best model at iteration: 1311 | smooth loss: 3.04512\n",
      "Best model at iteration: 1312 | smooth loss: 3.04506\n",
      "Best model at iteration: 1313 | smooth loss: 3.04418\n",
      "Best model at iteration: 1314 | smooth loss: 3.04354\n",
      "Best model at iteration: 1315 | smooth loss: 3.04258\n",
      "Best model at iteration: 1316 | smooth loss: 3.04198\n",
      "Best model at iteration: 1317 | smooth loss: 3.04085\n",
      "Best model at iteration: 1318 | smooth loss: 3.04017\n",
      "Best model at iteration: 1319 | smooth loss: 3.03933\n",
      "Best model at iteration: 1320 | smooth loss: 3.03921\n",
      "Best model at iteration: 1321 | smooth loss: 3.03850\n",
      "Best model at iteration: 1322 | smooth loss: 3.03783\n",
      "Best model at iteration: 1323 | smooth loss: 3.03759\n",
      "Best model at iteration: 1324 | smooth loss: 3.03683\n",
      "Best model at iteration: 1325 | smooth loss: 3.03625\n",
      "Best model at iteration: 1326 | smooth loss: 3.03584\n",
      "Best model at iteration: 1329 | smooth loss: 3.03566\n",
      "Best model at iteration: 1330 | smooth loss: 3.03512\n",
      "Best model at iteration: 1331 | smooth loss: 3.03457\n",
      "Best model at iteration: 1332 | smooth loss: 3.03419\n",
      "Best model at iteration: 1333 | smooth loss: 3.03358\n",
      "Best model at iteration: 1334 | smooth loss: 3.03337\n",
      "Best model at iteration: 1335 | smooth loss: 3.03334\n",
      "Best model at iteration: 1336 | smooth loss: 3.03293\n",
      "Best model at iteration: 1337 | smooth loss: 3.03210\n",
      "Best model at iteration: 1338 | smooth loss: 3.03188\n",
      "Best model at iteration: 1339 | smooth loss: 3.03171\n",
      "Best model at iteration: 1340 | smooth loss: 3.03129\n",
      "Best model at iteration: 1341 | smooth loss: 3.03103\n",
      "Best model at iteration: 1342 | smooth loss: 3.03019\n",
      "Best model at iteration: 1343 | smooth loss: 3.02935\n",
      "Best model at iteration: 1344 | smooth loss: 3.02858\n",
      "Best model at iteration: 1345 | smooth loss: 3.02802\n",
      "Best model at iteration: 1346 | smooth loss: 3.02747\n",
      "Best model at iteration: 1347 | smooth loss: 3.02623\n",
      "Best model at iteration: 1348 | smooth loss: 3.02590\n",
      "Best model at iteration: 1349 | smooth loss: 3.02589\n",
      "Best model at iteration: 1350 | smooth loss: 3.02496\n",
      "Best model at iteration: 1351 | smooth loss: 3.02408\n",
      "Best model at iteration: 1353 | smooth loss: 3.02384\n",
      "Best model at iteration: 1354 | smooth loss: 3.02293\n",
      "Best model at iteration: 1355 | smooth loss: 3.02203\n",
      "Best model at iteration: 1356 | smooth loss: 3.02139\n",
      "Best model at iteration: 1357 | smooth loss: 3.02035\n",
      "Best model at iteration: 1358 | smooth loss: 3.01949\n",
      "Best model at iteration: 1359 | smooth loss: 3.01869\n",
      "Best model at iteration: 1360 | smooth loss: 3.01836\n",
      "Best model at iteration: 1361 | smooth loss: 3.01750\n",
      "Best model at iteration: 1362 | smooth loss: 3.01729\n",
      "Best model at iteration: 1363 | smooth loss: 3.01674\n",
      "Best model at iteration: 1364 | smooth loss: 3.01648\n",
      "Best model at iteration: 1365 | smooth loss: 3.01609\n",
      "Best model at iteration: 1366 | smooth loss: 3.01521\n",
      "Best model at iteration: 1367 | smooth loss: 3.01460\n",
      "Best model at iteration: 1368 | smooth loss: 3.01386\n",
      "Best model at iteration: 1369 | smooth loss: 3.01328\n",
      "Best model at iteration: 1370 | smooth loss: 3.01301\n",
      "Best model at iteration: 1371 | smooth loss: 3.01275\n",
      "Best model at iteration: 1372 | smooth loss: 3.01261\n",
      "Best model at iteration: 1373 | smooth loss: 3.01193\n",
      "Best model at iteration: 1374 | smooth loss: 3.01120\n",
      "Best model at iteration: 1375 | smooth loss: 3.01057\n",
      "Best model at iteration: 1376 | smooth loss: 3.00960\n",
      "Best model at iteration: 1377 | smooth loss: 3.00941\n",
      "Best model at iteration: 1378 | smooth loss: 3.00833\n",
      "Best model at iteration: 1379 | smooth loss: 3.00754\n",
      "Best model at iteration: 1380 | smooth loss: 3.00728\n",
      "Best model at iteration: 1381 | smooth loss: 3.00645\n",
      "Best model at iteration: 1382 | smooth loss: 3.00579\n",
      "Best model at iteration: 1383 | smooth loss: 3.00546\n",
      "Best model at iteration: 1384 | smooth loss: 3.00475\n",
      "Best model at iteration: 1385 | smooth loss: 3.00389\n",
      "Best model at iteration: 1386 | smooth loss: 3.00297\n",
      "Best model at iteration: 1387 | smooth loss: 3.00280\n",
      "Best model at iteration: 1388 | smooth loss: 3.00231\n",
      "Best model at iteration: 1389 | smooth loss: 3.00171\n",
      "Best model at iteration: 1390 | smooth loss: 3.00128\n",
      "Best model at iteration: 1391 | smooth loss: 3.00040\n",
      "Best model at iteration: 1392 | smooth loss: 2.99988\n",
      "Best model at iteration: 1393 | smooth loss: 2.99888\n",
      "Best model at iteration: 1394 | smooth loss: 2.99789\n",
      "Best model at iteration: 1395 | smooth loss: 2.99762\n",
      "Best model at iteration: 1396 | smooth loss: 2.99700\n",
      "Best model at iteration: 1397 | smooth loss: 2.99650\n",
      "Best model at iteration: 1398 | smooth loss: 2.99549\n",
      "Best model at iteration: 1399 | smooth loss: 2.99484\n",
      "Best model at iteration: 1400 | smooth loss: 2.99440\n",
      "Best model at iteration: 1401 | smooth loss: 2.99390\n",
      "Best model at iteration: 1402 | smooth loss: 2.99360\n",
      "Best model at iteration: 1403 | smooth loss: 2.99312\n",
      "Best model at iteration: 1404 | smooth loss: 2.99253\n",
      "Best model at iteration: 1405 | smooth loss: 2.99243\n",
      "Best model at iteration: 1406 | smooth loss: 2.99201\n",
      "Best model at iteration: 1407 | smooth loss: 2.99148\n",
      "Best model at iteration: 1408 | smooth loss: 2.99136\n",
      "Best model at iteration: 1409 | smooth loss: 2.99053\n",
      "Best model at iteration: 1410 | smooth loss: 2.99007\n",
      "Best model at iteration: 1411 | smooth loss: 2.98951\n",
      "Best model at iteration: 1412 | smooth loss: 2.98879\n",
      "Best model at iteration: 1413 | smooth loss: 2.98813\n",
      "Best model at iteration: 1414 | smooth loss: 2.98750\n",
      "Best model at iteration: 1415 | smooth loss: 2.98690\n",
      "Best model at iteration: 1416 | smooth loss: 2.98641\n",
      "Best model at iteration: 1417 | smooth loss: 2.98581\n",
      "Best model at iteration: 1418 | smooth loss: 2.98556\n",
      "Best model at iteration: 1419 | smooth loss: 2.98463\n",
      "Best model at iteration: 1420 | smooth loss: 2.98432\n",
      "Best model at iteration: 1421 | smooth loss: 2.98401\n",
      "Best model at iteration: 1422 | smooth loss: 2.98311\n",
      "Best model at iteration: 1423 | smooth loss: 2.98268\n",
      "Best model at iteration: 1424 | smooth loss: 2.98226\n",
      "Best model at iteration: 1425 | smooth loss: 2.98150\n",
      "Best model at iteration: 1426 | smooth loss: 2.98108\n",
      "Best model at iteration: 1427 | smooth loss: 2.97985\n",
      "Best model at iteration: 1428 | smooth loss: 2.97899\n",
      "Best model at iteration: 1429 | smooth loss: 2.97842\n",
      "Best model at iteration: 1430 | smooth loss: 2.97781\n",
      "Best model at iteration: 1431 | smooth loss: 2.97683\n",
      "Best model at iteration: 1432 | smooth loss: 2.97573\n",
      "Best model at iteration: 1433 | smooth loss: 2.97505\n",
      "Best model at iteration: 1434 | smooth loss: 2.97456\n",
      "Best model at iteration: 1435 | smooth loss: 2.97358\n",
      "Best model at iteration: 1436 | smooth loss: 2.97300\n",
      "Best model at iteration: 1437 | smooth loss: 2.97189\n",
      "Best model at iteration: 1438 | smooth loss: 2.97092\n",
      "Best model at iteration: 1439 | smooth loss: 2.97005\n",
      "Best model at iteration: 1440 | smooth loss: 2.96978\n",
      "Best model at iteration: 1441 | smooth loss: 2.96930\n",
      "Best model at iteration: 1442 | smooth loss: 2.96820\n",
      "Best model at iteration: 1443 | smooth loss: 2.96709\n",
      "Best model at iteration: 1444 | smooth loss: 2.96627\n",
      "Best model at iteration: 1445 | smooth loss: 2.96542\n",
      "Best model at iteration: 1446 | smooth loss: 2.96465\n",
      "Best model at iteration: 1447 | smooth loss: 2.96373\n",
      "Best model at iteration: 1448 | smooth loss: 2.96298\n",
      "Best model at iteration: 1449 | smooth loss: 2.96204\n",
      "Best model at iteration: 1450 | smooth loss: 2.96140\n",
      "Best model at iteration: 1451 | smooth loss: 2.96092\n",
      "Best model at iteration: 1452 | smooth loss: 2.95982\n",
      "Best model at iteration: 1453 | smooth loss: 2.95909\n",
      "Best model at iteration: 1454 | smooth loss: 2.95820\n",
      "Best model at iteration: 1455 | smooth loss: 2.95733\n",
      "Best model at iteration: 1456 | smooth loss: 2.95657\n",
      "Best model at iteration: 1457 | smooth loss: 2.95564\n",
      "Best model at iteration: 1458 | smooth loss: 2.95463\n",
      "Best model at iteration: 1459 | smooth loss: 2.95384\n",
      "Best model at iteration: 1460 | smooth loss: 2.95345\n",
      "Best model at iteration: 1461 | smooth loss: 2.95290\n",
      "Best model at iteration: 1462 | smooth loss: 2.95225\n",
      "Best model at iteration: 1463 | smooth loss: 2.95137\n",
      "Best model at iteration: 1464 | smooth loss: 2.95032\n",
      "Best model at iteration: 1465 | smooth loss: 2.94968\n",
      "Best model at iteration: 1466 | smooth loss: 2.94898\n",
      "Best model at iteration: 1467 | smooth loss: 2.94837\n",
      "Best model at iteration: 1468 | smooth loss: 2.94775\n",
      "Best model at iteration: 1469 | smooth loss: 2.94654\n",
      "Best model at iteration: 1470 | smooth loss: 2.94518\n",
      "Best model at iteration: 1471 | smooth loss: 2.94458\n",
      "Best model at iteration: 1472 | smooth loss: 2.94410\n",
      "Best model at iteration: 1473 | smooth loss: 2.94386\n",
      "Best model at iteration: 1474 | smooth loss: 2.94290\n",
      "Best model at iteration: 1475 | smooth loss: 2.94194\n",
      "Best model at iteration: 1476 | smooth loss: 2.94145\n",
      "Best model at iteration: 1477 | smooth loss: 2.94046\n",
      "Best model at iteration: 1478 | smooth loss: 2.94004\n",
      "Best model at iteration: 1479 | smooth loss: 2.93920\n",
      "Best model at iteration: 1480 | smooth loss: 2.93917\n",
      "Best model at iteration: 1481 | smooth loss: 2.93883\n",
      "Best model at iteration: 1482 | smooth loss: 2.93852\n",
      "Best model at iteration: 1483 | smooth loss: 2.93820\n",
      "Best model at iteration: 1484 | smooth loss: 2.93762\n",
      "Best model at iteration: 1485 | smooth loss: 2.93726\n",
      "Best model at iteration: 1486 | smooth loss: 2.93682\n",
      "Best model at iteration: 1487 | smooth loss: 2.93611\n",
      "Best model at iteration: 1488 | smooth loss: 2.93551\n",
      "Best model at iteration: 1489 | smooth loss: 2.93458\n",
      "Best model at iteration: 1490 | smooth loss: 2.93360\n",
      "Best model at iteration: 1491 | smooth loss: 2.93282\n",
      "Best model at iteration: 1492 | smooth loss: 2.93211\n",
      "Best model at iteration: 1493 | smooth loss: 2.93107\n",
      "Best model at iteration: 1494 | smooth loss: 2.93066\n",
      "Best model at iteration: 1495 | smooth loss: 2.92977\n",
      "Best model at iteration: 1496 | smooth loss: 2.92929\n",
      "Best model at iteration: 1497 | smooth loss: 2.92905\n",
      "Best model at iteration: 1498 | smooth loss: 2.92834\n",
      "Best model at iteration: 1499 | smooth loss: 2.92773\n",
      "Best model at iteration: 1500 | smooth loss: 2.92706\n",
      "Best model at iteration: 1501 | smooth loss: 2.92691\n",
      "Best model at iteration: 1502 | smooth loss: 2.92588\n",
      "Best model at iteration: 1503 | smooth loss: 2.92541\n",
      "Best model at iteration: 1504 | smooth loss: 2.92473\n",
      "Best model at iteration: 1505 | smooth loss: 2.92419\n",
      "Best model at iteration: 1507 | smooth loss: 2.92329\n",
      "Best model at iteration: 1508 | smooth loss: 2.92265\n",
      "Best model at iteration: 1509 | smooth loss: 2.92138\n",
      "Best model at iteration: 1510 | smooth loss: 2.92053\n",
      "Best model at iteration: 1511 | smooth loss: 2.91975\n",
      "Best model at iteration: 1512 | smooth loss: 2.91918\n",
      "Best model at iteration: 1513 | smooth loss: 2.91858\n",
      "Best model at iteration: 1514 | smooth loss: 2.91809\n",
      "Best model at iteration: 1515 | smooth loss: 2.91733\n",
      "Best model at iteration: 1516 | smooth loss: 2.91694\n",
      "Best model at iteration: 1517 | smooth loss: 2.91663\n",
      "Best model at iteration: 1518 | smooth loss: 2.91547\n",
      "Best model at iteration: 1519 | smooth loss: 2.91478\n",
      "Best model at iteration: 1520 | smooth loss: 2.91379\n",
      "Best model at iteration: 1521 | smooth loss: 2.91309\n",
      "Best model at iteration: 1522 | smooth loss: 2.91266\n",
      "Best model at iteration: 1524 | smooth loss: 2.91172\n",
      "Best model at iteration: 1525 | smooth loss: 2.91143\n",
      "Best model at iteration: 1526 | smooth loss: 2.91067\n",
      "Best model at iteration: 1527 | smooth loss: 2.91034\n",
      "Best model at iteration: 1528 | smooth loss: 2.90979\n",
      "Best model at iteration: 1529 | smooth loss: 2.90915\n",
      "Best model at iteration: 1530 | smooth loss: 2.90847\n",
      "Best model at iteration: 1531 | smooth loss: 2.90839\n",
      "Best model at iteration: 1532 | smooth loss: 2.90783\n",
      "Best model at iteration: 1533 | smooth loss: 2.90761\n",
      "Best model at iteration: 1534 | smooth loss: 2.90740\n",
      "Best model at iteration: 1535 | smooth loss: 2.90683\n",
      "Best model at iteration: 1536 | smooth loss: 2.90630\n",
      "Best model at iteration: 1537 | smooth loss: 2.90573\n",
      "Best model at iteration: 1538 | smooth loss: 2.90522\n",
      "Best model at iteration: 1539 | smooth loss: 2.90473\n",
      "Best model at iteration: 1540 | smooth loss: 2.90423\n",
      "Best model at iteration: 1541 | smooth loss: 2.90405\n",
      "Best model at iteration: 1542 | smooth loss: 2.90339\n",
      "Best model at iteration: 1543 | smooth loss: 2.90290\n",
      "Best model at iteration: 1544 | smooth loss: 2.90230\n",
      "Best model at iteration: 1545 | smooth loss: 2.90194\n",
      "Best model at iteration: 1546 | smooth loss: 2.90148\n",
      "Best model at iteration: 1548 | smooth loss: 2.90104\n",
      "Best model at iteration: 1549 | smooth loss: 2.90095\n",
      "Best model at iteration: 1550 | smooth loss: 2.90017\n",
      "Best model at iteration: 1551 | smooth loss: 2.89989\n",
      "Best model at iteration: 1552 | smooth loss: 2.89910\n",
      "Best model at iteration: 1553 | smooth loss: 2.89890\n",
      "Best model at iteration: 1554 | smooth loss: 2.89795\n",
      "Best model at iteration: 1555 | smooth loss: 2.89763\n",
      "Best model at iteration: 1556 | smooth loss: 2.89693\n",
      "Best model at iteration: 1557 | smooth loss: 2.89637\n",
      "Best model at iteration: 1558 | smooth loss: 2.89600\n",
      "Best model at iteration: 1559 | smooth loss: 2.89560\n",
      "Best model at iteration: 1560 | smooth loss: 2.89546\n",
      "Best model at iteration: 1561 | smooth loss: 2.89515\n",
      "Best model at iteration: 1562 | smooth loss: 2.89474\n",
      "Best model at iteration: 1563 | smooth loss: 2.89471\n",
      "Best model at iteration: 1564 | smooth loss: 2.89402\n",
      "Best model at iteration: 1565 | smooth loss: 2.89372\n",
      "Best model at iteration: 1566 | smooth loss: 2.89299\n",
      "Best model at iteration: 1567 | smooth loss: 2.89267\n",
      "Best model at iteration: 1568 | smooth loss: 2.89229\n",
      "Best model at iteration: 1569 | smooth loss: 2.89144\n",
      "Best model at iteration: 1570 | smooth loss: 2.89062\n",
      "Best model at iteration: 1571 | smooth loss: 2.88995\n",
      "Best model at iteration: 1572 | smooth loss: 2.88924\n",
      "Best model at iteration: 1573 | smooth loss: 2.88847\n",
      "Best model at iteration: 1574 | smooth loss: 2.88770\n",
      "Best model at iteration: 1575 | smooth loss: 2.88721\n",
      "Best model at iteration: 1576 | smooth loss: 2.88662\n",
      "Best model at iteration: 1577 | smooth loss: 2.88611\n",
      "Best model at iteration: 1578 | smooth loss: 2.88547\n",
      "Best model at iteration: 1579 | smooth loss: 2.88457\n",
      "Best model at iteration: 1580 | smooth loss: 2.88438\n",
      "Best model at iteration: 1581 | smooth loss: 2.88366\n",
      "Best model at iteration: 1582 | smooth loss: 2.88300\n",
      "Best model at iteration: 1583 | smooth loss: 2.88238\n",
      "Best model at iteration: 1601 | smooth loss: 2.88235\n",
      "Best model at iteration: 1602 | smooth loss: 2.88194\n",
      "Best model at iteration: 1603 | smooth loss: 2.88161\n",
      "Best model at iteration: 1604 | smooth loss: 2.88088\n",
      "Best model at iteration: 1605 | smooth loss: 2.88025\n",
      "Best model at iteration: 1606 | smooth loss: 2.87981\n",
      "Best model at iteration: 1608 | smooth loss: 2.87980\n",
      "Best model at iteration: 1609 | smooth loss: 2.87978\n",
      "Best model at iteration: 1610 | smooth loss: 2.87927\n",
      "Best model at iteration: 1611 | smooth loss: 2.87899\n",
      "Best model at iteration: 1612 | smooth loss: 2.87895\n",
      "Best model at iteration: 1613 | smooth loss: 2.87807\n",
      "Best model at iteration: 1614 | smooth loss: 2.87777\n",
      "Best model at iteration: 1615 | smooth loss: 2.87686\n",
      "Best model at iteration: 1616 | smooth loss: 2.87595\n",
      "Best model at iteration: 1623 | smooth loss: 2.87544\n",
      "Best model at iteration: 1624 | smooth loss: 2.87523\n",
      "Best model at iteration: 1625 | smooth loss: 2.87446\n",
      "Best model at iteration: 1626 | smooth loss: 2.87430\n",
      "Best model at iteration: 1627 | smooth loss: 2.87362\n",
      "Best model at iteration: 1628 | smooth loss: 2.87360\n",
      "Best model at iteration: 1629 | smooth loss: 2.87336\n",
      "Best model at iteration: 1630 | smooth loss: 2.87302\n",
      "Best model at iteration: 1631 | smooth loss: 2.87239\n",
      "Best model at iteration: 1633 | smooth loss: 2.87186\n",
      "Best model at iteration: 1634 | smooth loss: 2.87173\n",
      "Best model at iteration: 1635 | smooth loss: 2.87150\n",
      "Best model at iteration: 1636 | smooth loss: 2.87112\n",
      "Best model at iteration: 1637 | smooth loss: 2.87020\n",
      "Best model at iteration: 1639 | smooth loss: 2.86980\n",
      "Best model at iteration: 1640 | smooth loss: 2.86932\n",
      "Best model at iteration: 1641 | smooth loss: 2.86875\n",
      "Best model at iteration: 1642 | smooth loss: 2.86851\n",
      "Best model at iteration: 1643 | smooth loss: 2.86845\n",
      "Best model at iteration: 1644 | smooth loss: 2.86780\n",
      "Best model at iteration: 1645 | smooth loss: 2.86672\n",
      "Best model at iteration: 1646 | smooth loss: 2.86618\n",
      "Best model at iteration: 1647 | smooth loss: 2.86512\n",
      "Best model at iteration: 1648 | smooth loss: 2.86464\n",
      "Best model at iteration: 1649 | smooth loss: 2.86402\n",
      "Best model at iteration: 1650 | smooth loss: 2.86351\n",
      "Best model at iteration: 1652 | smooth loss: 2.86314\n",
      "Best model at iteration: 1653 | smooth loss: 2.86284\n",
      "Best model at iteration: 1654 | smooth loss: 2.86199\n",
      "Best model at iteration: 1655 | smooth loss: 2.86188\n",
      "Best model at iteration: 1656 | smooth loss: 2.86134\n",
      "Best model at iteration: 1657 | smooth loss: 2.86095\n",
      "Best model at iteration: 1658 | smooth loss: 2.86033\n",
      "Best model at iteration: 1659 | smooth loss: 2.85935\n",
      "Best model at iteration: 1660 | smooth loss: 2.85913\n",
      "Best model at iteration: 1661 | smooth loss: 2.85886\n",
      "Best model at iteration: 1662 | smooth loss: 2.85876\n",
      "Best model at iteration: 1663 | smooth loss: 2.85808\n",
      "Best model at iteration: 1664 | smooth loss: 2.85756\n",
      "Best model at iteration: 1665 | smooth loss: 2.85718\n",
      "Best model at iteration: 1667 | smooth loss: 2.85682\n",
      "Best model at iteration: 1668 | smooth loss: 2.85642\n",
      "Best model at iteration: 1669 | smooth loss: 2.85586\n",
      "Best model at iteration: 1670 | smooth loss: 2.85549\n",
      "Best model at iteration: 1671 | smooth loss: 2.85475\n",
      "Best model at iteration: 1672 | smooth loss: 2.85379\n",
      "Best model at iteration: 1673 | smooth loss: 2.85355\n",
      "Best model at iteration: 1674 | smooth loss: 2.85260\n",
      "Best model at iteration: 1675 | smooth loss: 2.85184\n",
      "Best model at iteration: 1676 | smooth loss: 2.85149\n",
      "Best model at iteration: 1677 | smooth loss: 2.85146\n",
      "Best model at iteration: 1678 | smooth loss: 2.85115\n",
      "Best model at iteration: 1679 | smooth loss: 2.85031\n",
      "Best model at iteration: 1680 | smooth loss: 2.84973\n",
      "Best model at iteration: 1681 | smooth loss: 2.84892\n",
      "Best model at iteration: 1682 | smooth loss: 2.84815\n",
      "Best model at iteration: 1684 | smooth loss: 2.84792\n",
      "Best model at iteration: 1685 | smooth loss: 2.84745\n",
      "Best model at iteration: 1686 | smooth loss: 2.84669\n",
      "Best model at iteration: 1687 | smooth loss: 2.84600\n",
      "Best model at iteration: 1688 | smooth loss: 2.84549\n",
      "Best model at iteration: 1689 | smooth loss: 2.84545\n",
      "Best model at iteration: 1690 | smooth loss: 2.84475\n",
      "Best model at iteration: 1691 | smooth loss: 2.84410\n",
      "Best model at iteration: 1692 | smooth loss: 2.84363\n",
      "Best model at iteration: 1693 | smooth loss: 2.84252\n",
      "Best model at iteration: 1694 | smooth loss: 2.84220\n",
      "Best model at iteration: 1695 | smooth loss: 2.84157\n",
      "Best model at iteration: 1696 | smooth loss: 2.84074\n",
      "Best model at iteration: 1697 | smooth loss: 2.83991\n",
      "Best model at iteration: 1698 | smooth loss: 2.83967\n",
      "Best model at iteration: 1699 | smooth loss: 2.83902\n",
      "Best model at iteration: 1700 | smooth loss: 2.83855\n",
      "Best model at iteration: 1701 | smooth loss: 2.83811\n",
      "Best model at iteration: 1702 | smooth loss: 2.83705\n",
      "Best model at iteration: 1703 | smooth loss: 2.83608\n",
      "Best model at iteration: 1704 | smooth loss: 2.83512\n",
      "Best model at iteration: 1705 | smooth loss: 2.83376\n",
      "Best model at iteration: 1706 | smooth loss: 2.83318\n",
      "Best model at iteration: 1707 | smooth loss: 2.83289\n",
      "Best model at iteration: 1708 | smooth loss: 2.83200\n",
      "Best model at iteration: 1709 | smooth loss: 2.83160\n",
      "Best model at iteration: 1710 | smooth loss: 2.83070\n",
      "Best model at iteration: 1711 | smooth loss: 2.83053\n",
      "Best model at iteration: 1712 | smooth loss: 2.83015\n",
      "Best model at iteration: 1713 | smooth loss: 2.82980\n",
      "Best model at iteration: 1714 | smooth loss: 2.82977\n",
      "Best model at iteration: 1715 | smooth loss: 2.82965\n",
      "Best model at iteration: 1716 | smooth loss: 2.82952\n",
      "Best model at iteration: 1718 | smooth loss: 2.82942\n",
      "Best model at iteration: 1719 | smooth loss: 2.82890\n",
      "Best model at iteration: 1720 | smooth loss: 2.82847\n",
      "Best model at iteration: 1721 | smooth loss: 2.82803\n",
      "Best model at iteration: 1722 | smooth loss: 2.82715\n",
      "Best model at iteration: 1723 | smooth loss: 2.82702\n",
      "Best model at iteration: 1725 | smooth loss: 2.82640\n",
      "Best model at iteration: 1726 | smooth loss: 2.82611\n",
      "Best model at iteration: 1727 | smooth loss: 2.82592\n",
      "Best model at iteration: 1728 | smooth loss: 2.82537\n",
      "Best model at iteration: 1729 | smooth loss: 2.82508\n",
      "Best model at iteration: 1730 | smooth loss: 2.82432\n",
      "Best model at iteration: 1731 | smooth loss: 2.82399\n",
      "Best model at iteration: 1732 | smooth loss: 2.82338\n",
      "Best model at iteration: 1733 | smooth loss: 2.82260\n",
      "Best model at iteration: 1734 | smooth loss: 2.82235\n",
      "Best model at iteration: 1735 | smooth loss: 2.82217\n",
      "Best model at iteration: 1736 | smooth loss: 2.82164\n",
      "Best model at iteration: 1738 | smooth loss: 2.82125\n",
      "Best model at iteration: 1739 | smooth loss: 2.82027\n",
      "Best model at iteration: 1740 | smooth loss: 2.81979\n",
      "Best model at iteration: 1741 | smooth loss: 2.81898\n",
      "Best model at iteration: 1742 | smooth loss: 2.81879\n",
      "Best model at iteration: 1743 | smooth loss: 2.81843\n",
      "Best model at iteration: 1744 | smooth loss: 2.81824\n",
      "Best model at iteration: 1745 | smooth loss: 2.81818\n",
      "Best model at iteration: 1746 | smooth loss: 2.81769\n",
      "Best model at iteration: 1748 | smooth loss: 2.81719\n",
      "Best model at iteration: 1752 | smooth loss: 2.81709\n",
      "Best model at iteration: 1753 | smooth loss: 2.81631\n",
      "Best model at iteration: 1754 | smooth loss: 2.81573\n",
      "Best model at iteration: 1755 | smooth loss: 2.81504\n",
      "Best model at iteration: 1756 | smooth loss: 2.81494\n",
      "Best model at iteration: 1757 | smooth loss: 2.81401\n",
      "Best model at iteration: 1758 | smooth loss: 2.81325\n",
      "Best model at iteration: 1761 | smooth loss: 2.81281\n",
      "Best model at iteration: 1762 | smooth loss: 2.81235\n",
      "Best model at iteration: 1763 | smooth loss: 2.81190\n",
      "Best model at iteration: 1764 | smooth loss: 2.81172\n",
      "Best model at iteration: 1765 | smooth loss: 2.81114\n",
      "Best model at iteration: 1766 | smooth loss: 2.81072\n",
      "Best model at iteration: 1767 | smooth loss: 2.81026\n",
      "Best model at iteration: 1768 | smooth loss: 2.80943\n",
      "Best model at iteration: 1769 | smooth loss: 2.80889\n",
      "Best model at iteration: 1770 | smooth loss: 2.80798\n",
      "Best model at iteration: 1771 | smooth loss: 2.80779\n",
      "Best model at iteration: 1772 | smooth loss: 2.80719\n",
      "Best model at iteration: 1773 | smooth loss: 2.80633\n",
      "Best model at iteration: 1774 | smooth loss: 2.80568\n",
      "Best model at iteration: 1775 | smooth loss: 2.80490\n",
      "Best model at iteration: 1776 | smooth loss: 2.80424\n",
      "Best model at iteration: 1777 | smooth loss: 2.80320\n",
      "Best model at iteration: 1778 | smooth loss: 2.80268\n",
      "Best model at iteration: 1779 | smooth loss: 2.80228\n",
      "Best model at iteration: 1780 | smooth loss: 2.80158\n",
      "Best model at iteration: 1781 | smooth loss: 2.80103\n",
      "Best model at iteration: 1782 | smooth loss: 2.80046\n",
      "Best model at iteration: 1783 | smooth loss: 2.79987\n",
      "Best model at iteration: 1784 | smooth loss: 2.79918\n",
      "Best model at iteration: 1785 | smooth loss: 2.79913\n",
      "Best model at iteration: 1786 | smooth loss: 2.79810\n",
      "Best model at iteration: 1787 | smooth loss: 2.79792\n",
      "Best model at iteration: 1788 | smooth loss: 2.79709\n",
      "Best model at iteration: 1790 | smooth loss: 2.79649\n",
      "Best model at iteration: 1791 | smooth loss: 2.79563\n",
      "Best model at iteration: 1792 | smooth loss: 2.79478\n",
      "Best model at iteration: 1793 | smooth loss: 2.79420\n",
      "Best model at iteration: 1794 | smooth loss: 2.79324\n",
      "Best model at iteration: 1795 | smooth loss: 2.79268\n",
      "Best model at iteration: 1796 | smooth loss: 2.79180\n",
      "Best model at iteration: 1797 | smooth loss: 2.79138\n",
      "Best model at iteration: 1799 | smooth loss: 2.79062\n",
      "Best model at iteration: 1800 | smooth loss: 2.78960\n",
      "Best model at iteration: 1801 | smooth loss: 2.78894\n",
      "Best model at iteration: 1802 | smooth loss: 2.78834\n",
      "Best model at iteration: 1803 | smooth loss: 2.78802\n",
      "Best model at iteration: 1804 | smooth loss: 2.78714\n",
      "Best model at iteration: 1805 | smooth loss: 2.78695\n",
      "Best model at iteration: 1806 | smooth loss: 2.78639\n",
      "Best model at iteration: 1807 | smooth loss: 2.78578\n",
      "Best model at iteration: 1808 | smooth loss: 2.78481\n",
      "Best model at iteration: 1809 | smooth loss: 2.78448\n",
      "Best model at iteration: 1810 | smooth loss: 2.78392\n",
      "Best model at iteration: 1811 | smooth loss: 2.78329\n",
      "Best model at iteration: 1812 | smooth loss: 2.78274\n",
      "Best model at iteration: 1813 | smooth loss: 2.78241\n",
      "Best model at iteration: 1814 | smooth loss: 2.78215\n",
      "Best model at iteration: 1815 | smooth loss: 2.78149\n",
      "Best model at iteration: 1816 | smooth loss: 2.78118\n",
      "Best model at iteration: 1817 | smooth loss: 2.78064\n",
      "Best model at iteration: 1818 | smooth loss: 2.78018\n",
      "Best model at iteration: 1819 | smooth loss: 2.78001\n",
      "Best model at iteration: 1820 | smooth loss: 2.77945\n",
      "Best model at iteration: 1821 | smooth loss: 2.77871\n",
      "Best model at iteration: 1822 | smooth loss: 2.77850\n",
      "Best model at iteration: 1823 | smooth loss: 2.77788\n",
      "Best model at iteration: 1824 | smooth loss: 2.77769\n",
      "Best model at iteration: 1825 | smooth loss: 2.77653\n",
      "Best model at iteration: 1826 | smooth loss: 2.77566\n",
      "Best model at iteration: 1829 | smooth loss: 2.77561\n",
      "Best model at iteration: 1830 | smooth loss: 2.77525\n",
      "Best model at iteration: 1831 | smooth loss: 2.77479\n",
      "Best model at iteration: 1832 | smooth loss: 2.77464\n",
      "Best model at iteration: 1833 | smooth loss: 2.77409\n",
      "Best model at iteration: 1834 | smooth loss: 2.77365\n",
      "Best model at iteration: 1836 | smooth loss: 2.77344\n",
      "Best model at iteration: 1838 | smooth loss: 2.77306\n",
      "Best model at iteration: 1839 | smooth loss: 2.77283\n",
      "Best model at iteration: 1840 | smooth loss: 2.77239\n",
      "Best model at iteration: 1841 | smooth loss: 2.77230\n",
      "Best model at iteration: 1843 | smooth loss: 2.77157\n",
      "Best model at iteration: 1844 | smooth loss: 2.77097\n",
      "Best model at iteration: 1848 | smooth loss: 2.77046\n",
      "Best model at iteration: 1849 | smooth loss: 2.77011\n",
      "Best model at iteration: 1850 | smooth loss: 2.76953\n",
      "Best model at iteration: 1851 | smooth loss: 2.76891\n",
      "Best model at iteration: 1852 | smooth loss: 2.76787\n",
      "Best model at iteration: 1853 | smooth loss: 2.76739\n",
      "Best model at iteration: 1854 | smooth loss: 2.76676\n",
      "Best model at iteration: 1855 | smooth loss: 2.76655\n",
      "Best model at iteration: 1856 | smooth loss: 2.76539\n",
      "Best model at iteration: 1857 | smooth loss: 2.76503\n",
      "Best model at iteration: 1858 | smooth loss: 2.76447\n",
      "Best model at iteration: 1859 | smooth loss: 2.76420\n",
      "Best model at iteration: 1860 | smooth loss: 2.76387\n",
      "Best model at iteration: 1861 | smooth loss: 2.76329\n",
      "Best model at iteration: 1862 | smooth loss: 2.76298\n",
      "Best model at iteration: 1863 | smooth loss: 2.76288\n",
      "Best model at iteration: 1864 | smooth loss: 2.76224\n",
      "Best model at iteration: 1868 | smooth loss: 2.76202\n",
      "Best model at iteration: 1869 | smooth loss: 2.76113\n",
      "Best model at iteration: 1870 | smooth loss: 2.76051\n",
      "Best model at iteration: 1871 | smooth loss: 2.76025\n",
      "Best model at iteration: 1872 | smooth loss: 2.75983\n",
      "Best model at iteration: 1873 | smooth loss: 2.75963\n",
      "Best model at iteration: 1874 | smooth loss: 2.75935\n",
      "Best model at iteration: 1875 | smooth loss: 2.75829\n",
      "Best model at iteration: 1876 | smooth loss: 2.75795\n",
      "Best model at iteration: 1877 | smooth loss: 2.75743\n",
      "Best model at iteration: 1878 | smooth loss: 2.75727\n",
      "Best model at iteration: 1879 | smooth loss: 2.75688\n",
      "Best model at iteration: 1880 | smooth loss: 2.75660\n",
      "Best model at iteration: 1881 | smooth loss: 2.75619\n",
      "Best model at iteration: 1882 | smooth loss: 2.75613\n",
      "Best model at iteration: 1883 | smooth loss: 2.75583\n",
      "Best model at iteration: 1884 | smooth loss: 2.75569\n",
      "Best model at iteration: 1885 | smooth loss: 2.75470\n",
      "Best model at iteration: 1886 | smooth loss: 2.75422\n",
      "Best model at iteration: 1887 | smooth loss: 2.75401\n",
      "Best model at iteration: 1888 | smooth loss: 2.75372\n",
      "Best model at iteration: 1889 | smooth loss: 2.75335\n",
      "Best model at iteration: 1890 | smooth loss: 2.75313\n",
      "Best model at iteration: 1891 | smooth loss: 2.75280\n",
      "Best model at iteration: 1892 | smooth loss: 2.75200\n",
      "Best model at iteration: 1893 | smooth loss: 2.75128\n",
      "Best model at iteration: 1894 | smooth loss: 2.75048\n",
      "Best model at iteration: 1895 | smooth loss: 2.74998\n",
      "Best model at iteration: 1896 | smooth loss: 2.74938\n",
      "Best model at iteration: 1897 | smooth loss: 2.74891\n",
      "Best model at iteration: 1898 | smooth loss: 2.74851\n",
      "Best model at iteration: 1899 | smooth loss: 2.74770\n",
      "Best model at iteration: 1900 | smooth loss: 2.74733\n",
      "Best model at iteration: 1901 | smooth loss: 2.74669\n",
      "Best model at iteration: 1902 | smooth loss: 2.74584\n",
      "Best model at iteration: 1903 | smooth loss: 2.74532\n",
      "Best model at iteration: 1904 | smooth loss: 2.74482\n",
      "Best model at iteration: 1905 | smooth loss: 2.74442\n",
      "Best model at iteration: 1906 | smooth loss: 2.74390\n",
      "Best model at iteration: 1907 | smooth loss: 2.74321\n",
      "Best model at iteration: 1908 | smooth loss: 2.74224\n",
      "Best model at iteration: 1909 | smooth loss: 2.74196\n",
      "Best model at iteration: 1910 | smooth loss: 2.74195\n",
      "Best model at iteration: 1911 | smooth loss: 2.74161\n",
      "Best model at iteration: 1912 | smooth loss: 2.74091\n",
      "Best model at iteration: 1913 | smooth loss: 2.74016\n",
      "Best model at iteration: 1914 | smooth loss: 2.74004\n",
      "Best model at iteration: 1915 | smooth loss: 2.73993\n",
      "Best model at iteration: 1916 | smooth loss: 2.73940\n",
      "Best model at iteration: 1917 | smooth loss: 2.73869\n",
      "Best model at iteration: 1918 | smooth loss: 2.73851\n",
      "Best model at iteration: 1919 | smooth loss: 2.73769\n",
      "Best model at iteration: 1920 | smooth loss: 2.73684\n",
      "Best model at iteration: 1921 | smooth loss: 2.73623\n",
      "Best model at iteration: 1922 | smooth loss: 2.73553\n",
      "Best model at iteration: 1923 | smooth loss: 2.73543\n",
      "Best model at iteration: 1925 | smooth loss: 2.73518\n",
      "Best model at iteration: 1926 | smooth loss: 2.73462\n",
      "Best model at iteration: 1927 | smooth loss: 2.73417\n",
      "Best model at iteration: 1928 | smooth loss: 2.73345\n",
      "Best model at iteration: 1929 | smooth loss: 2.73262\n",
      "Best model at iteration: 1931 | smooth loss: 2.73228\n",
      "Best model at iteration: 1932 | smooth loss: 2.73212\n",
      "Best model at iteration: 1933 | smooth loss: 2.73197\n",
      "Best model at iteration: 1934 | smooth loss: 2.73107\n",
      "Best model at iteration: 1935 | smooth loss: 2.73037\n",
      "Best model at iteration: 1936 | smooth loss: 2.72972\n",
      "Best model at iteration: 1937 | smooth loss: 2.72939\n",
      "Best model at iteration: 1938 | smooth loss: 2.72895\n",
      "Best model at iteration: 1939 | smooth loss: 2.72829\n",
      "Best model at iteration: 1940 | smooth loss: 2.72768\n",
      "Best model at iteration: 1941 | smooth loss: 2.72685\n",
      "Best model at iteration: 1942 | smooth loss: 2.72656\n",
      "Best model at iteration: 1943 | smooth loss: 2.72654\n",
      "Best model at iteration: 1944 | smooth loss: 2.72572\n",
      "Best model at iteration: 1945 | smooth loss: 2.72466\n",
      "Best model at iteration: 1946 | smooth loss: 2.72380\n",
      "Best model at iteration: 1947 | smooth loss: 2.72328\n",
      "Best model at iteration: 1948 | smooth loss: 2.72266\n",
      "Best model at iteration: 1949 | smooth loss: 2.72214\n",
      "Best model at iteration: 1950 | smooth loss: 2.72178\n",
      "Best model at iteration: 1951 | smooth loss: 2.72117\n",
      "Best model at iteration: 1952 | smooth loss: 2.72093\n",
      "Best model at iteration: 1953 | smooth loss: 2.72085\n",
      "Best model at iteration: 1954 | smooth loss: 2.72031\n",
      "Best model at iteration: 1955 | smooth loss: 2.71980\n",
      "Best model at iteration: 1956 | smooth loss: 2.71953\n",
      "Best model at iteration: 1957 | smooth loss: 2.71905\n",
      "Best model at iteration: 1958 | smooth loss: 2.71859\n",
      "Best model at iteration: 1960 | smooth loss: 2.71841\n",
      "Best model at iteration: 1961 | smooth loss: 2.71764\n",
      "Best model at iteration: 1963 | smooth loss: 2.71709\n",
      "Best model at iteration: 1964 | smooth loss: 2.71646\n",
      "Best model at iteration: 1965 | smooth loss: 2.71614\n",
      "Best model at iteration: 1966 | smooth loss: 2.71564\n",
      "Best model at iteration: 1967 | smooth loss: 2.71527\n",
      "Best model at iteration: 1968 | smooth loss: 2.71460\n",
      "Best model at iteration: 1969 | smooth loss: 2.71446\n",
      "Best model at iteration: 1970 | smooth loss: 2.71408\n",
      "Best model at iteration: 1971 | smooth loss: 2.71389\n",
      "Best model at iteration: 1972 | smooth loss: 2.71360\n",
      "Best model at iteration: 1973 | smooth loss: 2.71293\n",
      "Best model at iteration: 1974 | smooth loss: 2.71233\n",
      "Best model at iteration: 1975 | smooth loss: 2.71204\n",
      "Best model at iteration: 1977 | smooth loss: 2.71161\n",
      "Best model at iteration: 1978 | smooth loss: 2.71102\n",
      "Best model at iteration: 1979 | smooth loss: 2.71046\n",
      "Best model at iteration: 1980 | smooth loss: 2.70982\n",
      "Best model at iteration: 1981 | smooth loss: 2.70891\n",
      "Best model at iteration: 1982 | smooth loss: 2.70774\n",
      "Best model at iteration: 1984 | smooth loss: 2.70728\n",
      "Best model at iteration: 1986 | smooth loss: 2.70722\n",
      "Best model at iteration: 1987 | smooth loss: 2.70697\n",
      "Best model at iteration: 1989 | smooth loss: 2.70693\n",
      "Best model at iteration: 1992 | smooth loss: 2.70618\n",
      "Best model at iteration: 1993 | smooth loss: 2.70597\n",
      "Best model at iteration: 1994 | smooth loss: 2.70567\n",
      "Best model at iteration: 1995 | smooth loss: 2.70544\n",
      "Best model at iteration: 1996 | smooth loss: 2.70494\n",
      "Best model at iteration: 1997 | smooth loss: 2.70413\n",
      "Best model at iteration: 1998 | smooth loss: 2.70398\n",
      "Best model at iteration: 1999 | smooth loss: 2.70375\n",
      "Best model at iteration: 2000 | smooth loss: 2.70300\n",
      "iter = 2000, smooth loss=2.7030020804210664\n",
      "e net lerple whoundhnid thim, shallr.\n",
      " Seren's thisg whan tmirt th futher Harreed bnald.\n",
      "\"B wif taufisn'n ghelid ta ned, mantn hase thinerdtot them-dongdeive\n",
      ",\"tha, aTs thoudedowle tol choul certothe \n",
      "\n",
      "Best model at iteration: 2001 | smooth loss: 2.70216\n",
      "Best model at iteration: 2002 | smooth loss: 2.70189\n",
      "Best model at iteration: 2003 | smooth loss: 2.70165\n",
      "Best model at iteration: 2004 | smooth loss: 2.70136\n",
      "Best model at iteration: 2005 | smooth loss: 2.70112\n",
      "Best model at iteration: 2007 | smooth loss: 2.70062\n",
      "Best model at iteration: 2008 | smooth loss: 2.69972\n",
      "Best model at iteration: 2009 | smooth loss: 2.69943\n",
      "Best model at iteration: 2010 | smooth loss: 2.69940\n",
      "Best model at iteration: 2011 | smooth loss: 2.69922\n",
      "Best model at iteration: 2012 | smooth loss: 2.69827\n",
      "Best model at iteration: 2013 | smooth loss: 2.69780\n",
      "Best model at iteration: 2014 | smooth loss: 2.69748\n",
      "Best model at iteration: 2015 | smooth loss: 2.69743\n",
      "Best model at iteration: 2016 | smooth loss: 2.69712\n",
      "Best model at iteration: 2017 | smooth loss: 2.69643\n",
      "Best model at iteration: 2018 | smooth loss: 2.69587\n",
      "Best model at iteration: 2019 | smooth loss: 2.69539\n",
      "Best model at iteration: 2020 | smooth loss: 2.69504\n",
      "Best model at iteration: 2021 | smooth loss: 2.69470\n",
      "Best model at iteration: 2022 | smooth loss: 2.69445\n",
      "Best model at iteration: 2023 | smooth loss: 2.69391\n",
      "Best model at iteration: 2025 | smooth loss: 2.69344\n",
      "Best model at iteration: 2027 | smooth loss: 2.69283\n",
      "Best model at iteration: 2028 | smooth loss: 2.69273\n",
      "Best model at iteration: 2031 | smooth loss: 2.69232\n",
      "Best model at iteration: 2032 | smooth loss: 2.69164\n",
      "Best model at iteration: 2033 | smooth loss: 2.69096\n",
      "Best model at iteration: 2034 | smooth loss: 2.69079\n",
      "Best model at iteration: 2035 | smooth loss: 2.68979\n",
      "Best model at iteration: 2036 | smooth loss: 2.68920\n",
      "Best model at iteration: 2037 | smooth loss: 2.68877\n",
      "Best model at iteration: 2038 | smooth loss: 2.68807\n",
      "Best model at iteration: 2039 | smooth loss: 2.68732\n",
      "Best model at iteration: 2041 | smooth loss: 2.68696\n",
      "Best model at iteration: 2042 | smooth loss: 2.68640\n",
      "Best model at iteration: 2043 | smooth loss: 2.68607\n",
      "Best model at iteration: 2044 | smooth loss: 2.68547\n",
      "Best model at iteration: 2045 | smooth loss: 2.68484\n",
      "Best model at iteration: 2046 | smooth loss: 2.68391\n",
      "Best model at iteration: 2047 | smooth loss: 2.68334\n",
      "Best model at iteration: 2048 | smooth loss: 2.68250\n",
      "Best model at iteration: 2049 | smooth loss: 2.68229\n",
      "Best model at iteration: 2050 | smooth loss: 2.68180\n",
      "Best model at iteration: 2051 | smooth loss: 2.68100\n",
      "Best model at iteration: 2052 | smooth loss: 2.68017\n",
      "Best model at iteration: 2053 | smooth loss: 2.67957\n",
      "Best model at iteration: 2055 | smooth loss: 2.67892\n",
      "Best model at iteration: 2056 | smooth loss: 2.67867\n",
      "Best model at iteration: 2057 | smooth loss: 2.67825\n",
      "Best model at iteration: 2058 | smooth loss: 2.67784\n",
      "Best model at iteration: 2059 | smooth loss: 2.67772\n",
      "Best model at iteration: 2060 | smooth loss: 2.67765\n",
      "Best model at iteration: 2061 | smooth loss: 2.67754\n",
      "Best model at iteration: 2063 | smooth loss: 2.67707\n",
      "Best model at iteration: 2064 | smooth loss: 2.67663\n",
      "Best model at iteration: 2065 | smooth loss: 2.67578\n",
      "Best model at iteration: 2067 | smooth loss: 2.67487\n",
      "Best model at iteration: 2068 | smooth loss: 2.67480\n",
      "Best model at iteration: 2071 | smooth loss: 2.67467\n",
      "Best model at iteration: 2072 | smooth loss: 2.67406\n",
      "Best model at iteration: 2073 | smooth loss: 2.67380\n",
      "Best model at iteration: 2074 | smooth loss: 2.67297\n",
      "Best model at iteration: 2075 | smooth loss: 2.67292\n",
      "Best model at iteration: 2077 | smooth loss: 2.67264\n",
      "Best model at iteration: 2078 | smooth loss: 2.67225\n",
      "Best model at iteration: 2079 | smooth loss: 2.67148\n",
      "Best model at iteration: 2080 | smooth loss: 2.67127\n",
      "Best model at iteration: 2081 | smooth loss: 2.67069\n",
      "Best model at iteration: 2082 | smooth loss: 2.67035\n",
      "Best model at iteration: 2083 | smooth loss: 2.66994\n",
      "Best model at iteration: 2084 | smooth loss: 2.66946\n",
      "Best model at iteration: 2085 | smooth loss: 2.66912\n",
      "Best model at iteration: 2086 | smooth loss: 2.66813\n",
      "Best model at iteration: 2087 | smooth loss: 2.66766\n",
      "Best model at iteration: 2088 | smooth loss: 2.66729\n",
      "Best model at iteration: 2089 | smooth loss: 2.66635\n",
      "Best model at iteration: 2090 | smooth loss: 2.66590\n",
      "Best model at iteration: 2091 | smooth loss: 2.66506\n",
      "Best model at iteration: 2092 | smooth loss: 2.66434\n",
      "Best model at iteration: 2093 | smooth loss: 2.66374\n",
      "Best model at iteration: 2094 | smooth loss: 2.66341\n",
      "Best model at iteration: 2095 | smooth loss: 2.66295\n",
      "Best model at iteration: 2096 | smooth loss: 2.66205\n",
      "Best model at iteration: 2097 | smooth loss: 2.66144\n",
      "Best model at iteration: 2098 | smooth loss: 2.66065\n",
      "Best model at iteration: 2099 | smooth loss: 2.65959\n",
      "Best model at iteration: 2100 | smooth loss: 2.65888\n",
      "Best model at iteration: 2101 | smooth loss: 2.65791\n",
      "Best model at iteration: 2102 | smooth loss: 2.65717\n",
      "Best model at iteration: 2103 | smooth loss: 2.65682\n",
      "Best model at iteration: 2104 | smooth loss: 2.65605\n",
      "Best model at iteration: 2105 | smooth loss: 2.65572\n",
      "Best model at iteration: 2106 | smooth loss: 2.65567\n",
      "Best model at iteration: 2107 | smooth loss: 2.65538\n",
      "Best model at iteration: 2108 | smooth loss: 2.65494\n",
      "Best model at iteration: 2109 | smooth loss: 2.65469\n",
      "Best model at iteration: 2110 | smooth loss: 2.65437\n",
      "Best model at iteration: 2111 | smooth loss: 2.65383\n",
      "Best model at iteration: 2112 | smooth loss: 2.65346\n",
      "Best model at iteration: 2113 | smooth loss: 2.65339\n",
      "Best model at iteration: 2114 | smooth loss: 2.65269\n",
      "Best model at iteration: 2115 | smooth loss: 2.65266\n",
      "Best model at iteration: 2116 | smooth loss: 2.65199\n",
      "Best model at iteration: 2117 | smooth loss: 2.65121\n",
      "Best model at iteration: 2118 | smooth loss: 2.65048\n",
      "Best model at iteration: 2122 | smooth loss: 2.65000\n",
      "Best model at iteration: 2123 | smooth loss: 2.64972\n",
      "Best model at iteration: 2124 | smooth loss: 2.64891\n",
      "Best model at iteration: 2125 | smooth loss: 2.64835\n",
      "Best model at iteration: 2126 | smooth loss: 2.64793\n",
      "Best model at iteration: 2127 | smooth loss: 2.64749\n",
      "Best model at iteration: 2128 | smooth loss: 2.64655\n",
      "Best model at iteration: 2129 | smooth loss: 2.64603\n",
      "Best model at iteration: 2130 | smooth loss: 2.64594\n",
      "Best model at iteration: 2132 | smooth loss: 2.64565\n",
      "Best model at iteration: 2133 | smooth loss: 2.64512\n",
      "Best model at iteration: 2134 | smooth loss: 2.64456\n",
      "Best model at iteration: 2135 | smooth loss: 2.64384\n",
      "Best model at iteration: 2136 | smooth loss: 2.64317\n",
      "Best model at iteration: 2137 | smooth loss: 2.64256\n",
      "Best model at iteration: 2138 | smooth loss: 2.64178\n",
      "Best model at iteration: 2139 | smooth loss: 2.64175\n",
      "Best model at iteration: 2140 | smooth loss: 2.64120\n",
      "Best model at iteration: 2141 | smooth loss: 2.64093\n",
      "Best model at iteration: 2142 | smooth loss: 2.64054\n",
      "Best model at iteration: 2143 | smooth loss: 2.64013\n",
      "Best model at iteration: 2144 | smooth loss: 2.63911\n",
      "Best model at iteration: 2145 | smooth loss: 2.63888\n",
      "Best model at iteration: 2146 | smooth loss: 2.63857\n",
      "Best model at iteration: 2147 | smooth loss: 2.63823\n",
      "Best model at iteration: 2148 | smooth loss: 2.63747\n",
      "Best model at iteration: 2149 | smooth loss: 2.63682\n",
      "Best model at iteration: 2150 | smooth loss: 2.63643\n",
      "Best model at iteration: 2151 | smooth loss: 2.63545\n",
      "Best model at iteration: 2153 | smooth loss: 2.63468\n",
      "Best model at iteration: 2154 | smooth loss: 2.63403\n",
      "Best model at iteration: 2155 | smooth loss: 2.63379\n",
      "Best model at iteration: 2156 | smooth loss: 2.63337\n",
      "Best model at iteration: 2157 | smooth loss: 2.63319\n",
      "Best model at iteration: 2159 | smooth loss: 2.63257\n",
      "Best model at iteration: 2160 | smooth loss: 2.63253\n",
      "Best model at iteration: 2161 | smooth loss: 2.63227\n",
      "Best model at iteration: 2162 | smooth loss: 2.63143\n",
      "Best model at iteration: 2163 | smooth loss: 2.63084\n",
      "Best model at iteration: 2164 | smooth loss: 2.63061\n",
      "Best model at iteration: 2165 | smooth loss: 2.62971\n",
      "Best model at iteration: 2167 | smooth loss: 2.62947\n",
      "Best model at iteration: 2168 | smooth loss: 2.62923\n",
      "Best model at iteration: 2169 | smooth loss: 2.62865\n",
      "Best model at iteration: 2170 | smooth loss: 2.62831\n",
      "Best model at iteration: 2171 | smooth loss: 2.62739\n",
      "Best model at iteration: 2172 | smooth loss: 2.62723\n",
      "Best model at iteration: 2173 | smooth loss: 2.62620\n",
      "Best model at iteration: 2175 | smooth loss: 2.62619\n",
      "Best model at iteration: 2176 | smooth loss: 2.62592\n",
      "Best model at iteration: 2189 | smooth loss: 2.62546\n",
      "Best model at iteration: 2190 | smooth loss: 2.62489\n",
      "Best model at iteration: 2191 | smooth loss: 2.62462\n",
      "Best model at iteration: 2192 | smooth loss: 2.62385\n",
      "Best model at iteration: 2193 | smooth loss: 2.62320\n",
      "Best model at iteration: 2195 | smooth loss: 2.62305\n",
      "Best model at iteration: 2196 | smooth loss: 2.62215\n",
      "Best model at iteration: 2198 | smooth loss: 2.62182\n",
      "Best model at iteration: 2199 | smooth loss: 2.62146\n",
      "Best model at iteration: 2200 | smooth loss: 2.62109\n",
      "Best model at iteration: 2201 | smooth loss: 2.62086\n",
      "Best model at iteration: 2202 | smooth loss: 2.62042\n",
      "Best model at iteration: 2203 | smooth loss: 2.62041\n",
      "Best model at iteration: 2204 | smooth loss: 2.62015\n",
      "Best model at iteration: 2205 | smooth loss: 2.61978\n",
      "Best model at iteration: 2206 | smooth loss: 2.61924\n",
      "Best model at iteration: 2208 | smooth loss: 2.61887\n",
      "Best model at iteration: 2209 | smooth loss: 2.61875\n",
      "Best model at iteration: 2210 | smooth loss: 2.61815\n",
      "Best model at iteration: 2213 | smooth loss: 2.61799\n",
      "Best model at iteration: 2214 | smooth loss: 2.61755\n",
      "Best model at iteration: 2217 | smooth loss: 2.61751\n",
      "Best model at iteration: 2218 | smooth loss: 2.61746\n",
      "Best model at iteration: 2219 | smooth loss: 2.61713\n",
      "Best model at iteration: 2220 | smooth loss: 2.61708\n",
      "Best model at iteration: 2223 | smooth loss: 2.61707\n",
      "Best model at iteration: 2224 | smooth loss: 2.61662\n",
      "Best model at iteration: 2225 | smooth loss: 2.61577\n",
      "Best model at iteration: 2226 | smooth loss: 2.61549\n",
      "Best model at iteration: 2227 | smooth loss: 2.61472\n",
      "Best model at iteration: 2228 | smooth loss: 2.61436\n",
      "Best model at iteration: 2229 | smooth loss: 2.61427\n",
      "Best model at iteration: 2231 | smooth loss: 2.61406\n",
      "Best model at iteration: 2232 | smooth loss: 2.61397\n",
      "Best model at iteration: 2233 | smooth loss: 2.61371\n",
      "Best model at iteration: 2234 | smooth loss: 2.61312\n",
      "Best model at iteration: 2235 | smooth loss: 2.61268\n",
      "Best model at iteration: 2243 | smooth loss: 2.61241\n",
      "Best model at iteration: 2244 | smooth loss: 2.61197\n",
      "Best model at iteration: 2246 | smooth loss: 2.61167\n",
      "Best model at iteration: 2247 | smooth loss: 2.61098\n",
      "Best model at iteration: 2248 | smooth loss: 2.61078\n",
      "Best model at iteration: 2249 | smooth loss: 2.60995\n",
      "Best model at iteration: 2250 | smooth loss: 2.60967\n",
      "Best model at iteration: 2251 | smooth loss: 2.60902\n",
      "Best model at iteration: 2252 | smooth loss: 2.60865\n",
      "Best model at iteration: 2253 | smooth loss: 2.60807\n",
      "Best model at iteration: 2254 | smooth loss: 2.60790\n",
      "Best model at iteration: 2255 | smooth loss: 2.60784\n",
      "Best model at iteration: 2256 | smooth loss: 2.60777\n",
      "Best model at iteration: 2257 | smooth loss: 2.60747\n",
      "Best model at iteration: 2258 | smooth loss: 2.60715\n",
      "Best model at iteration: 2259 | smooth loss: 2.60650\n",
      "Best model at iteration: 2260 | smooth loss: 2.60642\n",
      "Best model at iteration: 2261 | smooth loss: 2.60614\n",
      "Best model at iteration: 2262 | smooth loss: 2.60603\n",
      "Best model at iteration: 2263 | smooth loss: 2.60600\n",
      "Best model at iteration: 2264 | smooth loss: 2.60562\n",
      "Best model at iteration: 2266 | smooth loss: 2.60546\n",
      "Best model at iteration: 2267 | smooth loss: 2.60482\n",
      "Best model at iteration: 2268 | smooth loss: 2.60448\n",
      "Best model at iteration: 2270 | smooth loss: 2.60406\n",
      "Best model at iteration: 2271 | smooth loss: 2.60373\n",
      "Best model at iteration: 2272 | smooth loss: 2.60364\n",
      "Best model at iteration: 2273 | smooth loss: 2.60322\n",
      "Best model at iteration: 2275 | smooth loss: 2.60309\n",
      "Best model at iteration: 2276 | smooth loss: 2.60263\n",
      "Best model at iteration: 2277 | smooth loss: 2.60216\n",
      "Best model at iteration: 2278 | smooth loss: 2.60164\n",
      "Best model at iteration: 2280 | smooth loss: 2.60156\n",
      "Best model at iteration: 2281 | smooth loss: 2.60151\n",
      "Best model at iteration: 2282 | smooth loss: 2.60095\n",
      "Best model at iteration: 2283 | smooth loss: 2.60023\n",
      "Best model at iteration: 2284 | smooth loss: 2.59973\n",
      "Best model at iteration: 2285 | smooth loss: 2.59918\n",
      "Best model at iteration: 2286 | smooth loss: 2.59868\n",
      "Best model at iteration: 2287 | smooth loss: 2.59847\n",
      "Best model at iteration: 2288 | smooth loss: 2.59833\n",
      "Best model at iteration: 2289 | smooth loss: 2.59812\n",
      "Best model at iteration: 2290 | smooth loss: 2.59765\n",
      "Best model at iteration: 2291 | smooth loss: 2.59717\n",
      "Best model at iteration: 2292 | smooth loss: 2.59684\n",
      "Best model at iteration: 2293 | smooth loss: 2.59629\n",
      "Best model at iteration: 2294 | smooth loss: 2.59604\n",
      "Best model at iteration: 2295 | smooth loss: 2.59604\n",
      "Best model at iteration: 2296 | smooth loss: 2.59556\n",
      "Best model at iteration: 2298 | smooth loss: 2.59550\n",
      "Best model at iteration: 2299 | smooth loss: 2.59491\n",
      "Best model at iteration: 2300 | smooth loss: 2.59436\n",
      "Best model at iteration: 2301 | smooth loss: 2.59399\n",
      "Best model at iteration: 2302 | smooth loss: 2.59373\n",
      "Best model at iteration: 2303 | smooth loss: 2.59355\n",
      "Best model at iteration: 2325 | smooth loss: 2.59344\n",
      "Best model at iteration: 2326 | smooth loss: 2.59269\n",
      "Best model at iteration: 2327 | smooth loss: 2.59260\n",
      "Best model at iteration: 2328 | smooth loss: 2.59209\n",
      "Best model at iteration: 2329 | smooth loss: 2.59179\n",
      "Best model at iteration: 2330 | smooth loss: 2.59158\n",
      "Best model at iteration: 2333 | smooth loss: 2.59156\n",
      "Best model at iteration: 2334 | smooth loss: 2.59138\n",
      "Best model at iteration: 2335 | smooth loss: 2.59115\n",
      "Best model at iteration: 2336 | smooth loss: 2.59087\n",
      "Best model at iteration: 2337 | smooth loss: 2.59024\n",
      "Best model at iteration: 2338 | smooth loss: 2.58999\n",
      "Best model at iteration: 2339 | smooth loss: 2.58924\n",
      "Best model at iteration: 2340 | smooth loss: 2.58877\n",
      "Best model at iteration: 2341 | smooth loss: 2.58809\n",
      "Best model at iteration: 2342 | smooth loss: 2.58737\n",
      "Best model at iteration: 2343 | smooth loss: 2.58701\n",
      "Best model at iteration: 2344 | smooth loss: 2.58700\n",
      "Best model at iteration: 2345 | smooth loss: 2.58664\n",
      "Best model at iteration: 2346 | smooth loss: 2.58619\n",
      "Best model at iteration: 2347 | smooth loss: 2.58578\n",
      "Best model at iteration: 2348 | smooth loss: 2.58567\n",
      "Best model at iteration: 2349 | smooth loss: 2.58521\n",
      "Best model at iteration: 2350 | smooth loss: 2.58489\n",
      "Best model at iteration: 2351 | smooth loss: 2.58473\n",
      "Best model at iteration: 2352 | smooth loss: 2.58424\n",
      "Best model at iteration: 2353 | smooth loss: 2.58390\n",
      "Best model at iteration: 2355 | smooth loss: 2.58330\n",
      "Best model at iteration: 2356 | smooth loss: 2.58250\n",
      "Best model at iteration: 2359 | smooth loss: 2.58242\n",
      "Best model at iteration: 2360 | smooth loss: 2.58214\n",
      "Best model at iteration: 2361 | smooth loss: 2.58188\n",
      "Best model at iteration: 2363 | smooth loss: 2.58110\n",
      "Best model at iteration: 2364 | smooth loss: 2.58071\n",
      "Best model at iteration: 2365 | smooth loss: 2.57998\n",
      "Best model at iteration: 2367 | smooth loss: 2.57921\n",
      "Best model at iteration: 2368 | smooth loss: 2.57838\n",
      "Best model at iteration: 2369 | smooth loss: 2.57789\n",
      "Best model at iteration: 2371 | smooth loss: 2.57724\n",
      "Best model at iteration: 2372 | smooth loss: 2.57686\n",
      "Best model at iteration: 2373 | smooth loss: 2.57616\n",
      "Best model at iteration: 2374 | smooth loss: 2.57610\n",
      "Best model at iteration: 2375 | smooth loss: 2.57565\n",
      "Best model at iteration: 2376 | smooth loss: 2.57502\n",
      "Best model at iteration: 2377 | smooth loss: 2.57481\n",
      "Best model at iteration: 2378 | smooth loss: 2.57448\n",
      "Best model at iteration: 2379 | smooth loss: 2.57376\n",
      "Best model at iteration: 2380 | smooth loss: 2.57346\n",
      "Best model at iteration: 2381 | smooth loss: 2.57280\n",
      "Best model at iteration: 2382 | smooth loss: 2.57242\n",
      "Best model at iteration: 2383 | smooth loss: 2.57228\n",
      "Best model at iteration: 2384 | smooth loss: 2.57193\n",
      "Best model at iteration: 2385 | smooth loss: 2.57134\n",
      "Best model at iteration: 2386 | smooth loss: 2.57097\n",
      "Best model at iteration: 2387 | smooth loss: 2.57087\n",
      "Best model at iteration: 2388 | smooth loss: 2.56985\n",
      "Best model at iteration: 2389 | smooth loss: 2.56977\n",
      "Best model at iteration: 2390 | smooth loss: 2.56891\n",
      "Best model at iteration: 2391 | smooth loss: 2.56885\n",
      "Best model at iteration: 2394 | smooth loss: 2.56865\n",
      "Best model at iteration: 2395 | smooth loss: 2.56809\n",
      "Best model at iteration: 2396 | smooth loss: 2.56781\n",
      "Best model at iteration: 2397 | smooth loss: 2.56722\n",
      "Best model at iteration: 2398 | smooth loss: 2.56696\n",
      "Best model at iteration: 2399 | smooth loss: 2.56663\n",
      "Best model at iteration: 2400 | smooth loss: 2.56593\n",
      "Best model at iteration: 2402 | smooth loss: 2.56570\n",
      "Best model at iteration: 2403 | smooth loss: 2.56510\n",
      "Best model at iteration: 2407 | smooth loss: 2.56471\n",
      "Best model at iteration: 2408 | smooth loss: 2.56445\n",
      "Best model at iteration: 2409 | smooth loss: 2.56368\n",
      "Best model at iteration: 2410 | smooth loss: 2.56338\n",
      "Best model at iteration: 2411 | smooth loss: 2.56270\n",
      "Best model at iteration: 2412 | smooth loss: 2.56252\n",
      "Best model at iteration: 2413 | smooth loss: 2.56192\n",
      "Best model at iteration: 2414 | smooth loss: 2.56184\n",
      "Best model at iteration: 2415 | smooth loss: 2.56112\n",
      "Best model at iteration: 2416 | smooth loss: 2.56096\n",
      "Best model at iteration: 2417 | smooth loss: 2.56035\n",
      "Best model at iteration: 2418 | smooth loss: 2.55967\n",
      "Best model at iteration: 2419 | smooth loss: 2.55896\n",
      "Best model at iteration: 2420 | smooth loss: 2.55885\n",
      "Best model at iteration: 2422 | smooth loss: 2.55878\n",
      "Best model at iteration: 2423 | smooth loss: 2.55844\n",
      "Best model at iteration: 2424 | smooth loss: 2.55833\n",
      "Best model at iteration: 2426 | smooth loss: 2.55797\n",
      "Best model at iteration: 2427 | smooth loss: 2.55796\n",
      "Best model at iteration: 2428 | smooth loss: 2.55795\n",
      "Best model at iteration: 2429 | smooth loss: 2.55754\n",
      "Best model at iteration: 2430 | smooth loss: 2.55706\n",
      "Best model at iteration: 2431 | smooth loss: 2.55640\n",
      "Best model at iteration: 2432 | smooth loss: 2.55603\n",
      "Best model at iteration: 2433 | smooth loss: 2.55514\n",
      "Best model at iteration: 2434 | smooth loss: 2.55481\n",
      "Best model at iteration: 2435 | smooth loss: 2.55479\n",
      "Best model at iteration: 2436 | smooth loss: 2.55475\n",
      "Best model at iteration: 2437 | smooth loss: 2.55440\n",
      "Best model at iteration: 2438 | smooth loss: 2.55384\n",
      "Best model at iteration: 2439 | smooth loss: 2.55279\n",
      "Best model at iteration: 2440 | smooth loss: 2.55249\n",
      "Best model at iteration: 2442 | smooth loss: 2.55188\n",
      "Best model at iteration: 2443 | smooth loss: 2.55162\n",
      "Best model at iteration: 2444 | smooth loss: 2.55115\n",
      "Best model at iteration: 2445 | smooth loss: 2.55102\n",
      "Best model at iteration: 2446 | smooth loss: 2.55091\n",
      "Best model at iteration: 2447 | smooth loss: 2.55053\n",
      "Best model at iteration: 2448 | smooth loss: 2.54974\n",
      "Best model at iteration: 2450 | smooth loss: 2.54974\n",
      "Best model at iteration: 2451 | smooth loss: 2.54905\n",
      "Best model at iteration: 2452 | smooth loss: 2.54865\n",
      "Best model at iteration: 2453 | smooth loss: 2.54823\n",
      "Best model at iteration: 2455 | smooth loss: 2.54818\n",
      "Best model at iteration: 2456 | smooth loss: 2.54723\n",
      "Best model at iteration: 2457 | smooth loss: 2.54666\n",
      "Best model at iteration: 2458 | smooth loss: 2.54642\n",
      "Best model at iteration: 2459 | smooth loss: 2.54535\n",
      "Best model at iteration: 2460 | smooth loss: 2.54463\n",
      "Best model at iteration: 2461 | smooth loss: 2.54399\n",
      "Best model at iteration: 2462 | smooth loss: 2.54341\n",
      "Best model at iteration: 2463 | smooth loss: 2.54287\n",
      "Best model at iteration: 2464 | smooth loss: 2.54271\n",
      "Best model at iteration: 2466 | smooth loss: 2.54236\n",
      "Best model at iteration: 2467 | smooth loss: 2.54168\n",
      "Best model at iteration: 2468 | smooth loss: 2.54124\n",
      "Best model at iteration: 2469 | smooth loss: 2.54038\n",
      "Best model at iteration: 2470 | smooth loss: 2.53953\n",
      "Best model at iteration: 2471 | smooth loss: 2.53912\n",
      "Best model at iteration: 2472 | smooth loss: 2.53869\n",
      "Best model at iteration: 2473 | smooth loss: 2.53866\n",
      "Best model at iteration: 2474 | smooth loss: 2.53848\n",
      "Best model at iteration: 2475 | smooth loss: 2.53752\n",
      "Best model at iteration: 2476 | smooth loss: 2.53730\n",
      "Best model at iteration: 2477 | smooth loss: 2.53678\n",
      "Best model at iteration: 2478 | smooth loss: 2.53648\n",
      "Best model at iteration: 2479 | smooth loss: 2.53586\n",
      "Best model at iteration: 2480 | smooth loss: 2.53562\n",
      "Best model at iteration: 2481 | smooth loss: 2.53552\n",
      "Best model at iteration: 2482 | smooth loss: 2.53509\n",
      "Best model at iteration: 2487 | smooth loss: 2.53423\n",
      "Best model at iteration: 2489 | smooth loss: 2.53394\n",
      "Best model at iteration: 2490 | smooth loss: 2.53321\n",
      "Best model at iteration: 2491 | smooth loss: 2.53296\n",
      "Best model at iteration: 2492 | smooth loss: 2.53214\n",
      "Best model at iteration: 2494 | smooth loss: 2.53205\n",
      "Best model at iteration: 2495 | smooth loss: 2.53201\n",
      "Best model at iteration: 2496 | smooth loss: 2.53177\n",
      "Best model at iteration: 2497 | smooth loss: 2.53169\n",
      "Best model at iteration: 2498 | smooth loss: 2.53139\n",
      "Best model at iteration: 2499 | smooth loss: 2.53089\n",
      "Best model at iteration: 2500 | smooth loss: 2.52966\n",
      "Best model at iteration: 2501 | smooth loss: 2.52925\n",
      "Best model at iteration: 2502 | smooth loss: 2.52846\n",
      "Best model at iteration: 2503 | smooth loss: 2.52823\n",
      "Best model at iteration: 2504 | smooth loss: 2.52764\n",
      "Best model at iteration: 2505 | smooth loss: 2.52711\n",
      "Best model at iteration: 2506 | smooth loss: 2.52692\n",
      "Best model at iteration: 2507 | smooth loss: 2.52657\n",
      "Best model at iteration: 2508 | smooth loss: 2.52648\n",
      "Best model at iteration: 2510 | smooth loss: 2.52623\n",
      "Best model at iteration: 2511 | smooth loss: 2.52614\n",
      "Best model at iteration: 2520 | smooth loss: 2.52598\n",
      "Best model at iteration: 2521 | smooth loss: 2.52568\n",
      "Best model at iteration: 2522 | smooth loss: 2.52550\n",
      "Best model at iteration: 2523 | smooth loss: 2.52496\n",
      "Best model at iteration: 2524 | smooth loss: 2.52460\n",
      "Best model at iteration: 2526 | smooth loss: 2.52430\n",
      "Best model at iteration: 2527 | smooth loss: 2.52391\n",
      "Best model at iteration: 2528 | smooth loss: 2.52313\n",
      "Best model at iteration: 2529 | smooth loss: 2.52246\n",
      "Best model at iteration: 2530 | smooth loss: 2.52199\n",
      "Best model at iteration: 2531 | smooth loss: 2.52177\n",
      "Best model at iteration: 2532 | smooth loss: 2.52130\n",
      "Best model at iteration: 2536 | smooth loss: 2.52127\n",
      "Best model at iteration: 2537 | smooth loss: 2.52089\n",
      "Best model at iteration: 2538 | smooth loss: 2.51985\n",
      "Best model at iteration: 2539 | smooth loss: 2.51958\n",
      "Best model at iteration: 2540 | smooth loss: 2.51939\n",
      "Best model at iteration: 2541 | smooth loss: 2.51897\n",
      "Best model at iteration: 2543 | smooth loss: 2.51866\n",
      "Best model at iteration: 2544 | smooth loss: 2.51802\n",
      "Best model at iteration: 2547 | smooth loss: 2.51741\n",
      "Best model at iteration: 2555 | smooth loss: 2.51722\n",
      "Best model at iteration: 2556 | smooth loss: 2.51661\n",
      "Best model at iteration: 2557 | smooth loss: 2.51651\n",
      "Best model at iteration: 2558 | smooth loss: 2.51617\n",
      "Best model at iteration: 2559 | smooth loss: 2.51530\n",
      "Best model at iteration: 2560 | smooth loss: 2.51495\n",
      "Best model at iteration: 2561 | smooth loss: 2.51473\n",
      "Best model at iteration: 2562 | smooth loss: 2.51417\n",
      "Best model at iteration: 2563 | smooth loss: 2.51352\n",
      "Best model at iteration: 2564 | smooth loss: 2.51332\n",
      "Best model at iteration: 2566 | smooth loss: 2.51302\n",
      "Best model at iteration: 2567 | smooth loss: 2.51267\n",
      "Best model at iteration: 2568 | smooth loss: 2.51266\n",
      "Best model at iteration: 2569 | smooth loss: 2.51237\n",
      "Best model at iteration: 2571 | smooth loss: 2.51189\n",
      "Best model at iteration: 2572 | smooth loss: 2.51157\n",
      "Best model at iteration: 2573 | smooth loss: 2.51128\n",
      "Best model at iteration: 2574 | smooth loss: 2.51098\n",
      "Best model at iteration: 2575 | smooth loss: 2.51060\n",
      "Best model at iteration: 2577 | smooth loss: 2.51051\n",
      "Best model at iteration: 2578 | smooth loss: 2.51008\n",
      "Best model at iteration: 2579 | smooth loss: 2.50986\n",
      "Best model at iteration: 2580 | smooth loss: 2.50934\n",
      "Best model at iteration: 2581 | smooth loss: 2.50927\n",
      "Best model at iteration: 2582 | smooth loss: 2.50881\n",
      "Best model at iteration: 2583 | smooth loss: 2.50844\n",
      "Best model at iteration: 2584 | smooth loss: 2.50807\n",
      "Best model at iteration: 2585 | smooth loss: 2.50756\n",
      "Best model at iteration: 2586 | smooth loss: 2.50736\n",
      "Best model at iteration: 2587 | smooth loss: 2.50705\n",
      "Best model at iteration: 2588 | smooth loss: 2.50684\n",
      "Best model at iteration: 2599 | smooth loss: 2.50669\n",
      "Best model at iteration: 2601 | smooth loss: 2.50617\n",
      "Best model at iteration: 2602 | smooth loss: 2.50612\n",
      "Best model at iteration: 2603 | smooth loss: 2.50597\n",
      "Best model at iteration: 2604 | smooth loss: 2.50568\n",
      "Best model at iteration: 2605 | smooth loss: 2.50557\n",
      "Best model at iteration: 2606 | smooth loss: 2.50523\n",
      "Best model at iteration: 2608 | smooth loss: 2.50505\n",
      "Best model at iteration: 2610 | smooth loss: 2.50449\n",
      "Best model at iteration: 2611 | smooth loss: 2.50412\n",
      "Best model at iteration: 2612 | smooth loss: 2.50347\n",
      "Best model at iteration: 2613 | smooth loss: 2.50329\n",
      "Best model at iteration: 2614 | smooth loss: 2.50256\n",
      "Best model at iteration: 2615 | smooth loss: 2.50242\n",
      "Best model at iteration: 2618 | smooth loss: 2.50215\n",
      "Best model at iteration: 2619 | smooth loss: 2.50191\n",
      "Best model at iteration: 2620 | smooth loss: 2.50159\n",
      "Best model at iteration: 2621 | smooth loss: 2.50148\n",
      "Best model at iteration: 2622 | smooth loss: 2.50137\n",
      "Best model at iteration: 2623 | smooth loss: 2.50126\n",
      "Best model at iteration: 2624 | smooth loss: 2.50121\n",
      "Best model at iteration: 2625 | smooth loss: 2.50093\n",
      "Best model at iteration: 2626 | smooth loss: 2.50027\n",
      "Best model at iteration: 2627 | smooth loss: 2.49982\n",
      "Best model at iteration: 2628 | smooth loss: 2.49971\n",
      "Best model at iteration: 2629 | smooth loss: 2.49894\n",
      "Best model at iteration: 2630 | smooth loss: 2.49857\n",
      "Best model at iteration: 2631 | smooth loss: 2.49846\n",
      "Best model at iteration: 2632 | smooth loss: 2.49821\n",
      "Best model at iteration: 2633 | smooth loss: 2.49755\n",
      "Best model at iteration: 2634 | smooth loss: 2.49703\n",
      "Best model at iteration: 2635 | smooth loss: 2.49667\n",
      "Best model at iteration: 2636 | smooth loss: 2.49647\n",
      "Best model at iteration: 2637 | smooth loss: 2.49630\n",
      "Best model at iteration: 2638 | smooth loss: 2.49618\n",
      "Best model at iteration: 2639 | smooth loss: 2.49580\n",
      "Best model at iteration: 2640 | smooth loss: 2.49515\n",
      "Best model at iteration: 2641 | smooth loss: 2.49431\n",
      "Best model at iteration: 2642 | smooth loss: 2.49399\n",
      "Best model at iteration: 2643 | smooth loss: 2.49320\n",
      "Best model at iteration: 2646 | smooth loss: 2.49247\n",
      "Best model at iteration: 2647 | smooth loss: 2.49213\n",
      "Best model at iteration: 2648 | smooth loss: 2.49175\n",
      "Best model at iteration: 2649 | smooth loss: 2.49119\n",
      "Best model at iteration: 2650 | smooth loss: 2.49094\n",
      "Best model at iteration: 2651 | smooth loss: 2.49034\n",
      "Best model at iteration: 2654 | smooth loss: 2.49013\n",
      "Best model at iteration: 2655 | smooth loss: 2.48933\n",
      "Best model at iteration: 2656 | smooth loss: 2.48867\n",
      "Best model at iteration: 2657 | smooth loss: 2.48832\n",
      "Best model at iteration: 2660 | smooth loss: 2.48801\n",
      "Best model at iteration: 2661 | smooth loss: 2.48743\n",
      "Best model at iteration: 2662 | smooth loss: 2.48726\n",
      "Best model at iteration: 2663 | smooth loss: 2.48722\n",
      "Best model at iteration: 2664 | smooth loss: 2.48710\n",
      "Best model at iteration: 2665 | smooth loss: 2.48693\n",
      "Best model at iteration: 2666 | smooth loss: 2.48630\n",
      "Best model at iteration: 2667 | smooth loss: 2.48600\n",
      "Best model at iteration: 2669 | smooth loss: 2.48600\n",
      "Best model at iteration: 2674 | smooth loss: 2.48598\n",
      "Best model at iteration: 2675 | smooth loss: 2.48550\n",
      "Best model at iteration: 2676 | smooth loss: 2.48503\n",
      "Best model at iteration: 2677 | smooth loss: 2.48487\n",
      "Best model at iteration: 2679 | smooth loss: 2.48460\n",
      "Best model at iteration: 2680 | smooth loss: 2.48408\n",
      "Best model at iteration: 2681 | smooth loss: 2.48369\n",
      "Best model at iteration: 2682 | smooth loss: 2.48324\n",
      "Best model at iteration: 2683 | smooth loss: 2.48245\n",
      "Best model at iteration: 2684 | smooth loss: 2.48233\n",
      "Best model at iteration: 2685 | smooth loss: 2.48223\n",
      "Best model at iteration: 2686 | smooth loss: 2.48123\n",
      "Best model at iteration: 2687 | smooth loss: 2.48079\n",
      "Best model at iteration: 2688 | smooth loss: 2.48039\n",
      "Best model at iteration: 2690 | smooth loss: 2.48017\n",
      "Best model at iteration: 2691 | smooth loss: 2.47985\n",
      "Best model at iteration: 2692 | smooth loss: 2.47956\n",
      "Best model at iteration: 2693 | smooth loss: 2.47866\n",
      "Best model at iteration: 2694 | smooth loss: 2.47844\n",
      "Best model at iteration: 2695 | smooth loss: 2.47788\n",
      "Best model at iteration: 2696 | smooth loss: 2.47759\n",
      "Best model at iteration: 2697 | smooth loss: 2.47722\n",
      "Best model at iteration: 2698 | smooth loss: 2.47657\n",
      "Best model at iteration: 2699 | smooth loss: 2.47608\n",
      "Best model at iteration: 2700 | smooth loss: 2.47600\n",
      "Best model at iteration: 2701 | smooth loss: 2.47559\n",
      "Best model at iteration: 2702 | smooth loss: 2.47548\n",
      "Best model at iteration: 2703 | smooth loss: 2.47455\n",
      "Best model at iteration: 2704 | smooth loss: 2.47392\n",
      "Best model at iteration: 2705 | smooth loss: 2.47366\n",
      "Best model at iteration: 2706 | smooth loss: 2.47337\n",
      "Best model at iteration: 2707 | smooth loss: 2.47306\n",
      "Best model at iteration: 2710 | smooth loss: 2.47305\n",
      "Best model at iteration: 2711 | smooth loss: 2.47250\n",
      "Best model at iteration: 2712 | smooth loss: 2.47206\n",
      "Best model at iteration: 2713 | smooth loss: 2.47191\n",
      "Best model at iteration: 2714 | smooth loss: 2.47146\n",
      "Best model at iteration: 2715 | smooth loss: 2.47049\n",
      "Best model at iteration: 2716 | smooth loss: 2.46987\n",
      "Best model at iteration: 2717 | smooth loss: 2.46971\n",
      "Best model at iteration: 2718 | smooth loss: 2.46957\n",
      "Best model at iteration: 2719 | smooth loss: 2.46927\n",
      "Best model at iteration: 2720 | smooth loss: 2.46925\n",
      "Best model at iteration: 2721 | smooth loss: 2.46860\n",
      "Best model at iteration: 2722 | smooth loss: 2.46801\n",
      "Best model at iteration: 2723 | smooth loss: 2.46777\n",
      "Best model at iteration: 2724 | smooth loss: 2.46701\n",
      "Best model at iteration: 2725 | smooth loss: 2.46687\n",
      "Best model at iteration: 2726 | smooth loss: 2.46638\n",
      "Best model at iteration: 2732 | smooth loss: 2.46595\n",
      "Best model at iteration: 2734 | smooth loss: 2.46593\n",
      "Best model at iteration: 2737 | smooth loss: 2.46513\n",
      "Best model at iteration: 2738 | smooth loss: 2.46443\n",
      "Best model at iteration: 2739 | smooth loss: 2.46396\n",
      "Best model at iteration: 2740 | smooth loss: 2.46355\n",
      "Best model at iteration: 2741 | smooth loss: 2.46326\n",
      "Best model at iteration: 2742 | smooth loss: 2.46288\n",
      "Best model at iteration: 2743 | smooth loss: 2.46228\n",
      "Best model at iteration: 2744 | smooth loss: 2.46213\n",
      "Best model at iteration: 2746 | smooth loss: 2.46118\n",
      "Best model at iteration: 2747 | smooth loss: 2.46065\n",
      "Best model at iteration: 2748 | smooth loss: 2.46012\n",
      "Best model at iteration: 2749 | smooth loss: 2.45953\n",
      "Best model at iteration: 2750 | smooth loss: 2.45900\n",
      "Best model at iteration: 2751 | smooth loss: 2.45875\n",
      "Best model at iteration: 2752 | smooth loss: 2.45820\n",
      "Best model at iteration: 2753 | smooth loss: 2.45757\n",
      "Best model at iteration: 2756 | smooth loss: 2.45701\n",
      "Best model at iteration: 2757 | smooth loss: 2.45665\n",
      "Best model at iteration: 2758 | smooth loss: 2.45661\n",
      "Best model at iteration: 2759 | smooth loss: 2.45629\n",
      "Best model at iteration: 2760 | smooth loss: 2.45604\n",
      "Best model at iteration: 2761 | smooth loss: 2.45574\n",
      "Best model at iteration: 2763 | smooth loss: 2.45503\n",
      "Best model at iteration: 2764 | smooth loss: 2.45435\n",
      "Best model at iteration: 2765 | smooth loss: 2.45414\n",
      "Best model at iteration: 2766 | smooth loss: 2.45377\n",
      "Best model at iteration: 2767 | smooth loss: 2.45337\n",
      "Best model at iteration: 2768 | smooth loss: 2.45307\n",
      "Best model at iteration: 2769 | smooth loss: 2.45282\n",
      "Best model at iteration: 2770 | smooth loss: 2.45275\n",
      "Best model at iteration: 2772 | smooth loss: 2.45270\n",
      "Best model at iteration: 2773 | smooth loss: 2.45205\n",
      "Best model at iteration: 2774 | smooth loss: 2.45166\n",
      "Best model at iteration: 2775 | smooth loss: 2.45136\n",
      "Best model at iteration: 2776 | smooth loss: 2.45085\n",
      "Best model at iteration: 2777 | smooth loss: 2.45080\n",
      "Best model at iteration: 2778 | smooth loss: 2.45043\n",
      "Best model at iteration: 2779 | smooth loss: 2.45039\n",
      "Best model at iteration: 2780 | smooth loss: 2.44996\n",
      "Best model at iteration: 2781 | smooth loss: 2.44959\n",
      "Best model at iteration: 2782 | smooth loss: 2.44896\n",
      "Best model at iteration: 2783 | smooth loss: 2.44786\n",
      "Best model at iteration: 2784 | smooth loss: 2.44723\n",
      "Best model at iteration: 2785 | smooth loss: 2.44683\n",
      "Best model at iteration: 2786 | smooth loss: 2.44630\n",
      "Best model at iteration: 2787 | smooth loss: 2.44591\n",
      "Best model at iteration: 2789 | smooth loss: 2.44570\n",
      "Best model at iteration: 2790 | smooth loss: 2.44478\n",
      "Best model at iteration: 2791 | smooth loss: 2.44475\n",
      "Best model at iteration: 2792 | smooth loss: 2.44435\n",
      "Best model at iteration: 2794 | smooth loss: 2.44388\n",
      "Best model at iteration: 2795 | smooth loss: 2.44351\n",
      "Best model at iteration: 2796 | smooth loss: 2.44329\n",
      "Best model at iteration: 2797 | smooth loss: 2.44247\n",
      "Best model at iteration: 2798 | smooth loss: 2.44203\n",
      "Best model at iteration: 2801 | smooth loss: 2.44144\n",
      "Best model at iteration: 2802 | smooth loss: 2.44094\n",
      "Best model at iteration: 2803 | smooth loss: 2.44066\n",
      "Best model at iteration: 2804 | smooth loss: 2.44019\n",
      "Best model at iteration: 2805 | smooth loss: 2.43925\n",
      "Best model at iteration: 2806 | smooth loss: 2.43912\n",
      "Best model at iteration: 2807 | smooth loss: 2.43894\n",
      "Best model at iteration: 2808 | smooth loss: 2.43824\n",
      "Best model at iteration: 2809 | smooth loss: 2.43753\n",
      "Best model at iteration: 2810 | smooth loss: 2.43701\n",
      "Best model at iteration: 2811 | smooth loss: 2.43698\n",
      "Best model at iteration: 2812 | smooth loss: 2.43661\n",
      "Best model at iteration: 2813 | smooth loss: 2.43586\n",
      "Best model at iteration: 2814 | smooth loss: 2.43542\n",
      "Best model at iteration: 2815 | smooth loss: 2.43488\n",
      "Best model at iteration: 2816 | smooth loss: 2.43473\n",
      "Best model at iteration: 2817 | smooth loss: 2.43460\n",
      "Best model at iteration: 2818 | smooth loss: 2.43433\n",
      "Best model at iteration: 2819 | smooth loss: 2.43384\n",
      "Best model at iteration: 2820 | smooth loss: 2.43375\n",
      "Best model at iteration: 2821 | smooth loss: 2.43340\n",
      "Best model at iteration: 2822 | smooth loss: 2.43334\n",
      "Best model at iteration: 2823 | smooth loss: 2.43268\n",
      "Best model at iteration: 2824 | smooth loss: 2.43200\n",
      "Best model at iteration: 2827 | smooth loss: 2.43200\n",
      "Best model at iteration: 2828 | smooth loss: 2.43153\n",
      "Best model at iteration: 2829 | smooth loss: 2.43137\n",
      "Best model at iteration: 2833 | smooth loss: 2.43090\n",
      "Best model at iteration: 2834 | smooth loss: 2.43017\n",
      "Best model at iteration: 2835 | smooth loss: 2.42966\n",
      "Best model at iteration: 2836 | smooth loss: 2.42916\n",
      "Best model at iteration: 2837 | smooth loss: 2.42869\n",
      "Best model at iteration: 2840 | smooth loss: 2.42867\n",
      "Best model at iteration: 2841 | smooth loss: 2.42826\n",
      "Best model at iteration: 2842 | smooth loss: 2.42817\n",
      "Best model at iteration: 2843 | smooth loss: 2.42739\n",
      "Best model at iteration: 2844 | smooth loss: 2.42704\n",
      "Best model at iteration: 2845 | smooth loss: 2.42681\n",
      "Best model at iteration: 2846 | smooth loss: 2.42632\n",
      "Best model at iteration: 2847 | smooth loss: 2.42607\n",
      "Best model at iteration: 2848 | smooth loss: 2.42573\n",
      "Best model at iteration: 2849 | smooth loss: 2.42534\n",
      "Best model at iteration: 2851 | smooth loss: 2.42533\n",
      "Best model at iteration: 2852 | smooth loss: 2.42528\n",
      "Best model at iteration: 2853 | smooth loss: 2.42453\n",
      "Best model at iteration: 2854 | smooth loss: 2.42423\n",
      "Best model at iteration: 2855 | smooth loss: 2.42341\n",
      "Best model at iteration: 2856 | smooth loss: 2.42334\n",
      "Best model at iteration: 2857 | smooth loss: 2.42241\n",
      "Best model at iteration: 2858 | smooth loss: 2.42217\n",
      "Best model at iteration: 2860 | smooth loss: 2.42145\n",
      "Best model at iteration: 2862 | smooth loss: 2.42109\n",
      "Best model at iteration: 2863 | smooth loss: 2.42054\n",
      "Best model at iteration: 2864 | smooth loss: 2.42025\n",
      "Best model at iteration: 2865 | smooth loss: 2.41932\n",
      "Best model at iteration: 2866 | smooth loss: 2.41899\n",
      "Best model at iteration: 2867 | smooth loss: 2.41870\n",
      "Best model at iteration: 2868 | smooth loss: 2.41818\n",
      "Best model at iteration: 2869 | smooth loss: 2.41744\n",
      "Best model at iteration: 2870 | smooth loss: 2.41648\n",
      "Best model at iteration: 2871 | smooth loss: 2.41621\n",
      "Best model at iteration: 2873 | smooth loss: 2.41581\n",
      "Best model at iteration: 2875 | smooth loss: 2.41520\n",
      "Best model at iteration: 2876 | smooth loss: 2.41512\n",
      "Best model at iteration: 2877 | smooth loss: 2.41457\n",
      "Best model at iteration: 2878 | smooth loss: 2.41418\n",
      "Best model at iteration: 2879 | smooth loss: 2.41352\n",
      "Best model at iteration: 2880 | smooth loss: 2.41336\n",
      "Best model at iteration: 2882 | smooth loss: 2.41256\n",
      "Best model at iteration: 2885 | smooth loss: 2.41244\n",
      "Best model at iteration: 2886 | smooth loss: 2.41213\n",
      "Best model at iteration: 2890 | smooth loss: 2.41198\n",
      "Best model at iteration: 2891 | smooth loss: 2.41158\n",
      "Best model at iteration: 2892 | smooth loss: 2.41141\n",
      "Best model at iteration: 2893 | smooth loss: 2.41052\n",
      "Best model at iteration: 2894 | smooth loss: 2.40950\n",
      "Best model at iteration: 2895 | smooth loss: 2.40922\n",
      "Best model at iteration: 2896 | smooth loss: 2.40891\n",
      "Best model at iteration: 2899 | smooth loss: 2.40859\n",
      "Best model at iteration: 2900 | smooth loss: 2.40819\n",
      "Best model at iteration: 2905 | smooth loss: 2.40781\n",
      "Best model at iteration: 2906 | smooth loss: 2.40759\n",
      "Best model at iteration: 2907 | smooth loss: 2.40720\n",
      "Best model at iteration: 2908 | smooth loss: 2.40669\n",
      "Best model at iteration: 2909 | smooth loss: 2.40621\n",
      "Best model at iteration: 2910 | smooth loss: 2.40604\n",
      "Best model at iteration: 2911 | smooth loss: 2.40544\n",
      "Best model at iteration: 2912 | smooth loss: 2.40507\n",
      "Best model at iteration: 2914 | smooth loss: 2.40446\n",
      "Best model at iteration: 2915 | smooth loss: 2.40422\n",
      "Best model at iteration: 2917 | smooth loss: 2.40391\n",
      "Best model at iteration: 2918 | smooth loss: 2.40334\n",
      "Best model at iteration: 2919 | smooth loss: 2.40293\n",
      "Best model at iteration: 2921 | smooth loss: 2.40282\n",
      "Best model at iteration: 2922 | smooth loss: 2.40281\n",
      "Best model at iteration: 2923 | smooth loss: 2.40263\n",
      "Best model at iteration: 2924 | smooth loss: 2.40214\n",
      "Best model at iteration: 2925 | smooth loss: 2.40176\n",
      "Best model at iteration: 2927 | smooth loss: 2.40152\n",
      "Best model at iteration: 2929 | smooth loss: 2.40104\n",
      "Best model at iteration: 2930 | smooth loss: 2.40066\n",
      "Best model at iteration: 2931 | smooth loss: 2.40021\n",
      "Best model at iteration: 2932 | smooth loss: 2.39942\n",
      "Best model at iteration: 2933 | smooth loss: 2.39874\n",
      "Best model at iteration: 2934 | smooth loss: 2.39800\n",
      "Best model at iteration: 2938 | smooth loss: 2.39751\n",
      "Best model at iteration: 2939 | smooth loss: 2.39745\n",
      "Best model at iteration: 2940 | smooth loss: 2.39731\n",
      "Best model at iteration: 2941 | smooth loss: 2.39702\n",
      "Best model at iteration: 2942 | smooth loss: 2.39659\n",
      "Best model at iteration: 2943 | smooth loss: 2.39599\n",
      "Best model at iteration: 2944 | smooth loss: 2.39540\n",
      "Best model at iteration: 2945 | smooth loss: 2.39475\n",
      "Best model at iteration: 2959 | smooth loss: 2.39471\n",
      "Best model at iteration: 2960 | smooth loss: 2.39449\n",
      "Best model at iteration: 2961 | smooth loss: 2.39449\n",
      "Best model at iteration: 2962 | smooth loss: 2.39359\n",
      "Best model at iteration: 2963 | smooth loss: 2.39294\n",
      "Best model at iteration: 2964 | smooth loss: 2.39288\n",
      "Best model at iteration: 2966 | smooth loss: 2.39262\n",
      "Best model at iteration: 2968 | smooth loss: 2.39174\n",
      "Best model at iteration: 2969 | smooth loss: 2.39150\n",
      "Best model at iteration: 2970 | smooth loss: 2.39111\n",
      "Best model at iteration: 2971 | smooth loss: 2.39089\n",
      "Best model at iteration: 2972 | smooth loss: 2.38970\n",
      "Best model at iteration: 2973 | smooth loss: 2.38945\n",
      "Best model at iteration: 2974 | smooth loss: 2.38930\n",
      "Best model at iteration: 2976 | smooth loss: 2.38912\n",
      "Best model at iteration: 2979 | smooth loss: 2.38871\n",
      "Best model at iteration: 2980 | smooth loss: 2.38764\n",
      "Best model at iteration: 2981 | smooth loss: 2.38728\n",
      "Best model at iteration: 2982 | smooth loss: 2.38683\n",
      "Best model at iteration: 2983 | smooth loss: 2.38597\n",
      "Best model at iteration: 2984 | smooth loss: 2.38596\n",
      "Best model at iteration: 2985 | smooth loss: 2.38568\n",
      "Best model at iteration: 2986 | smooth loss: 2.38539\n",
      "Best model at iteration: 2987 | smooth loss: 2.38530\n",
      "Best model at iteration: 2988 | smooth loss: 2.38445\n",
      "Best model at iteration: 2989 | smooth loss: 2.38399\n",
      "Best model at iteration: 2990 | smooth loss: 2.38357\n",
      "Best model at iteration: 2991 | smooth loss: 2.38343\n",
      "Best model at iteration: 2993 | smooth loss: 2.38297\n",
      "Best model at iteration: 2994 | smooth loss: 2.38201\n",
      "Best model at iteration: 2995 | smooth loss: 2.38169\n",
      "Best model at iteration: 2996 | smooth loss: 2.38054\n",
      "Best model at iteration: 2997 | smooth loss: 2.38033\n",
      "Best model at iteration: 2998 | smooth loss: 2.37985\n",
      "Best model at iteration: 2999 | smooth loss: 2.37886\n",
      "Best model at iteration: 3000 | smooth loss: 2.37878\n",
      "iter = 3000, smooth loss=2.3787849231542646\n",
      "nd aidring the Luing inithireney ar you hoo's tice ahin so fhes boaritt hi andom his soukinis ous Unct opkead sire fircousca.\n",
      "Wers the.\"Wons, anganis ursimgrt Preopeastey s te starme- tto ste that bem\n",
      "\n",
      "Best model at iteration: 3003 | smooth loss: 2.37832\n",
      "Best model at iteration: 3004 | smooth loss: 2.37770\n",
      "Best model at iteration: 3005 | smooth loss: 2.37721\n",
      "Best model at iteration: 3051 | smooth loss: 2.37682\n",
      "Best model at iteration: 3057 | smooth loss: 2.37672\n",
      "Best model at iteration: 3058 | smooth loss: 2.37658\n",
      "Best model at iteration: 3061 | smooth loss: 2.37625\n",
      "Best model at iteration: 3064 | smooth loss: 2.37614\n",
      "Best model at iteration: 3067 | smooth loss: 2.37577\n",
      "Best model at iteration: 3068 | smooth loss: 2.37535\n",
      "Best model at iteration: 3069 | smooth loss: 2.37500\n",
      "Best model at iteration: 3070 | smooth loss: 2.37434\n",
      "Best model at iteration: 3071 | smooth loss: 2.37416\n",
      "Best model at iteration: 3072 | smooth loss: 2.37403\n",
      "Best model at iteration: 3081 | smooth loss: 2.37378\n",
      "Best model at iteration: 3082 | smooth loss: 2.37328\n",
      "Best model at iteration: 3083 | smooth loss: 2.37317\n",
      "Best model at iteration: 3084 | smooth loss: 2.37260\n",
      "Best model at iteration: 3085 | smooth loss: 2.37222\n",
      "Best model at iteration: 3086 | smooth loss: 2.37199\n",
      "Best model at iteration: 3087 | smooth loss: 2.37175\n",
      "Best model at iteration: 3088 | smooth loss: 2.37116\n",
      "Best model at iteration: 3090 | smooth loss: 2.37113\n",
      "Best model at iteration: 3091 | smooth loss: 2.37081\n",
      "Best model at iteration: 3092 | smooth loss: 2.37066\n",
      "Best model at iteration: 3093 | smooth loss: 2.37051\n",
      "Best model at iteration: 3094 | smooth loss: 2.37038\n",
      "Best model at iteration: 3095 | smooth loss: 2.37019\n",
      "Best model at iteration: 3096 | smooth loss: 2.36940\n",
      "Best model at iteration: 3097 | smooth loss: 2.36901\n",
      "Best model at iteration: 3098 | smooth loss: 2.36859\n",
      "Best model at iteration: 3099 | smooth loss: 2.36799\n",
      "Best model at iteration: 3100 | smooth loss: 2.36758\n",
      "Best model at iteration: 3101 | smooth loss: 2.36756\n",
      "Best model at iteration: 3102 | smooth loss: 2.36754\n",
      "Best model at iteration: 3104 | smooth loss: 2.36753\n",
      "Best model at iteration: 3105 | smooth loss: 2.36709\n",
      "Best model at iteration: 3108 | smooth loss: 2.36671\n",
      "Best model at iteration: 3110 | smooth loss: 2.36652\n",
      "Best model at iteration: 3111 | smooth loss: 2.36620\n",
      "Best model at iteration: 3112 | smooth loss: 2.36612\n",
      "Best model at iteration: 3113 | smooth loss: 2.36600\n",
      "Best model at iteration: 3114 | smooth loss: 2.36571\n",
      "Best model at iteration: 3119 | smooth loss: 2.36491\n",
      "Best model at iteration: 3120 | smooth loss: 2.36468\n",
      "Best model at iteration: 3121 | smooth loss: 2.36452\n",
      "Best model at iteration: 3122 | smooth loss: 2.36399\n",
      "Best model at iteration: 3137 | smooth loss: 2.36396\n",
      "Best model at iteration: 3138 | smooth loss: 2.36342\n",
      "Best model at iteration: 3139 | smooth loss: 2.36328\n",
      "Best model at iteration: 3140 | smooth loss: 2.36316\n",
      "Best model at iteration: 3141 | smooth loss: 2.36304\n",
      "Best model at iteration: 3142 | smooth loss: 2.36239\n",
      "Best model at iteration: 3143 | smooth loss: 2.36207\n",
      "Best model at iteration: 3145 | smooth loss: 2.36206\n",
      "Best model at iteration: 3146 | smooth loss: 2.36195\n",
      "Best model at iteration: 3147 | smooth loss: 2.36180\n",
      "Best model at iteration: 3149 | smooth loss: 2.36153\n",
      "Best model at iteration: 3150 | smooth loss: 2.36099\n",
      "Best model at iteration: 3151 | smooth loss: 2.36092\n",
      "Best model at iteration: 3152 | smooth loss: 2.36084\n",
      "Best model at iteration: 3153 | smooth loss: 2.36054\n",
      "Best model at iteration: 3154 | smooth loss: 2.36003\n",
      "Best model at iteration: 3155 | smooth loss: 2.35974\n",
      "Best model at iteration: 3156 | smooth loss: 2.35909\n",
      "Best model at iteration: 3157 | smooth loss: 2.35867\n",
      "Best model at iteration: 3158 | smooth loss: 2.35812\n",
      "Best model at iteration: 3159 | smooth loss: 2.35749\n",
      "Best model at iteration: 3160 | smooth loss: 2.35719\n",
      "Best model at iteration: 3161 | smooth loss: 2.35645\n",
      "Best model at iteration: 3163 | smooth loss: 2.35628\n",
      "Best model at iteration: 3164 | smooth loss: 2.35606\n",
      "Best model at iteration: 3165 | smooth loss: 2.35565\n",
      "Best model at iteration: 3166 | smooth loss: 2.35549\n",
      "Best model at iteration: 3167 | smooth loss: 2.35534\n",
      "Best model at iteration: 3168 | smooth loss: 2.35490\n",
      "Best model at iteration: 3169 | smooth loss: 2.35468\n",
      "Best model at iteration: 3170 | smooth loss: 2.35439\n",
      "Best model at iteration: 3171 | smooth loss: 2.35426\n",
      "Best model at iteration: 3173 | smooth loss: 2.35372\n",
      "Best model at iteration: 3174 | smooth loss: 2.35356\n",
      "Best model at iteration: 3176 | smooth loss: 2.35343\n",
      "Best model at iteration: 3177 | smooth loss: 2.35289\n",
      "Best model at iteration: 3178 | smooth loss: 2.35285\n",
      "Best model at iteration: 3180 | smooth loss: 2.35236\n",
      "Best model at iteration: 3182 | smooth loss: 2.35223\n",
      "Best model at iteration: 3183 | smooth loss: 2.35138\n",
      "Best model at iteration: 3184 | smooth loss: 2.35138\n",
      "Best model at iteration: 3185 | smooth loss: 2.35098\n",
      "Best model at iteration: 3186 | smooth loss: 2.35039\n",
      "Best model at iteration: 3187 | smooth loss: 2.34952\n",
      "Best model at iteration: 3188 | smooth loss: 2.34935\n",
      "Best model at iteration: 3189 | smooth loss: 2.34931\n",
      "Best model at iteration: 3191 | smooth loss: 2.34920\n",
      "Best model at iteration: 3192 | smooth loss: 2.34895\n",
      "Best model at iteration: 3193 | smooth loss: 2.34895\n",
      "Best model at iteration: 3195 | smooth loss: 2.34879\n",
      "Best model at iteration: 3196 | smooth loss: 2.34865\n",
      "Best model at iteration: 3197 | smooth loss: 2.34849\n",
      "Best model at iteration: 3199 | smooth loss: 2.34829\n",
      "Best model at iteration: 3201 | smooth loss: 2.34788\n",
      "Best model at iteration: 3202 | smooth loss: 2.34778\n",
      "Best model at iteration: 3203 | smooth loss: 2.34693\n",
      "Best model at iteration: 3204 | smooth loss: 2.34656\n",
      "Best model at iteration: 3205 | smooth loss: 2.34613\n",
      "Best model at iteration: 3207 | smooth loss: 2.34598\n",
      "Best model at iteration: 3210 | smooth loss: 2.34597\n",
      "Best model at iteration: 3211 | smooth loss: 2.34542\n",
      "Best model at iteration: 3214 | smooth loss: 2.34531\n",
      "Best model at iteration: 3216 | smooth loss: 2.34505\n",
      "Best model at iteration: 3217 | smooth loss: 2.34492\n",
      "Best model at iteration: 3218 | smooth loss: 2.34457\n",
      "Best model at iteration: 3219 | smooth loss: 2.34432\n",
      "Best model at iteration: 3220 | smooth loss: 2.34424\n",
      "Best model at iteration: 3232 | smooth loss: 2.34370\n",
      "Best model at iteration: 3233 | smooth loss: 2.34320\n",
      "Best model at iteration: 3234 | smooth loss: 2.34306\n",
      "Best model at iteration: 3235 | smooth loss: 2.34285\n",
      "Best model at iteration: 3236 | smooth loss: 2.34261\n",
      "Best model at iteration: 3237 | smooth loss: 2.34232\n",
      "Best model at iteration: 3238 | smooth loss: 2.34177\n",
      "Best model at iteration: 3240 | smooth loss: 2.34163\n",
      "Best model at iteration: 3241 | smooth loss: 2.34125\n",
      "Best model at iteration: 3242 | smooth loss: 2.34058\n",
      "Best model at iteration: 3243 | smooth loss: 2.34025\n",
      "Best model at iteration: 3244 | smooth loss: 2.33969\n",
      "Best model at iteration: 3267 | smooth loss: 2.33962\n",
      "Best model at iteration: 3268 | smooth loss: 2.33918\n",
      "Best model at iteration: 3271 | smooth loss: 2.33916\n",
      "Best model at iteration: 3272 | smooth loss: 2.33896\n",
      "Best model at iteration: 3278 | smooth loss: 2.33895\n",
      "Best model at iteration: 3279 | smooth loss: 2.33887\n",
      "Best model at iteration: 3281 | smooth loss: 2.33885\n",
      "Best model at iteration: 3282 | smooth loss: 2.33819\n",
      "Best model at iteration: 3283 | smooth loss: 2.33759\n",
      "Best model at iteration: 3296 | smooth loss: 2.33724\n",
      "Best model at iteration: 3319 | smooth loss: 2.33691\n",
      "Best model at iteration: 3320 | smooth loss: 2.33650\n",
      "Best model at iteration: 3321 | smooth loss: 2.33605\n",
      "Best model at iteration: 3322 | smooth loss: 2.33597\n",
      "Best model at iteration: 3323 | smooth loss: 2.33501\n",
      "Best model at iteration: 3324 | smooth loss: 2.33473\n",
      "Best model at iteration: 3325 | smooth loss: 2.33454\n",
      "Best model at iteration: 3328 | smooth loss: 2.33432\n",
      "Best model at iteration: 3329 | smooth loss: 2.33417\n",
      "Best model at iteration: 3330 | smooth loss: 2.33341\n",
      "Best model at iteration: 3338 | smooth loss: 2.33323\n",
      "Best model at iteration: 3339 | smooth loss: 2.33317\n",
      "Best model at iteration: 3340 | smooth loss: 2.33299\n",
      "Best model at iteration: 3342 | smooth loss: 2.33280\n",
      "Best model at iteration: 3343 | smooth loss: 2.33273\n",
      "Best model at iteration: 3345 | smooth loss: 2.33237\n",
      "Best model at iteration: 3347 | smooth loss: 2.33205\n",
      "Best model at iteration: 3348 | smooth loss: 2.33194\n",
      "Best model at iteration: 3350 | smooth loss: 2.33184\n",
      "Best model at iteration: 3351 | smooth loss: 2.33165\n",
      "Best model at iteration: 3352 | smooth loss: 2.33161\n",
      "Best model at iteration: 3353 | smooth loss: 2.33159\n",
      "Best model at iteration: 3354 | smooth loss: 2.33139\n",
      "Best model at iteration: 3355 | smooth loss: 2.33088\n",
      "Best model at iteration: 3356 | smooth loss: 2.33082\n",
      "Best model at iteration: 3357 | smooth loss: 2.33050\n",
      "Best model at iteration: 3361 | smooth loss: 2.33043\n",
      "Best model at iteration: 3362 | smooth loss: 2.33037\n",
      "Best model at iteration: 3363 | smooth loss: 2.32992\n",
      "Best model at iteration: 3364 | smooth loss: 2.32990\n",
      "Best model at iteration: 3366 | smooth loss: 2.32955\n",
      "Best model at iteration: 3367 | smooth loss: 2.32953\n",
      "Best model at iteration: 3369 | smooth loss: 2.32944\n",
      "Best model at iteration: 3375 | smooth loss: 2.32919\n",
      "Best model at iteration: 3376 | smooth loss: 2.32897\n",
      "Best model at iteration: 3377 | smooth loss: 2.32879\n",
      "Best model at iteration: 3378 | smooth loss: 2.32874\n",
      "Best model at iteration: 3386 | smooth loss: 2.32856\n",
      "Best model at iteration: 3387 | smooth loss: 2.32837\n",
      "Best model at iteration: 3388 | smooth loss: 2.32825\n",
      "Best model at iteration: 3390 | smooth loss: 2.32802\n",
      "Best model at iteration: 3391 | smooth loss: 2.32753\n",
      "Best model at iteration: 3392 | smooth loss: 2.32728\n",
      "Best model at iteration: 3393 | smooth loss: 2.32662\n",
      "Best model at iteration: 3394 | smooth loss: 2.32615\n",
      "Best model at iteration: 3398 | smooth loss: 2.32609\n",
      "Best model at iteration: 3399 | smooth loss: 2.32593\n",
      "Best model at iteration: 3400 | smooth loss: 2.32580\n",
      "Best model at iteration: 3409 | smooth loss: 2.32561\n",
      "Best model at iteration: 3410 | smooth loss: 2.32530\n",
      "Best model at iteration: 3411 | smooth loss: 2.32528\n",
      "Best model at iteration: 3412 | smooth loss: 2.32505\n",
      "Best model at iteration: 3413 | smooth loss: 2.32426\n",
      "Best model at iteration: 3415 | smooth loss: 2.32402\n",
      "Best model at iteration: 3419 | smooth loss: 2.32376\n",
      "Best model at iteration: 3420 | smooth loss: 2.32374\n",
      "Best model at iteration: 3421 | smooth loss: 2.32338\n",
      "Best model at iteration: 3425 | smooth loss: 2.32325\n",
      "Best model at iteration: 3426 | smooth loss: 2.32323\n",
      "Best model at iteration: 3427 | smooth loss: 2.32304\n",
      "Best model at iteration: 3428 | smooth loss: 2.32278\n",
      "Best model at iteration: 3438 | smooth loss: 2.32264\n",
      "Best model at iteration: 3439 | smooth loss: 2.32211\n",
      "Best model at iteration: 3441 | smooth loss: 2.32207\n",
      "Best model at iteration: 3442 | smooth loss: 2.32150\n",
      "Best model at iteration: 3443 | smooth loss: 2.32116\n",
      "Best model at iteration: 3444 | smooth loss: 2.32096\n",
      "Best model at iteration: 3445 | smooth loss: 2.32039\n",
      "Best model at iteration: 3446 | smooth loss: 2.32016\n",
      "Best model at iteration: 3449 | smooth loss: 2.31986\n",
      "Best model at iteration: 3450 | smooth loss: 2.31928\n",
      "Best model at iteration: 3451 | smooth loss: 2.31921\n",
      "Best model at iteration: 3452 | smooth loss: 2.31902\n",
      "Best model at iteration: 3453 | smooth loss: 2.31884\n",
      "Best model at iteration: 3458 | smooth loss: 2.31851\n",
      "Best model at iteration: 3459 | smooth loss: 2.31823\n",
      "Best model at iteration: 3460 | smooth loss: 2.31817\n",
      "Best model at iteration: 3462 | smooth loss: 2.31780\n",
      "Best model at iteration: 3463 | smooth loss: 2.31753\n",
      "Best model at iteration: 3464 | smooth loss: 2.31743\n",
      "Best model at iteration: 3465 | smooth loss: 2.31705\n",
      "Best model at iteration: 3466 | smooth loss: 2.31673\n",
      "Best model at iteration: 3468 | smooth loss: 2.31652\n",
      "Best model at iteration: 3469 | smooth loss: 2.31621\n",
      "Best model at iteration: 3471 | smooth loss: 2.31619\n",
      "Best model at iteration: 3472 | smooth loss: 2.31596\n",
      "Best model at iteration: 3476 | smooth loss: 2.31556\n",
      "Best model at iteration: 3477 | smooth loss: 2.31525\n",
      "Best model at iteration: 3478 | smooth loss: 2.31497\n",
      "Best model at iteration: 3479 | smooth loss: 2.31485\n",
      "Best model at iteration: 3486 | smooth loss: 2.31483\n",
      "Best model at iteration: 3487 | smooth loss: 2.31446\n",
      "Best model at iteration: 3489 | smooth loss: 2.31414\n",
      "Best model at iteration: 3490 | smooth loss: 2.31383\n",
      "Best model at iteration: 3491 | smooth loss: 2.31340\n",
      "Best model at iteration: 3493 | smooth loss: 2.31334\n",
      "Best model at iteration: 3494 | smooth loss: 2.31284\n",
      "Best model at iteration: 3495 | smooth loss: 2.31265\n",
      "Best model at iteration: 3496 | smooth loss: 2.31246\n",
      "Best model at iteration: 3497 | smooth loss: 2.31176\n",
      "Best model at iteration: 3499 | smooth loss: 2.31168\n",
      "Best model at iteration: 3500 | smooth loss: 2.31168\n",
      "Best model at iteration: 3502 | smooth loss: 2.31125\n",
      "Best model at iteration: 3503 | smooth loss: 2.31093\n",
      "Best model at iteration: 3504 | smooth loss: 2.31026\n",
      "Best model at iteration: 3505 | smooth loss: 2.30999\n",
      "Best model at iteration: 3506 | smooth loss: 2.30970\n",
      "Best model at iteration: 3510 | smooth loss: 2.30945\n",
      "Best model at iteration: 3511 | smooth loss: 2.30902\n",
      "Best model at iteration: 3512 | smooth loss: 2.30892\n",
      "Best model at iteration: 3513 | smooth loss: 2.30875\n",
      "Best model at iteration: 3514 | smooth loss: 2.30828\n",
      "Best model at iteration: 3515 | smooth loss: 2.30823\n",
      "Best model at iteration: 3516 | smooth loss: 2.30788\n",
      "Best model at iteration: 3517 | smooth loss: 2.30747\n",
      "Best model at iteration: 3518 | smooth loss: 2.30741\n",
      "Best model at iteration: 3519 | smooth loss: 2.30723\n",
      "Best model at iteration: 3520 | smooth loss: 2.30632\n",
      "Best model at iteration: 3521 | smooth loss: 2.30575\n",
      "Best model at iteration: 3522 | smooth loss: 2.30515\n",
      "Best model at iteration: 3524 | smooth loss: 2.30463\n",
      "Best model at iteration: 3525 | smooth loss: 2.30444\n",
      "Best model at iteration: 3526 | smooth loss: 2.30372\n",
      "Best model at iteration: 3538 | smooth loss: 2.30347\n",
      "Best model at iteration: 3539 | smooth loss: 2.30258\n",
      "Best model at iteration: 3544 | smooth loss: 2.30246\n",
      "Best model at iteration: 3546 | smooth loss: 2.30215\n",
      "Best model at iteration: 3547 | smooth loss: 2.30160\n",
      "Best model at iteration: 3548 | smooth loss: 2.30115\n",
      "Best model at iteration: 3564 | smooth loss: 2.30102\n",
      "Best model at iteration: 3565 | smooth loss: 2.30027\n",
      "Best model at iteration: 3566 | smooth loss: 2.29987\n",
      "Best model at iteration: 3569 | smooth loss: 2.29975\n",
      "Best model at iteration: 3572 | smooth loss: 2.29962\n",
      "Best model at iteration: 3573 | smooth loss: 2.29917\n",
      "Best model at iteration: 3574 | smooth loss: 2.29899\n",
      "Best model at iteration: 3575 | smooth loss: 2.29828\n",
      "Best model at iteration: 3576 | smooth loss: 2.29795\n",
      "Best model at iteration: 3577 | smooth loss: 2.29775\n",
      "Best model at iteration: 3578 | smooth loss: 2.29736\n",
      "Best model at iteration: 3579 | smooth loss: 2.29711\n",
      "Best model at iteration: 3581 | smooth loss: 2.29707\n",
      "Best model at iteration: 3583 | smooth loss: 2.29638\n",
      "Best model at iteration: 3584 | smooth loss: 2.29601\n",
      "Best model at iteration: 3585 | smooth loss: 2.29574\n",
      "Best model at iteration: 3586 | smooth loss: 2.29515\n",
      "Best model at iteration: 3587 | smooth loss: 2.29489\n",
      "Best model at iteration: 3594 | smooth loss: 2.29483\n",
      "Best model at iteration: 3596 | smooth loss: 2.29455\n",
      "Best model at iteration: 3597 | smooth loss: 2.29451\n",
      "Best model at iteration: 3602 | smooth loss: 2.29444\n",
      "Best model at iteration: 3604 | smooth loss: 2.29376\n",
      "Best model at iteration: 3607 | smooth loss: 2.29351\n",
      "Best model at iteration: 3608 | smooth loss: 2.29324\n",
      "Best model at iteration: 3609 | smooth loss: 2.29292\n",
      "Best model at iteration: 3610 | smooth loss: 2.29264\n",
      "Best model at iteration: 3611 | smooth loss: 2.29262\n",
      "Best model at iteration: 3616 | smooth loss: 2.29215\n",
      "Best model at iteration: 3617 | smooth loss: 2.29147\n",
      "Best model at iteration: 3618 | smooth loss: 2.29059\n",
      "Best model at iteration: 3619 | smooth loss: 2.29039\n",
      "Best model at iteration: 3627 | smooth loss: 2.29014\n",
      "Best model at iteration: 3628 | smooth loss: 2.29002\n",
      "Best model at iteration: 3631 | smooth loss: 2.28964\n",
      "Best model at iteration: 3632 | smooth loss: 2.28903\n",
      "Best model at iteration: 3633 | smooth loss: 2.28891\n",
      "Best model at iteration: 3693 | smooth loss: 2.28849\n",
      "Best model at iteration: 3694 | smooth loss: 2.28828\n",
      "Best model at iteration: 3695 | smooth loss: 2.28798\n",
      "Best model at iteration: 3696 | smooth loss: 2.28777\n",
      "Best model at iteration: 3697 | smooth loss: 2.28748\n",
      "Best model at iteration: 3698 | smooth loss: 2.28707\n",
      "Best model at iteration: 3700 | smooth loss: 2.28694\n",
      "Best model at iteration: 3701 | smooth loss: 2.28684\n",
      "Best model at iteration: 3721 | smooth loss: 2.28668\n",
      "Best model at iteration: 3722 | smooth loss: 2.28651\n",
      "Best model at iteration: 3723 | smooth loss: 2.28629\n",
      "Best model at iteration: 3727 | smooth loss: 2.28565\n",
      "Best model at iteration: 3728 | smooth loss: 2.28532\n",
      "Best model at iteration: 3729 | smooth loss: 2.28531\n",
      "Best model at iteration: 3730 | smooth loss: 2.28518\n",
      "Best model at iteration: 3734 | smooth loss: 2.28501\n",
      "Best model at iteration: 3735 | smooth loss: 2.28456\n",
      "Best model at iteration: 3736 | smooth loss: 2.28424\n",
      "Best model at iteration: 3737 | smooth loss: 2.28399\n",
      "Best model at iteration: 3741 | smooth loss: 2.28369\n",
      "Best model at iteration: 3748 | smooth loss: 2.28312\n",
      "Best model at iteration: 3749 | smooth loss: 2.28274\n",
      "Best model at iteration: 3751 | smooth loss: 2.28241\n",
      "Best model at iteration: 3752 | smooth loss: 2.28228\n",
      "Best model at iteration: 3753 | smooth loss: 2.28205\n",
      "Best model at iteration: 3776 | smooth loss: 2.28178\n",
      "Best model at iteration: 3777 | smooth loss: 2.28162\n",
      "Best model at iteration: 3778 | smooth loss: 2.28132\n",
      "Best model at iteration: 3806 | smooth loss: 2.28127\n",
      "Best model at iteration: 3807 | smooth loss: 2.28122\n",
      "Best model at iteration: 3808 | smooth loss: 2.28106\n",
      "Best model at iteration: 3809 | smooth loss: 2.28062\n",
      "Best model at iteration: 3810 | smooth loss: 2.28036\n",
      "Best model at iteration: 3811 | smooth loss: 2.27992\n",
      "Best model at iteration: 3812 | smooth loss: 2.27949\n",
      "Best model at iteration: 3813 | smooth loss: 2.27934\n",
      "Best model at iteration: 3815 | smooth loss: 2.27925\n",
      "Best model at iteration: 3818 | smooth loss: 2.27858\n",
      "Best model at iteration: 3819 | smooth loss: 2.27818\n",
      "Best model at iteration: 3820 | smooth loss: 2.27806\n",
      "Best model at iteration: 3821 | smooth loss: 2.27800\n",
      "Best model at iteration: 3822 | smooth loss: 2.27793\n",
      "Best model at iteration: 3824 | smooth loss: 2.27767\n",
      "Best model at iteration: 3825 | smooth loss: 2.27740\n",
      "Best model at iteration: 3826 | smooth loss: 2.27731\n",
      "Best model at iteration: 3833 | smooth loss: 2.27693\n",
      "Best model at iteration: 3834 | smooth loss: 2.27632\n",
      "Best model at iteration: 3835 | smooth loss: 2.27627\n",
      "Best model at iteration: 3836 | smooth loss: 2.27596\n",
      "Best model at iteration: 3838 | smooth loss: 2.27586\n",
      "Best model at iteration: 3839 | smooth loss: 2.27563\n",
      "Best model at iteration: 3840 | smooth loss: 2.27532\n",
      "Best model at iteration: 3843 | smooth loss: 2.27503\n",
      "Best model at iteration: 3845 | smooth loss: 2.27480\n",
      "Best model at iteration: 3846 | smooth loss: 2.27437\n",
      "Best model at iteration: 3847 | smooth loss: 2.27425\n",
      "Best model at iteration: 3848 | smooth loss: 2.27400\n",
      "Best model at iteration: 3849 | smooth loss: 2.27369\n",
      "Best model at iteration: 3851 | smooth loss: 2.27348\n",
      "Best model at iteration: 3855 | smooth loss: 2.27327\n",
      "Best model at iteration: 3856 | smooth loss: 2.27305\n",
      "Best model at iteration: 3859 | smooth loss: 2.27278\n",
      "Best model at iteration: 3861 | smooth loss: 2.27259\n",
      "Best model at iteration: 3863 | smooth loss: 2.27245\n",
      "Best model at iteration: 3865 | smooth loss: 2.27241\n",
      "Best model at iteration: 3866 | smooth loss: 2.27235\n",
      "Best model at iteration: 3872 | smooth loss: 2.27229\n",
      "Best model at iteration: 3873 | smooth loss: 2.27201\n",
      "Best model at iteration: 3874 | smooth loss: 2.27190\n",
      "Best model at iteration: 3875 | smooth loss: 2.27128\n",
      "Best model at iteration: 3876 | smooth loss: 2.27083\n",
      "Best model at iteration: 3903 | smooth loss: 2.27041\n",
      "Best model at iteration: 3904 | smooth loss: 2.27003\n",
      "Best model at iteration: 3905 | smooth loss: 2.26934\n",
      "Best model at iteration: 3906 | smooth loss: 2.26863\n",
      "Best model at iteration: 3908 | smooth loss: 2.26862\n",
      "Best model at iteration: 3909 | smooth loss: 2.26836\n",
      "Best model at iteration: 3910 | smooth loss: 2.26815\n",
      "Best model at iteration: 3911 | smooth loss: 2.26775\n",
      "Best model at iteration: 3912 | smooth loss: 2.26741\n",
      "Best model at iteration: 3913 | smooth loss: 2.26711\n",
      "Best model at iteration: 3917 | smooth loss: 2.26708\n",
      "Best model at iteration: 3918 | smooth loss: 2.26685\n",
      "Best model at iteration: 3931 | smooth loss: 2.26666\n",
      "Best model at iteration: 3932 | smooth loss: 2.26602\n",
      "Best model at iteration: 3933 | smooth loss: 2.26592\n",
      "Best model at iteration: 3934 | smooth loss: 2.26577\n",
      "Best model at iteration: 3938 | smooth loss: 2.26561\n",
      "Best model at iteration: 3939 | smooth loss: 2.26556\n",
      "Best model at iteration: 3946 | smooth loss: 2.26549\n",
      "Best model at iteration: 3949 | smooth loss: 2.26543\n",
      "Best model at iteration: 3950 | smooth loss: 2.26534\n",
      "Best model at iteration: 3951 | smooth loss: 2.26482\n",
      "Best model at iteration: 3953 | smooth loss: 2.26469\n",
      "Best model at iteration: 3954 | smooth loss: 2.26398\n",
      "Best model at iteration: 3955 | smooth loss: 2.26370\n",
      "Best model at iteration: 3956 | smooth loss: 2.26344\n",
      "Best model at iteration: 3957 | smooth loss: 2.26314\n",
      "Best model at iteration: 3958 | smooth loss: 2.26278\n",
      "Best model at iteration: 3959 | smooth loss: 2.26182\n",
      "Best model at iteration: 3960 | smooth loss: 2.26163\n",
      "Best model at iteration: 3961 | smooth loss: 2.26156\n",
      "Best model at iteration: 3962 | smooth loss: 2.26154\n",
      "Best model at iteration: 3964 | smooth loss: 2.26147\n",
      "Best model at iteration: 3965 | smooth loss: 2.26146\n",
      "Best model at iteration: 3973 | smooth loss: 2.26116\n",
      "Best model at iteration: 3975 | smooth loss: 2.26104\n",
      "Best model at iteration: 3976 | smooth loss: 2.26064\n",
      "Best model at iteration: 3977 | smooth loss: 2.26054\n",
      "Best model at iteration: 3978 | smooth loss: 2.25985\n",
      "Best model at iteration: 3979 | smooth loss: 2.25980\n",
      "Best model at iteration: 3981 | smooth loss: 2.25954\n",
      "Best model at iteration: 3982 | smooth loss: 2.25951\n",
      "Best model at iteration: 3983 | smooth loss: 2.25921\n",
      "Best model at iteration: 3997 | smooth loss: 2.25918\n",
      "Best model at iteration: 4000 | smooth loss: 2.25907\n",
      "iter = 4000, smooth loss=2.259066106731833\n",
      " ticbofting yit nof midAim... Mpeithey the Ctitingon, bule Dhat ong thend treyont fad to fotey ilthillohe wist ef thatrey.\n",
      "\"Thing.\n",
      "\"Woo't lugrutowing ynowh bedincing aid whwarkeef oa intaking\"n dArd a\n",
      "\n",
      "Best model at iteration: 4002 | smooth loss: 2.25877\n",
      "Best model at iteration: 4003 | smooth loss: 2.25853\n",
      "Best model at iteration: 4005 | smooth loss: 2.25839\n",
      "Best model at iteration: 4011 | smooth loss: 2.25825\n",
      "Best model at iteration: 4012 | smooth loss: 2.25803\n",
      "Best model at iteration: 4014 | smooth loss: 2.25800\n",
      "Best model at iteration: 4015 | smooth loss: 2.25779\n",
      "Best model at iteration: 4018 | smooth loss: 2.25742\n",
      "Best model at iteration: 4019 | smooth loss: 2.25647\n",
      "Best model at iteration: 4020 | smooth loss: 2.25602\n",
      "Best model at iteration: 4026 | smooth loss: 2.25592\n",
      "Best model at iteration: 4028 | smooth loss: 2.25590\n",
      "Best model at iteration: 4029 | smooth loss: 2.25564\n",
      "Best model at iteration: 4030 | smooth loss: 2.25523\n",
      "Best model at iteration: 4031 | smooth loss: 2.25503\n",
      "Best model at iteration: 4035 | smooth loss: 2.25455\n",
      "Best model at iteration: 4036 | smooth loss: 2.25415\n",
      "Best model at iteration: 4037 | smooth loss: 2.25324\n",
      "Best model at iteration: 4038 | smooth loss: 2.25264\n",
      "Best model at iteration: 4039 | smooth loss: 2.25230\n",
      "Best model at iteration: 4041 | smooth loss: 2.25198\n",
      "Best model at iteration: 4042 | smooth loss: 2.25170\n",
      "Best model at iteration: 4044 | smooth loss: 2.25164\n",
      "Best model at iteration: 4055 | smooth loss: 2.25147\n",
      "Best model at iteration: 4056 | smooth loss: 2.25104\n",
      "Best model at iteration: 4059 | smooth loss: 2.25101\n",
      "Best model at iteration: 4061 | smooth loss: 2.25064\n",
      "Best model at iteration: 4062 | smooth loss: 2.25029\n",
      "Best model at iteration: 4069 | smooth loss: 2.25020\n",
      "Best model at iteration: 4070 | smooth loss: 2.24989\n",
      "Best model at iteration: 4071 | smooth loss: 2.24918\n",
      "Best model at iteration: 4072 | smooth loss: 2.24909\n",
      "Best model at iteration: 4108 | smooth loss: 2.24895\n",
      "Best model at iteration: 4111 | smooth loss: 2.24888\n",
      "Best model at iteration: 4112 | smooth loss: 2.24844\n",
      "Best model at iteration: 4113 | smooth loss: 2.24740\n",
      "Best model at iteration: 4115 | smooth loss: 2.24697\n",
      "Best model at iteration: 4117 | smooth loss: 2.24683\n",
      "Best model at iteration: 4119 | smooth loss: 2.24656\n",
      "Best model at iteration: 4121 | smooth loss: 2.24553\n",
      "Best model at iteration: 4122 | smooth loss: 2.24494\n",
      "Best model at iteration: 4123 | smooth loss: 2.24476\n",
      "Best model at iteration: 4124 | smooth loss: 2.24473\n",
      "Best model at iteration: 4125 | smooth loss: 2.24470\n",
      "Best model at iteration: 4126 | smooth loss: 2.24414\n",
      "Best model at iteration: 4127 | smooth loss: 2.24398\n",
      "Best model at iteration: 4128 | smooth loss: 2.24394\n",
      "Best model at iteration: 4131 | smooth loss: 2.24390\n",
      "Best model at iteration: 4132 | smooth loss: 2.24343\n",
      "Best model at iteration: 4133 | smooth loss: 2.24336\n",
      "Best model at iteration: 4134 | smooth loss: 2.24327\n",
      "Best model at iteration: 4136 | smooth loss: 2.24268\n",
      "Best model at iteration: 4137 | smooth loss: 2.24259\n",
      "Best model at iteration: 4138 | smooth loss: 2.24237\n",
      "Best model at iteration: 4139 | smooth loss: 2.24229\n",
      "Best model at iteration: 4142 | smooth loss: 2.24180\n",
      "Best model at iteration: 4143 | smooth loss: 2.24170\n",
      "Best model at iteration: 4144 | smooth loss: 2.24167\n",
      "Best model at iteration: 4145 | smooth loss: 2.24109\n",
      "Best model at iteration: 4146 | smooth loss: 2.24101\n",
      "Best model at iteration: 4173 | smooth loss: 2.24098\n",
      "Best model at iteration: 4174 | smooth loss: 2.24089\n",
      "Best model at iteration: 4176 | smooth loss: 2.24080\n",
      "Best model at iteration: 4177 | smooth loss: 2.24073\n",
      "Best model at iteration: 4178 | smooth loss: 2.24056\n",
      "Best model at iteration: 4180 | smooth loss: 2.24032\n",
      "Best model at iteration: 4185 | smooth loss: 2.24028\n",
      "Best model at iteration: 4186 | smooth loss: 2.24027\n",
      "Best model at iteration: 4187 | smooth loss: 2.24021\n",
      "Best model at iteration: 4188 | smooth loss: 2.23988\n",
      "Best model at iteration: 4192 | smooth loss: 2.23947\n",
      "Best model at iteration: 4196 | smooth loss: 2.23938\n",
      "Best model at iteration: 4206 | smooth loss: 2.23908\n",
      "Best model at iteration: 4211 | smooth loss: 2.23902\n",
      "Best model at iteration: 4214 | smooth loss: 2.23899\n",
      "Best model at iteration: 4215 | smooth loss: 2.23893\n",
      "Best model at iteration: 4219 | smooth loss: 2.23842\n",
      "Best model at iteration: 4220 | smooth loss: 2.23815\n",
      "Best model at iteration: 4221 | smooth loss: 2.23793\n",
      "Best model at iteration: 4222 | smooth loss: 2.23739\n",
      "Best model at iteration: 4234 | smooth loss: 2.23724\n",
      "Best model at iteration: 4235 | smooth loss: 2.23670\n",
      "Best model at iteration: 4239 | smooth loss: 2.23661\n",
      "Best model at iteration: 4240 | smooth loss: 2.23643\n",
      "Best model at iteration: 4241 | smooth loss: 2.23637\n",
      "Best model at iteration: 4242 | smooth loss: 2.23620\n",
      "Best model at iteration: 4243 | smooth loss: 2.23537\n",
      "Best model at iteration: 4244 | smooth loss: 2.23507\n",
      "Best model at iteration: 4245 | smooth loss: 2.23441\n",
      "Best model at iteration: 4247 | smooth loss: 2.23438\n",
      "Best model at iteration: 4248 | smooth loss: 2.23401\n",
      "Best model at iteration: 4249 | smooth loss: 2.23387\n",
      "Best model at iteration: 4250 | smooth loss: 2.23386\n",
      "Best model at iteration: 4251 | smooth loss: 2.23364\n",
      "Best model at iteration: 4260 | smooth loss: 2.23350\n",
      "Best model at iteration: 4261 | smooth loss: 2.23297\n",
      "Best model at iteration: 4262 | smooth loss: 2.23233\n",
      "Best model at iteration: 4270 | smooth loss: 2.23208\n",
      "Best model at iteration: 4316 | smooth loss: 2.23155\n",
      "Best model at iteration: 4321 | smooth loss: 2.23130\n",
      "Best model at iteration: 4322 | smooth loss: 2.23101\n",
      "Best model at iteration: 4323 | smooth loss: 2.23088\n",
      "Best model at iteration: 4324 | smooth loss: 2.23066\n",
      "Best model at iteration: 4325 | smooth loss: 2.23061\n",
      "Best model at iteration: 4326 | smooth loss: 2.23042\n",
      "Best model at iteration: 4327 | smooth loss: 2.23036\n",
      "Best model at iteration: 4333 | smooth loss: 2.23015\n",
      "Best model at iteration: 4334 | smooth loss: 2.23007\n",
      "Best model at iteration: 4335 | smooth loss: 2.22987\n",
      "Best model at iteration: 4337 | smooth loss: 2.22956\n",
      "Best model at iteration: 4338 | smooth loss: 2.22925\n",
      "Best model at iteration: 4339 | smooth loss: 2.22896\n",
      "Best model at iteration: 4341 | smooth loss: 2.22869\n",
      "Best model at iteration: 4344 | smooth loss: 2.22865\n",
      "Best model at iteration: 4345 | smooth loss: 2.22864\n",
      "Best model at iteration: 4346 | smooth loss: 2.22845\n",
      "Best model at iteration: 4361 | smooth loss: 2.22802\n",
      "Best model at iteration: 4362 | smooth loss: 2.22755\n",
      "Best model at iteration: 4370 | smooth loss: 2.22748\n",
      "Best model at iteration: 4371 | smooth loss: 2.22700\n",
      "Best model at iteration: 4372 | smooth loss: 2.22652\n",
      "Best model at iteration: 4373 | smooth loss: 2.22596\n",
      "Best model at iteration: 4374 | smooth loss: 2.22565\n",
      "Best model at iteration: 4377 | smooth loss: 2.22486\n",
      "Best model at iteration: 4378 | smooth loss: 2.22452\n",
      "Best model at iteration: 4379 | smooth loss: 2.22414\n",
      "Best model at iteration: 4384 | smooth loss: 2.22402\n",
      "Best model at iteration: 4411 | smooth loss: 2.22381\n",
      "Best model at iteration: 4413 | smooth loss: 2.22366\n",
      "Best model at iteration: 4414 | smooth loss: 2.22315\n",
      "Best model at iteration: 4416 | smooth loss: 2.22305\n",
      "Best model at iteration: 4417 | smooth loss: 2.22292\n",
      "Best model at iteration: 4419 | smooth loss: 2.22250\n",
      "Best model at iteration: 4420 | smooth loss: 2.22209\n",
      "Best model at iteration: 4421 | smooth loss: 2.22199\n",
      "Best model at iteration: 4422 | smooth loss: 2.22187\n",
      "Best model at iteration: 4423 | smooth loss: 2.22112\n",
      "Best model at iteration: 4424 | smooth loss: 2.22098\n",
      "Best model at iteration: 4425 | smooth loss: 2.22043\n",
      "Best model at iteration: 4507 | smooth loss: 2.22042\n",
      "Best model at iteration: 4508 | smooth loss: 2.22014\n",
      "Best model at iteration: 4509 | smooth loss: 2.22002\n",
      "Best model at iteration: 4513 | smooth loss: 2.21995\n",
      "Best model at iteration: 4515 | smooth loss: 2.21946\n",
      "Best model at iteration: 4517 | smooth loss: 2.21911\n",
      "Best model at iteration: 4518 | smooth loss: 2.21883\n",
      "Best model at iteration: 4519 | smooth loss: 2.21755\n",
      "Best model at iteration: 4520 | smooth loss: 2.21736\n",
      "Best model at iteration: 4521 | smooth loss: 2.21726\n",
      "Best model at iteration: 4522 | smooth loss: 2.21704\n",
      "Best model at iteration: 4523 | smooth loss: 2.21621\n",
      "Best model at iteration: 4524 | smooth loss: 2.21616\n",
      "Best model at iteration: 4525 | smooth loss: 2.21589\n",
      "Best model at iteration: 4527 | smooth loss: 2.21540\n",
      "Best model at iteration: 4528 | smooth loss: 2.21503\n",
      "Best model at iteration: 4529 | smooth loss: 2.21465\n",
      "Best model at iteration: 4530 | smooth loss: 2.21420\n",
      "Best model at iteration: 4531 | smooth loss: 2.21408\n",
      "Best model at iteration: 4542 | smooth loss: 2.21385\n",
      "Best model at iteration: 4543 | smooth loss: 2.21280\n",
      "Best model at iteration: 4544 | smooth loss: 2.21252\n",
      "Best model at iteration: 4547 | smooth loss: 2.21203\n",
      "Best model at iteration: 4548 | smooth loss: 2.21200\n",
      "Best model at iteration: 4549 | smooth loss: 2.21129\n",
      "Best model at iteration: 4550 | smooth loss: 2.21084\n",
      "Best model at iteration: 4556 | smooth loss: 2.21056\n",
      "Best model at iteration: 4557 | smooth loss: 2.21045\n",
      "Best model at iteration: 4558 | smooth loss: 2.21030\n",
      "Best model at iteration: 4559 | smooth loss: 2.20991\n",
      "Best model at iteration: 4560 | smooth loss: 2.20974\n",
      "Best model at iteration: 4561 | smooth loss: 2.20953\n",
      "Best model at iteration: 4565 | smooth loss: 2.20909\n",
      "Best model at iteration: 4566 | smooth loss: 2.20870\n",
      "Best model at iteration: 4567 | smooth loss: 2.20819\n",
      "Best model at iteration: 4572 | smooth loss: 2.20808\n",
      "Best model at iteration: 4578 | smooth loss: 2.20790\n",
      "Best model at iteration: 4580 | smooth loss: 2.20718\n",
      "Best model at iteration: 4581 | smooth loss: 2.20689\n",
      "Best model at iteration: 4586 | smooth loss: 2.20679\n",
      "Best model at iteration: 4589 | smooth loss: 2.20669\n",
      "Best model at iteration: 4590 | smooth loss: 2.20660\n",
      "Best model at iteration: 4591 | smooth loss: 2.20653\n",
      "Best model at iteration: 4594 | smooth loss: 2.20643\n",
      "Best model at iteration: 4600 | smooth loss: 2.20570\n",
      "Best model at iteration: 4601 | smooth loss: 2.20554\n",
      "Best model at iteration: 4602 | smooth loss: 2.20452\n",
      "Best model at iteration: 4605 | smooth loss: 2.20423\n",
      "Best model at iteration: 4612 | smooth loss: 2.20387\n",
      "Best model at iteration: 4613 | smooth loss: 2.20375\n",
      "Best model at iteration: 4691 | smooth loss: 2.20365\n",
      "Best model at iteration: 4704 | smooth loss: 2.20364\n",
      "Best model at iteration: 4705 | smooth loss: 2.20360\n",
      "Best model at iteration: 4706 | smooth loss: 2.20302\n",
      "Best model at iteration: 4707 | smooth loss: 2.20280\n",
      "Best model at iteration: 4708 | smooth loss: 2.20258\n",
      "Best model at iteration: 4710 | smooth loss: 2.20255\n",
      "Best model at iteration: 4713 | smooth loss: 2.20233\n",
      "Best model at iteration: 4714 | smooth loss: 2.20186\n",
      "Best model at iteration: 4715 | smooth loss: 2.20156\n",
      "Best model at iteration: 4716 | smooth loss: 2.20066\n",
      "Best model at iteration: 4730 | smooth loss: 2.20064\n",
      "Best model at iteration: 4731 | smooth loss: 2.20011\n",
      "Best model at iteration: 4732 | smooth loss: 2.20004\n",
      "Best model at iteration: 4733 | smooth loss: 2.19972\n",
      "Best model at iteration: 4734 | smooth loss: 2.19964\n",
      "Best model at iteration: 4736 | smooth loss: 2.19919\n",
      "Best model at iteration: 4737 | smooth loss: 2.19899\n",
      "Best model at iteration: 4738 | smooth loss: 2.19865\n",
      "Best model at iteration: 4739 | smooth loss: 2.19850\n",
      "Best model at iteration: 4743 | smooth loss: 2.19848\n",
      "Best model at iteration: 4761 | smooth loss: 2.19813\n",
      "Best model at iteration: 4763 | smooth loss: 2.19789\n",
      "Best model at iteration: 4772 | smooth loss: 2.19756\n",
      "Best model at iteration: 4773 | smooth loss: 2.19708\n",
      "Best model at iteration: 4779 | smooth loss: 2.19654\n",
      "Best model at iteration: 4780 | smooth loss: 2.19635\n",
      "Best model at iteration: 4781 | smooth loss: 2.19573\n",
      "Best model at iteration: 4782 | smooth loss: 2.19533\n",
      "Best model at iteration: 4785 | smooth loss: 2.19514\n",
      "Best model at iteration: 4786 | smooth loss: 2.19472\n",
      "Best model at iteration: 4787 | smooth loss: 2.19460\n",
      "Best model at iteration: 4789 | smooth loss: 2.19432\n",
      "Best model at iteration: 4790 | smooth loss: 2.19401\n",
      "Best model at iteration: 4798 | smooth loss: 2.19394\n",
      "Best model at iteration: 4799 | smooth loss: 2.19319\n",
      "Best model at iteration: 4800 | smooth loss: 2.19306\n",
      "Best model at iteration: 4801 | smooth loss: 2.19269\n",
      "Best model at iteration: 4804 | smooth loss: 2.19256\n",
      "Best model at iteration: 4805 | smooth loss: 2.19232\n",
      "Best model at iteration: 4806 | smooth loss: 2.19204\n",
      "Best model at iteration: 4807 | smooth loss: 2.19189\n",
      "Best model at iteration: 4808 | smooth loss: 2.19171\n",
      "Best model at iteration: 4810 | smooth loss: 2.19144\n",
      "Best model at iteration: 4811 | smooth loss: 2.19131\n",
      "Best model at iteration: 4812 | smooth loss: 2.19074\n",
      "Best model at iteration: 4813 | smooth loss: 2.19055\n",
      "Best model at iteration: 4814 | smooth loss: 2.19008\n",
      "Best model at iteration: 4821 | smooth loss: 2.18963\n",
      "Best model at iteration: 4822 | smooth loss: 2.18935\n",
      "Best model at iteration: 4832 | smooth loss: 2.18895\n",
      "Best model at iteration: 4833 | smooth loss: 2.18890\n",
      "Best model at iteration: 4867 | smooth loss: 2.18890\n",
      "Best model at iteration: 4871 | smooth loss: 2.18889\n",
      "Best model at iteration: 4872 | smooth loss: 2.18871\n",
      "Best model at iteration: 4874 | smooth loss: 2.18857\n",
      "Best model at iteration: 4996 | smooth loss: 2.18800\n",
      "Best model at iteration: 4997 | smooth loss: 2.18749\n",
      "Best model at iteration: 4998 | smooth loss: 2.18749\n",
      "Best model at iteration: 4999 | smooth loss: 2.18731\n",
      "Best model at iteration: 5000 | smooth loss: 2.18674\n",
      "iter = 5000, smooth loss=2.1867372207159392\n",
      "nge blowsivere tpouk- ppompady wes or the waght urgithrreang n, as Rot th.  \"Jme Weat have cothite ghim bavingald Magga llo.\n",
      "\n",
      "Harut, for bway,\" aver vered, ee oll ppaingit wathed juscoungly loaded a w\n",
      "\n",
      "Best model at iteration: 5007 | smooth loss: 2.18660\n",
      "Best model at iteration: 5013 | smooth loss: 2.18607\n",
      "Best model at iteration: 5014 | smooth loss: 2.18578\n",
      "Best model at iteration: 5015 | smooth loss: 2.18522\n",
      "Best model at iteration: 5016 | smooth loss: 2.18504\n",
      "Best model at iteration: 5089 | smooth loss: 2.18493\n",
      "Best model at iteration: 5090 | smooth loss: 2.18430\n",
      "Best model at iteration: 5091 | smooth loss: 2.18418\n",
      "Best model at iteration: 5092 | smooth loss: 2.18365\n",
      "Best model at iteration: 5093 | smooth loss: 2.18350\n",
      "Best model at iteration: 5095 | smooth loss: 2.18305\n",
      "Best model at iteration: 5096 | smooth loss: 2.18255\n",
      "Best model at iteration: 5097 | smooth loss: 2.18192\n",
      "Best model at iteration: 5099 | smooth loss: 2.18189\n",
      "Best model at iteration: 5102 | smooth loss: 2.18131\n",
      "Best model at iteration: 5103 | smooth loss: 2.18105\n",
      "Best model at iteration: 5104 | smooth loss: 2.18074\n",
      "Best model at iteration: 5105 | smooth loss: 2.18069\n",
      "Best model at iteration: 5107 | smooth loss: 2.18020\n",
      "Best model at iteration: 5108 | smooth loss: 2.17988\n",
      "Best model at iteration: 5109 | smooth loss: 2.17978\n",
      "Best model at iteration: 5110 | smooth loss: 2.17942\n",
      "Best model at iteration: 5111 | smooth loss: 2.17918\n",
      "Best model at iteration: 5112 | smooth loss: 2.17888\n",
      "Best model at iteration: 5113 | smooth loss: 2.17854\n",
      "Best model at iteration: 5116 | smooth loss: 2.17838\n",
      "Best model at iteration: 5119 | smooth loss: 2.17820\n",
      "Best model at iteration: 5120 | smooth loss: 2.17769\n",
      "Best model at iteration: 5122 | smooth loss: 2.17746\n",
      "Best model at iteration: 5123 | smooth loss: 2.17737\n",
      "Best model at iteration: 5472 | smooth loss: 2.17735\n",
      "Best model at iteration: 5473 | smooth loss: 2.17732\n",
      "Best model at iteration: 5474 | smooth loss: 2.17708\n",
      "Best model at iteration: 5476 | smooth loss: 2.17702\n",
      "Best model at iteration: 5481 | smooth loss: 2.17700\n",
      "Best model at iteration: 5505 | smooth loss: 2.17676\n",
      "Best model at iteration: 5511 | smooth loss: 2.17665\n",
      "Best model at iteration: 5513 | smooth loss: 2.17656\n",
      "Best model at iteration: 5516 | smooth loss: 2.17652\n",
      "Best model at iteration: 5519 | smooth loss: 2.17625\n",
      "Best model at iteration: 5520 | smooth loss: 2.17591\n",
      "Best model at iteration: 5521 | smooth loss: 2.17574\n",
      "Best model at iteration: 5522 | smooth loss: 2.17571\n",
      "Best model at iteration: 5523 | smooth loss: 2.17553\n",
      "Best model at iteration: 5530 | smooth loss: 2.17523\n",
      "Best model at iteration: 5531 | smooth loss: 2.17492\n",
      "Best model at iteration: 5532 | smooth loss: 2.17472\n",
      "Best model at iteration: 5533 | smooth loss: 2.17433\n",
      "Best model at iteration: 5538 | smooth loss: 2.17424\n",
      "Best model at iteration: 5545 | smooth loss: 2.17418\n",
      "Best model at iteration: 5551 | smooth loss: 2.17385\n",
      "Best model at iteration: 5554 | smooth loss: 2.17365\n",
      "Best model at iteration: 5555 | smooth loss: 2.17353\n",
      "Best model at iteration: 5556 | smooth loss: 2.17323\n",
      "Best model at iteration: 5623 | smooth loss: 2.17322\n",
      "Best model at iteration: 5624 | smooth loss: 2.17304\n",
      "Best model at iteration: 5625 | smooth loss: 2.17291\n",
      "Best model at iteration: 5626 | smooth loss: 2.17287\n",
      "Best model at iteration: 5627 | smooth loss: 2.17270\n",
      "Best model at iteration: 5633 | smooth loss: 2.17263\n",
      "Best model at iteration: 5634 | smooth loss: 2.17216\n",
      "Best model at iteration: 5638 | smooth loss: 2.17201\n",
      "Best model at iteration: 5639 | smooth loss: 2.17186\n",
      "Best model at iteration: 5640 | smooth loss: 2.17181\n",
      "Best model at iteration: 5641 | smooth loss: 2.17135\n",
      "Best model at iteration: 5642 | smooth loss: 2.17101\n",
      "Best model at iteration: 5643 | smooth loss: 2.17092\n",
      "Best model at iteration: 5644 | smooth loss: 2.17069\n",
      "Best model at iteration: 5645 | smooth loss: 2.17061\n",
      "Best model at iteration: 5648 | smooth loss: 2.17046\n",
      "Best model at iteration: 5649 | smooth loss: 2.17021\n",
      "Best model at iteration: 5651 | smooth loss: 2.16996\n",
      "Best model at iteration: 5652 | smooth loss: 2.16986\n",
      "Best model at iteration: 5653 | smooth loss: 2.16893\n",
      "Best model at iteration: 5654 | smooth loss: 2.16850\n",
      "Best model at iteration: 5716 | smooth loss: 2.16841\n",
      "Best model at iteration: 5717 | smooth loss: 2.16829\n",
      "Best model at iteration: 5718 | smooth loss: 2.16790\n",
      "Best model at iteration: 5719 | smooth loss: 2.16729\n",
      "Best model at iteration: 5720 | smooth loss: 2.16651\n",
      "Best model at iteration: 5721 | smooth loss: 2.16640\n",
      "Best model at iteration: 5723 | smooth loss: 2.16639\n",
      "Best model at iteration: 5733 | smooth loss: 2.16605\n",
      "Best model at iteration: 5735 | smooth loss: 2.16567\n",
      "Best model at iteration: 5736 | smooth loss: 2.16532\n",
      "Best model at iteration: 5737 | smooth loss: 2.16475\n",
      "Best model at iteration: 5738 | smooth loss: 2.16441\n",
      "Best model at iteration: 5742 | smooth loss: 2.16428\n",
      "Best model at iteration: 5743 | smooth loss: 2.16412\n",
      "iter = 6000, smooth loss=2.16607853499109\n",
      " bout layked Wiasl an whind co tKr.\" Woull As om wos n's, laldtwor and to tie qrouthe sive I yok a dowing rod on ssid... an thek ay thandoy and on in ward jesard wat spoundly upourdonguther gohes Harr\n",
      "\n",
      "Best model at iteration: 6099 | smooth loss: 2.16404\n",
      "Best model at iteration: 6100 | smooth loss: 2.16353\n",
      "Best model at iteration: 6207 | smooth loss: 2.16330\n",
      "Best model at iteration: 6208 | smooth loss: 2.16308\n",
      "Best model at iteration: 6216 | smooth loss: 2.16262\n",
      "Best model at iteration: 6217 | smooth loss: 2.16231\n",
      "Best model at iteration: 6218 | smooth loss: 2.16186\n",
      "Best model at iteration: 6219 | smooth loss: 2.16149\n",
      "Best model at iteration: 6220 | smooth loss: 2.16118\n",
      "Best model at iteration: 6224 | smooth loss: 2.16095\n",
      "Best model at iteration: 6225 | smooth loss: 2.16083\n",
      "Best model at iteration: 6226 | smooth loss: 2.16053\n",
      "Best model at iteration: 6227 | smooth loss: 2.16040\n",
      "Best model at iteration: 6228 | smooth loss: 2.16004\n",
      "Best model at iteration: 6229 | smooth loss: 2.15960\n",
      "Best model at iteration: 6230 | smooth loss: 2.15949\n",
      "Best model at iteration: 6267 | smooth loss: 2.15932\n",
      "Best model at iteration: 6268 | smooth loss: 2.15926\n",
      "Best model at iteration: 6272 | smooth loss: 2.15919\n",
      "Best model at iteration: 6274 | smooth loss: 2.15919\n",
      "Best model at iteration: 6275 | smooth loss: 2.15881\n",
      "Best model at iteration: 6276 | smooth loss: 2.15856\n",
      "Best model at iteration: 6277 | smooth loss: 2.15850\n",
      "Best model at iteration: 6278 | smooth loss: 2.15846\n",
      "Best model at iteration: 6836 | smooth loss: 2.15835\n",
      "Best model at iteration: 6838 | smooth loss: 2.15793\n",
      "Best model at iteration: 6840 | smooth loss: 2.15735\n",
      "Best model at iteration: 6841 | smooth loss: 2.15731\n",
      "Best model at iteration: 6846 | smooth loss: 2.15729\n",
      "Best model at iteration: 6847 | smooth loss: 2.15726\n",
      "Best model at iteration: 6848 | smooth loss: 2.15707\n",
      "Best model at iteration: 6849 | smooth loss: 2.15695\n",
      "Best model at iteration: 6850 | smooth loss: 2.15679\n",
      "Best model at iteration: 6851 | smooth loss: 2.15654\n",
      "Best model at iteration: 6852 | smooth loss: 2.15641\n",
      "Best model at iteration: 6853 | smooth loss: 2.15625\n",
      "Best model at iteration: 6854 | smooth loss: 2.15613\n",
      "Best model at iteration: 6907 | smooth loss: 2.15598\n",
      "Best model at iteration: 6909 | smooth loss: 2.15593\n",
      "Best model at iteration: 6910 | smooth loss: 2.15582\n",
      "Best model at iteration: 6911 | smooth loss: 2.15536\n",
      "Best model at iteration: 6914 | smooth loss: 2.15533\n",
      "Best model at iteration: 6915 | smooth loss: 2.15531\n",
      "Best model at iteration: 6916 | smooth loss: 2.15490\n",
      "Best model at iteration: 6918 | smooth loss: 2.15441\n",
      "Best model at iteration: 6919 | smooth loss: 2.15428\n",
      "Best model at iteration: 6921 | smooth loss: 2.15410\n",
      "Best model at iteration: 6922 | smooth loss: 2.15398\n",
      "Best model at iteration: 6924 | smooth loss: 2.15384\n",
      "Best model at iteration: 6925 | smooth loss: 2.15381\n",
      "Best model at iteration: 6926 | smooth loss: 2.15376\n",
      "Best model at iteration: 6927 | smooth loss: 2.15368\n",
      "Best model at iteration: 6928 | smooth loss: 2.15303\n",
      "Best model at iteration: 6929 | smooth loss: 2.15277\n",
      "Best model at iteration: 6963 | smooth loss: 2.15237\n",
      "Best model at iteration: 6967 | smooth loss: 2.15206\n",
      "Best model at iteration: 6968 | smooth loss: 2.15203\n",
      "Best model at iteration: 6969 | smooth loss: 2.15202\n",
      "Best model at iteration: 6970 | smooth loss: 2.15163\n",
      "Best model at iteration: 6972 | smooth loss: 2.15150\n",
      "Best model at iteration: 6973 | smooth loss: 2.15096\n",
      "Best model at iteration: 6974 | smooth loss: 2.15063\n",
      "Best model at iteration: 6975 | smooth loss: 2.15033\n",
      "Best model at iteration: 6976 | smooth loss: 2.15017\n",
      "Best model at iteration: 6977 | smooth loss: 2.14968\n",
      "Best model at iteration: 6978 | smooth loss: 2.14961\n",
      "Best model at iteration: 6979 | smooth loss: 2.14953\n",
      "Best model at iteration: 6982 | smooth loss: 2.14936\n",
      "Best model at iteration: 6983 | smooth loss: 2.14913\n",
      "Best model at iteration: 6984 | smooth loss: 2.14853\n",
      "Best model at iteration: 6986 | smooth loss: 2.14797\n",
      "Best model at iteration: 6990 | smooth loss: 2.14796\n",
      "Best model at iteration: 6993 | smooth loss: 2.14778\n",
      "Best model at iteration: 6998 | smooth loss: 2.14778\n",
      "iter = 7000, smooth loss=2.1482479016922147\n",
      "lery.\n",
      "\"Fiinals breen one Harduss couth rard byok .\n",
      "BThe TOx HeGdiad sad suiglirr.\n",
      "\".  Lhomm sthand and Moninstseald yout dich dor ssed ult hendeds, whno tide the BTxom ghio gladinid wasculadsed bint l\n",
      "\n",
      "Best model at iteration: 7002 | smooth loss: 2.14741\n",
      "Best model at iteration: 7003 | smooth loss: 2.14709\n",
      "Best model at iteration: 7004 | smooth loss: 2.14686\n",
      "Best model at iteration: 7006 | smooth loss: 2.14655\n",
      "Best model at iteration: 7007 | smooth loss: 2.14650\n",
      "Best model at iteration: 7013 | smooth loss: 2.14649\n",
      "Best model at iteration: 7015 | smooth loss: 2.14622\n",
      "Best model at iteration: 7030 | smooth loss: 2.14584\n",
      "Best model at iteration: 7067 | smooth loss: 2.14549\n",
      "Best model at iteration: 7072 | smooth loss: 2.14548\n",
      "Best model at iteration: 7079 | smooth loss: 2.14489\n",
      "Best model at iteration: 7080 | smooth loss: 2.14485\n",
      "Best model at iteration: 7081 | smooth loss: 2.14448\n",
      "Best model at iteration: 7082 | smooth loss: 2.14441\n",
      "Best model at iteration: 7083 | smooth loss: 2.14418\n",
      "Best model at iteration: 7084 | smooth loss: 2.14349\n",
      "Best model at iteration: 7085 | smooth loss: 2.14258\n",
      "Best model at iteration: 7087 | smooth loss: 2.14222\n",
      "Best model at iteration: 7088 | smooth loss: 2.14215\n",
      "Best model at iteration: 7089 | smooth loss: 2.14151\n",
      "Best model at iteration: 7090 | smooth loss: 2.14147\n",
      "Best model at iteration: 7091 | smooth loss: 2.14129\n",
      "Best model at iteration: 7096 | smooth loss: 2.14094\n",
      "Best model at iteration: 7097 | smooth loss: 2.14082\n",
      "Best model at iteration: 7098 | smooth loss: 2.14027\n",
      "Best model at iteration: 7101 | smooth loss: 2.13996\n",
      "Best model at iteration: 7102 | smooth loss: 2.13983\n",
      "Best model at iteration: 7107 | smooth loss: 2.13966\n",
      "Best model at iteration: 7117 | smooth loss: 2.13944\n",
      "Best model at iteration: 7118 | smooth loss: 2.13922\n",
      "Best model at iteration: 7135 | smooth loss: 2.13901\n",
      "Best model at iteration: 7136 | smooth loss: 2.13855\n",
      "Best model at iteration: 7140 | smooth loss: 2.13842\n",
      "Best model at iteration: 7141 | smooth loss: 2.13789\n",
      "Best model at iteration: 7142 | smooth loss: 2.13730\n",
      "Best model at iteration: 7143 | smooth loss: 2.13679\n",
      "Best model at iteration: 7145 | smooth loss: 2.13655\n",
      "Best model at iteration: 7146 | smooth loss: 2.13627\n",
      "Best model at iteration: 7147 | smooth loss: 2.13571\n",
      "Best model at iteration: 7148 | smooth loss: 2.13542\n",
      "Best model at iteration: 7150 | smooth loss: 2.13525\n",
      "Best model at iteration: 7153 | smooth loss: 2.13491\n",
      "Best model at iteration: 7154 | smooth loss: 2.13457\n",
      "Best model at iteration: 7155 | smooth loss: 2.13431\n",
      "Best model at iteration: 7156 | smooth loss: 2.13409\n",
      "Best model at iteration: 7157 | smooth loss: 2.13380\n",
      "Best model at iteration: 7158 | smooth loss: 2.13367\n",
      "Best model at iteration: 7159 | smooth loss: 2.13343\n",
      "Best model at iteration: 7160 | smooth loss: 2.13290\n",
      "Best model at iteration: 7161 | smooth loss: 2.13261\n",
      "Best model at iteration: 7162 | smooth loss: 2.13229\n",
      "Best model at iteration: 7164 | smooth loss: 2.13214\n",
      "Best model at iteration: 7166 | smooth loss: 2.13208\n",
      "Best model at iteration: 7172 | smooth loss: 2.13188\n",
      "Best model at iteration: 7173 | smooth loss: 2.13160\n",
      "Best model at iteration: 7174 | smooth loss: 2.13158\n",
      "Best model at iteration: 7175 | smooth loss: 2.13120\n",
      "Best model at iteration: 7176 | smooth loss: 2.13066\n",
      "Best model at iteration: 7177 | smooth loss: 2.12984\n",
      "Best model at iteration: 7178 | smooth loss: 2.12982\n",
      "Best model at iteration: 7179 | smooth loss: 2.12936\n",
      "Best model at iteration: 7180 | smooth loss: 2.12888\n",
      "Best model at iteration: 7181 | smooth loss: 2.12852\n",
      "Best model at iteration: 7185 | smooth loss: 2.12831\n",
      "Best model at iteration: 7187 | smooth loss: 2.12790\n",
      "Best model at iteration: 7188 | smooth loss: 2.12728\n",
      "Best model at iteration: 7189 | smooth loss: 2.12704\n",
      "Best model at iteration: 7192 | smooth loss: 2.12673\n",
      "Best model at iteration: 7194 | smooth loss: 2.12628\n",
      "Best model at iteration: 7196 | smooth loss: 2.12583\n",
      "Best model at iteration: 7197 | smooth loss: 2.12556\n",
      "Best model at iteration: 7198 | smooth loss: 2.12538\n",
      "Best model at iteration: 7200 | smooth loss: 2.12504\n",
      "Best model at iteration: 7202 | smooth loss: 2.12475\n",
      "Best model at iteration: 7203 | smooth loss: 2.12455\n",
      "Best model at iteration: 7204 | smooth loss: 2.12415\n",
      "Best model at iteration: 7205 | smooth loss: 2.12405\n",
      "Best model at iteration: 7206 | smooth loss: 2.12376\n",
      "Best model at iteration: 7207 | smooth loss: 2.12366\n",
      "Best model at iteration: 7209 | smooth loss: 2.12344\n",
      "Best model at iteration: 7210 | smooth loss: 2.12342\n",
      "Best model at iteration: 7212 | smooth loss: 2.12310\n",
      "Best model at iteration: 7213 | smooth loss: 2.12267\n",
      "Best model at iteration: 7214 | smooth loss: 2.12216\n",
      "Best model at iteration: 7215 | smooth loss: 2.12211\n",
      "Best model at iteration: 7216 | smooth loss: 2.12197\n",
      "Best model at iteration: 7226 | smooth loss: 2.12181\n",
      "Best model at iteration: 7231 | smooth loss: 2.12178\n",
      "Best model at iteration: 7232 | smooth loss: 2.12134\n",
      "Best model at iteration: 7233 | smooth loss: 2.12122\n",
      "Best model at iteration: 7234 | smooth loss: 2.12086\n",
      "Best model at iteration: 7236 | smooth loss: 2.11998\n",
      "Best model at iteration: 7237 | smooth loss: 2.11958\n",
      "Best model at iteration: 7239 | smooth loss: 2.11950\n",
      "Best model at iteration: 7242 | smooth loss: 2.11920\n",
      "Best model at iteration: 7243 | smooth loss: 2.11894\n",
      "Best model at iteration: 7244 | smooth loss: 2.11824\n",
      "Best model at iteration: 7245 | smooth loss: 2.11798\n",
      "Best model at iteration: 7246 | smooth loss: 2.11788\n",
      "Best model at iteration: 7247 | smooth loss: 2.11781\n",
      "Best model at iteration: 7248 | smooth loss: 2.11723\n",
      "Best model at iteration: 7249 | smooth loss: 2.11717\n",
      "Best model at iteration: 7253 | smooth loss: 2.11697\n",
      "Best model at iteration: 7254 | smooth loss: 2.11680\n",
      "Best model at iteration: 7255 | smooth loss: 2.11625\n",
      "Best model at iteration: 7256 | smooth loss: 2.11587\n",
      "Best model at iteration: 7257 | smooth loss: 2.11556\n",
      "Best model at iteration: 7258 | smooth loss: 2.11505\n",
      "Best model at iteration: 7259 | smooth loss: 2.11486\n",
      "Best model at iteration: 7264 | smooth loss: 2.11461\n",
      "Best model at iteration: 7265 | smooth loss: 2.11403\n",
      "Best model at iteration: 7266 | smooth loss: 2.11392\n",
      "Best model at iteration: 7267 | smooth loss: 2.11340\n",
      "Best model at iteration: 7268 | smooth loss: 2.11275\n",
      "Best model at iteration: 7269 | smooth loss: 2.11262\n",
      "Best model at iteration: 7271 | smooth loss: 2.11208\n",
      "Best model at iteration: 7272 | smooth loss: 2.11156\n",
      "Best model at iteration: 7273 | smooth loss: 2.11129\n",
      "Best model at iteration: 7274 | smooth loss: 2.11063\n",
      "Best model at iteration: 7275 | smooth loss: 2.11008\n",
      "Best model at iteration: 7277 | smooth loss: 2.10974\n",
      "Best model at iteration: 7279 | smooth loss: 2.10943\n",
      "Best model at iteration: 7280 | smooth loss: 2.10937\n",
      "Best model at iteration: 7281 | smooth loss: 2.10928\n",
      "Best model at iteration: 7282 | smooth loss: 2.10888\n",
      "Best model at iteration: 7285 | smooth loss: 2.10881\n",
      "Best model at iteration: 7289 | smooth loss: 2.10853\n",
      "Best model at iteration: 7290 | smooth loss: 2.10838\n",
      "Best model at iteration: 7291 | smooth loss: 2.10807\n",
      "Best model at iteration: 7292 | smooth loss: 2.10802\n",
      "Best model at iteration: 7293 | smooth loss: 2.10778\n",
      "Best model at iteration: 7294 | smooth loss: 2.10763\n",
      "Best model at iteration: 7295 | smooth loss: 2.10752\n",
      "Best model at iteration: 7296 | smooth loss: 2.10744\n",
      "Best model at iteration: 7298 | smooth loss: 2.10710\n",
      "Best model at iteration: 7308 | smooth loss: 2.10697\n",
      "Best model at iteration: 7310 | smooth loss: 2.10686\n",
      "Best model at iteration: 7311 | smooth loss: 2.10642\n",
      "Best model at iteration: 7312 | smooth loss: 2.10592\n",
      "Best model at iteration: 7318 | smooth loss: 2.10584\n",
      "Best model at iteration: 7319 | smooth loss: 2.10560\n",
      "Best model at iteration: 7320 | smooth loss: 2.10537\n",
      "Best model at iteration: 7322 | smooth loss: 2.10517\n",
      "Best model at iteration: 7323 | smooth loss: 2.10497\n",
      "Best model at iteration: 7324 | smooth loss: 2.10459\n",
      "Best model at iteration: 7325 | smooth loss: 2.10454\n",
      "Best model at iteration: 7326 | smooth loss: 2.10452\n",
      "Best model at iteration: 7331 | smooth loss: 2.10451\n",
      "Best model at iteration: 7332 | smooth loss: 2.10407\n",
      "Best model at iteration: 7334 | smooth loss: 2.10368\n",
      "Best model at iteration: 7336 | smooth loss: 2.10368\n",
      "Best model at iteration: 7337 | smooth loss: 2.10306\n",
      "Best model at iteration: 7338 | smooth loss: 2.10263\n",
      "Best model at iteration: 7381 | smooth loss: 2.10243\n",
      "Best model at iteration: 7382 | smooth loss: 2.10214\n",
      "Best model at iteration: 7384 | smooth loss: 2.10210\n",
      "Best model at iteration: 7385 | smooth loss: 2.10147\n",
      "Best model at iteration: 7386 | smooth loss: 2.10135\n",
      "Best model at iteration: 7387 | smooth loss: 2.10130\n",
      "Best model at iteration: 7392 | smooth loss: 2.10095\n",
      "Best model at iteration: 7393 | smooth loss: 2.10088\n",
      "Best model at iteration: 7394 | smooth loss: 2.10068\n",
      "Best model at iteration: 7395 | smooth loss: 2.10057\n",
      "Best model at iteration: 7396 | smooth loss: 2.10035\n",
      "Best model at iteration: 7397 | smooth loss: 2.10024\n",
      "Best model at iteration: 7398 | smooth loss: 2.10016\n",
      "Best model at iteration: 7399 | smooth loss: 2.09962\n",
      "Best model at iteration: 7401 | smooth loss: 2.09956\n",
      "Best model at iteration: 7402 | smooth loss: 2.09953\n",
      "Best model at iteration: 7403 | smooth loss: 2.09902\n",
      "Best model at iteration: 7404 | smooth loss: 2.09865\n",
      "Best model at iteration: 7405 | smooth loss: 2.09837\n",
      "Best model at iteration: 7407 | smooth loss: 2.09824\n",
      "Best model at iteration: 7408 | smooth loss: 2.09807\n",
      "Best model at iteration: 7409 | smooth loss: 2.09776\n",
      "Best model at iteration: 7450 | smooth loss: 2.09766\n",
      "Best model at iteration: 7452 | smooth loss: 2.09707\n",
      "Best model at iteration: 7453 | smooth loss: 2.09675\n",
      "Best model at iteration: 7454 | smooth loss: 2.09617\n",
      "Best model at iteration: 7456 | smooth loss: 2.09567\n",
      "Best model at iteration: 7459 | smooth loss: 2.09558\n",
      "Best model at iteration: 7460 | smooth loss: 2.09552\n",
      "Best model at iteration: 7463 | smooth loss: 2.09532\n",
      "Best model at iteration: 7464 | smooth loss: 2.09495\n",
      "Best model at iteration: 7465 | smooth loss: 2.09478\n",
      "Best model at iteration: 7466 | smooth loss: 2.09476\n",
      "Best model at iteration: 7467 | smooth loss: 2.09471\n",
      "Best model at iteration: 7487 | smooth loss: 2.09446\n",
      "Best model at iteration: 7488 | smooth loss: 2.09387\n",
      "Best model at iteration: 7489 | smooth loss: 2.09336\n",
      "Best model at iteration: 7490 | smooth loss: 2.09300\n",
      "Best model at iteration: 7491 | smooth loss: 2.09277\n",
      "Best model at iteration: 7492 | smooth loss: 2.09210\n",
      "Best model at iteration: 7493 | smooth loss: 2.09197\n",
      "Best model at iteration: 7497 | smooth loss: 2.09145\n",
      "Best model at iteration: 7500 | smooth loss: 2.09120\n",
      "Best model at iteration: 7502 | smooth loss: 2.09105\n",
      "Best model at iteration: 7509 | smooth loss: 2.09085\n",
      "Best model at iteration: 7510 | smooth loss: 2.09034\n",
      "Best model at iteration: 7511 | smooth loss: 2.09014\n",
      "Best model at iteration: 7512 | smooth loss: 2.09004\n",
      "Best model at iteration: 7519 | smooth loss: 2.08977\n",
      "Best model at iteration: 7524 | smooth loss: 2.08958\n",
      "Best model at iteration: 7525 | smooth loss: 2.08930\n",
      "Best model at iteration: 7538 | smooth loss: 2.08897\n",
      "Best model at iteration: 7539 | smooth loss: 2.08894\n",
      "Best model at iteration: 7540 | smooth loss: 2.08854\n",
      "Best model at iteration: 7541 | smooth loss: 2.08845\n",
      "Best model at iteration: 7542 | smooth loss: 2.08811\n",
      "Best model at iteration: 7543 | smooth loss: 2.08793\n",
      "Best model at iteration: 7544 | smooth loss: 2.08771\n",
      "Best model at iteration: 7547 | smooth loss: 2.08761\n",
      "Best model at iteration: 7548 | smooth loss: 2.08761\n",
      "Best model at iteration: 7549 | smooth loss: 2.08732\n",
      "Best model at iteration: 7550 | smooth loss: 2.08727\n",
      "Best model at iteration: 7551 | smooth loss: 2.08687\n",
      "Best model at iteration: 7552 | smooth loss: 2.08679\n",
      "Best model at iteration: 7554 | smooth loss: 2.08648\n",
      "Best model at iteration: 7555 | smooth loss: 2.08606\n",
      "Best model at iteration: 7556 | smooth loss: 2.08570\n",
      "Best model at iteration: 7557 | smooth loss: 2.08547\n",
      "Best model at iteration: 7558 | smooth loss: 2.08469\n",
      "Best model at iteration: 7559 | smooth loss: 2.08454\n",
      "Best model at iteration: 7563 | smooth loss: 2.08446\n",
      "Best model at iteration: 7564 | smooth loss: 2.08405\n",
      "Best model at iteration: 7630 | smooth loss: 2.08384\n",
      "Best model at iteration: 7631 | smooth loss: 2.08315\n",
      "Best model at iteration: 7634 | smooth loss: 2.08285\n",
      "Best model at iteration: 7635 | smooth loss: 2.08234\n",
      "Best model at iteration: 7636 | smooth loss: 2.08210\n",
      "Best model at iteration: 7637 | smooth loss: 2.08187\n",
      "Best model at iteration: 7638 | smooth loss: 2.08182\n",
      "Best model at iteration: 7640 | smooth loss: 2.08149\n",
      "Best model at iteration: 7646 | smooth loss: 2.08117\n",
      "Best model at iteration: 7647 | smooth loss: 2.08104\n",
      "Best model at iteration: 7648 | smooth loss: 2.08034\n",
      "Best model at iteration: 7649 | smooth loss: 2.08000\n",
      "Best model at iteration: 7650 | smooth loss: 2.07990\n",
      "Best model at iteration: 7651 | smooth loss: 2.07958\n",
      "Best model at iteration: 7652 | smooth loss: 2.07951\n",
      "Best model at iteration: 7659 | smooth loss: 2.07893\n",
      "Best model at iteration: 7661 | smooth loss: 2.07865\n",
      "Best model at iteration: 7668 | smooth loss: 2.07857\n",
      "Best model at iteration: 7672 | smooth loss: 2.07846\n",
      "Best model at iteration: 7673 | smooth loss: 2.07820\n",
      "Best model at iteration: 7674 | smooth loss: 2.07787\n",
      "Best model at iteration: 7677 | smooth loss: 2.07783\n",
      "Best model at iteration: 7678 | smooth loss: 2.07756\n",
      "Best model at iteration: 7679 | smooth loss: 2.07703\n",
      "Best model at iteration: 7680 | smooth loss: 2.07699\n",
      "Best model at iteration: 7682 | smooth loss: 2.07694\n",
      "Best model at iteration: 7685 | smooth loss: 2.07675\n",
      "Best model at iteration: 7686 | smooth loss: 2.07619\n",
      "Best model at iteration: 7687 | smooth loss: 2.07574\n",
      "Best model at iteration: 7688 | smooth loss: 2.07553\n",
      "Best model at iteration: 7689 | smooth loss: 2.07483\n",
      "Best model at iteration: 7690 | smooth loss: 2.07465\n",
      "Best model at iteration: 7693 | smooth loss: 2.07438\n",
      "Best model at iteration: 7694 | smooth loss: 2.07424\n",
      "Best model at iteration: 7695 | smooth loss: 2.07388\n",
      "Best model at iteration: 7698 | smooth loss: 2.07387\n",
      "Best model at iteration: 7699 | smooth loss: 2.07373\n",
      "Best model at iteration: 7701 | smooth loss: 2.07348\n",
      "Best model at iteration: 7702 | smooth loss: 2.07300\n",
      "Best model at iteration: 7703 | smooth loss: 2.07232\n",
      "Best model at iteration: 7704 | smooth loss: 2.07211\n",
      "Best model at iteration: 7705 | smooth loss: 2.07194\n",
      "Best model at iteration: 7706 | smooth loss: 2.07155\n",
      "Best model at iteration: 7707 | smooth loss: 2.07120\n",
      "Best model at iteration: 7708 | smooth loss: 2.07092\n",
      "Best model at iteration: 7714 | smooth loss: 2.07063\n",
      "Best model at iteration: 7715 | smooth loss: 2.07033\n",
      "Best model at iteration: 7716 | smooth loss: 2.07012\n",
      "Best model at iteration: 7718 | smooth loss: 2.06993\n",
      "Best model at iteration: 7719 | smooth loss: 2.06986\n",
      "Best model at iteration: 7720 | smooth loss: 2.06934\n",
      "Best model at iteration: 7721 | smooth loss: 2.06909\n",
      "Best model at iteration: 7725 | smooth loss: 2.06889\n",
      "Best model at iteration: 7726 | smooth loss: 2.06852\n",
      "Best model at iteration: 7770 | smooth loss: 2.06832\n",
      "Best model at iteration: 7772 | smooth loss: 2.06761\n",
      "Best model at iteration: 7773 | smooth loss: 2.06740\n",
      "Best model at iteration: 7776 | smooth loss: 2.06711\n",
      "Best model at iteration: 7777 | smooth loss: 2.06683\n",
      "Best model at iteration: 7796 | smooth loss: 2.06673\n",
      "Best model at iteration: 7797 | smooth loss: 2.06643\n",
      "Best model at iteration: 7798 | smooth loss: 2.06643\n",
      "Best model at iteration: 7813 | smooth loss: 2.06636\n",
      "Best model at iteration: 7814 | smooth loss: 2.06622\n",
      "Best model at iteration: 7825 | smooth loss: 2.06563\n",
      "Best model at iteration: 7826 | smooth loss: 2.06538\n",
      "Best model at iteration: 7827 | smooth loss: 2.06527\n",
      "Best model at iteration: 7830 | smooth loss: 2.06521\n",
      "Best model at iteration: 7833 | smooth loss: 2.06517\n",
      "Best model at iteration: 7834 | smooth loss: 2.06474\n",
      "Best model at iteration: 7835 | smooth loss: 2.06412\n",
      "Best model at iteration: 7836 | smooth loss: 2.06385\n",
      "Best model at iteration: 7837 | smooth loss: 2.06366\n",
      "Best model at iteration: 7838 | smooth loss: 2.06360\n",
      "Best model at iteration: 7839 | smooth loss: 2.06358\n",
      "Best model at iteration: 7844 | smooth loss: 2.06351\n",
      "Best model at iteration: 7846 | smooth loss: 2.06346\n",
      "Best model at iteration: 7847 | smooth loss: 2.06340\n",
      "Best model at iteration: 7849 | smooth loss: 2.06301\n",
      "Best model at iteration: 7853 | smooth loss: 2.06287\n",
      "Best model at iteration: 7854 | smooth loss: 2.06269\n",
      "Best model at iteration: 7855 | smooth loss: 2.06225\n",
      "Best model at iteration: 7856 | smooth loss: 2.06193\n",
      "Best model at iteration: 7857 | smooth loss: 2.06180\n",
      "Best model at iteration: 7858 | smooth loss: 2.06170\n",
      "Best model at iteration: 7865 | smooth loss: 2.06085\n",
      "Best model at iteration: 7866 | smooth loss: 2.06014\n",
      "Best model at iteration: 7867 | smooth loss: 2.05991\n",
      "Best model at iteration: 7868 | smooth loss: 2.05936\n",
      "Best model at iteration: 7869 | smooth loss: 2.05895\n",
      "Best model at iteration: 7870 | smooth loss: 2.05865\n",
      "Best model at iteration: 7871 | smooth loss: 2.05842\n",
      "Best model at iteration: 7873 | smooth loss: 2.05835\n",
      "Best model at iteration: 7874 | smooth loss: 2.05755\n",
      "Best model at iteration: 7875 | smooth loss: 2.05743\n",
      "Best model at iteration: 7876 | smooth loss: 2.05741\n",
      "Best model at iteration: 7877 | smooth loss: 2.05721\n",
      "Best model at iteration: 7880 | smooth loss: 2.05721\n",
      "Best model at iteration: 7884 | smooth loss: 2.05683\n",
      "Best model at iteration: 7885 | smooth loss: 2.05674\n",
      "Best model at iteration: 7886 | smooth loss: 2.05596\n",
      "Best model at iteration: 7888 | smooth loss: 2.05574\n",
      "Best model at iteration: 7889 | smooth loss: 2.05535\n",
      "Best model at iteration: 7890 | smooth loss: 2.05514\n",
      "Best model at iteration: 7891 | smooth loss: 2.05447\n",
      "Best model at iteration: 7893 | smooth loss: 2.05413\n",
      "Best model at iteration: 7894 | smooth loss: 2.05409\n",
      "Best model at iteration: 7895 | smooth loss: 2.05325\n",
      "Best model at iteration: 7897 | smooth loss: 2.05287\n",
      "Best model at iteration: 7912 | smooth loss: 2.05279\n",
      "Best model at iteration: 7913 | smooth loss: 2.05249\n",
      "Best model at iteration: 7914 | smooth loss: 2.05234\n",
      "Best model at iteration: 7937 | smooth loss: 2.05229\n",
      "Best model at iteration: 7938 | smooth loss: 2.05168\n",
      "Best model at iteration: 7939 | smooth loss: 2.05115\n",
      "Best model at iteration: 7949 | smooth loss: 2.05107\n",
      "Best model at iteration: 7950 | smooth loss: 2.05078\n",
      "Best model at iteration: 7951 | smooth loss: 2.05074\n",
      "Best model at iteration: 7952 | smooth loss: 2.05028\n",
      "Best model at iteration: 7953 | smooth loss: 2.05012\n",
      "Best model at iteration: 7955 | smooth loss: 2.04998\n",
      "Best model at iteration: 7956 | smooth loss: 2.04975\n",
      "Best model at iteration: 7957 | smooth loss: 2.04940\n",
      "Best model at iteration: 7958 | smooth loss: 2.04898\n",
      "Best model at iteration: 7959 | smooth loss: 2.04883\n",
      "Best model at iteration: 7974 | smooth loss: 2.04839\n",
      "Best model at iteration: 7975 | smooth loss: 2.04756\n",
      "Best model at iteration: 7976 | smooth loss: 2.04734\n",
      "Best model at iteration: 7977 | smooth loss: 2.04694\n",
      "Best model at iteration: 7978 | smooth loss: 2.04640\n",
      "iter = 8000, smooth loss=2.049996607690156\n",
      "gsse to blinkey alm.. Dingor Mr. We slo hever, ttse un Mr. Ay to \"Ind hig tond ig the nammbecounding. Darredile, ay rrecaned,\" said My. C\"I's over atcheread dinkingaincursinvise,\" has and his latint a\n",
      "\n",
      "Best model at iteration: 8090 | smooth loss: 2.04627\n",
      "Best model at iteration: 8091 | smooth loss: 2.04594\n",
      "Best model at iteration: 8092 | smooth loss: 2.04523\n",
      "Best model at iteration: 8096 | smooth loss: 2.04518\n",
      "Best model at iteration: 8097 | smooth loss: 2.04501\n",
      "Best model at iteration: 8098 | smooth loss: 2.04483\n",
      "Best model at iteration: 8099 | smooth loss: 2.04421\n",
      "Best model at iteration: 8100 | smooth loss: 2.04374\n",
      "Best model at iteration: 8102 | smooth loss: 2.04366\n",
      "Best model at iteration: 8107 | smooth loss: 2.04365\n",
      "Best model at iteration: 8108 | smooth loss: 2.04341\n",
      "Best model at iteration: 8110 | smooth loss: 2.04333\n",
      "Best model at iteration: 8112 | smooth loss: 2.04316\n",
      "Best model at iteration: 8115 | smooth loss: 2.04273\n",
      "Best model at iteration: 8117 | smooth loss: 2.04262\n",
      "Best model at iteration: 8119 | smooth loss: 2.04210\n",
      "Best model at iteration: 8120 | smooth loss: 2.04140\n",
      "Best model at iteration: 8121 | smooth loss: 2.04133\n",
      "Best model at iteration: 8122 | smooth loss: 2.04117\n",
      "Best model at iteration: 8123 | smooth loss: 2.04055\n",
      "Best model at iteration: 8125 | smooth loss: 2.04015\n",
      "Best model at iteration: 8126 | smooth loss: 2.04013\n",
      "Best model at iteration: 8127 | smooth loss: 2.03994\n",
      "Best model at iteration: 8131 | smooth loss: 2.03983\n",
      "Best model at iteration: 8135 | smooth loss: 2.03960\n",
      "Best model at iteration: 8137 | smooth loss: 2.03943\n",
      "Best model at iteration: 8138 | smooth loss: 2.03891\n",
      "Best model at iteration: 8142 | smooth loss: 2.03860\n",
      "Best model at iteration: 8144 | smooth loss: 2.03858\n",
      "Best model at iteration: 8145 | smooth loss: 2.03832\n",
      "Best model at iteration: 8147 | smooth loss: 2.03774\n",
      "Best model at iteration: 8148 | smooth loss: 2.03751\n",
      "Best model at iteration: 8149 | smooth loss: 2.03723\n",
      "Best model at iteration: 8151 | smooth loss: 2.03665\n",
      "Best model at iteration: 8152 | smooth loss: 2.03654\n",
      "Best model at iteration: 8153 | smooth loss: 2.03641\n",
      "Best model at iteration: 8157 | smooth loss: 2.03608\n",
      "Best model at iteration: 8165 | smooth loss: 2.03536\n",
      "Best model at iteration: 8166 | smooth loss: 2.03530\n",
      "Best model at iteration: 8169 | smooth loss: 2.03523\n",
      "Best model at iteration: 8170 | smooth loss: 2.03500\n",
      "Best model at iteration: 8183 | smooth loss: 2.03474\n",
      "Best model at iteration: 8184 | smooth loss: 2.03412\n",
      "Best model at iteration: 8200 | smooth loss: 2.03406\n",
      "Best model at iteration: 8203 | smooth loss: 2.03380\n",
      "Best model at iteration: 8219 | smooth loss: 2.03358\n",
      "Best model at iteration: 8220 | smooth loss: 2.03350\n",
      "Best model at iteration: 8225 | smooth loss: 2.03341\n",
      "Best model at iteration: 8237 | smooth loss: 2.03318\n",
      "Best model at iteration: 8244 | smooth loss: 2.03310\n",
      "Best model at iteration: 8247 | smooth loss: 2.03283\n",
      "Best model at iteration: 8248 | smooth loss: 2.03239\n",
      "Best model at iteration: 8250 | smooth loss: 2.03222\n",
      "Best model at iteration: 8251 | smooth loss: 2.03208\n",
      "Best model at iteration: 8252 | smooth loss: 2.03176\n",
      "Best model at iteration: 8253 | smooth loss: 2.03134\n",
      "Best model at iteration: 8254 | smooth loss: 2.03133\n",
      "Best model at iteration: 8255 | smooth loss: 2.03086\n",
      "Best model at iteration: 8256 | smooth loss: 2.03051\n",
      "Best model at iteration: 8261 | smooth loss: 2.03012\n",
      "Best model at iteration: 8262 | smooth loss: 2.02992\n",
      "Best model at iteration: 8263 | smooth loss: 2.02968\n",
      "Best model at iteration: 8264 | smooth loss: 2.02909\n",
      "Best model at iteration: 8265 | smooth loss: 2.02885\n",
      "Best model at iteration: 8269 | smooth loss: 2.02880\n",
      "Best model at iteration: 8270 | smooth loss: 2.02859\n",
      "Best model at iteration: 8271 | smooth loss: 2.02845\n",
      "Best model at iteration: 8272 | smooth loss: 2.02816\n",
      "Best model at iteration: 8273 | smooth loss: 2.02785\n",
      "Best model at iteration: 8274 | smooth loss: 2.02765\n",
      "Best model at iteration: 8275 | smooth loss: 2.02763\n",
      "Best model at iteration: 8276 | smooth loss: 2.02762\n",
      "Best model at iteration: 8279 | smooth loss: 2.02728\n",
      "Best model at iteration: 8286 | smooth loss: 2.02710\n",
      "Best model at iteration: 8287 | smooth loss: 2.02698\n",
      "Best model at iteration: 8289 | smooth loss: 2.02651\n",
      "Best model at iteration: 8298 | smooth loss: 2.02647\n",
      "Best model at iteration: 8301 | smooth loss: 2.02570\n",
      "Best model at iteration: 8302 | smooth loss: 2.02567\n",
      "Best model at iteration: 8303 | smooth loss: 2.02555\n",
      "Best model at iteration: 8307 | smooth loss: 2.02545\n",
      "Best model at iteration: 8310 | smooth loss: 2.02524\n",
      "Best model at iteration: 8342 | smooth loss: 2.02462\n",
      "Best model at iteration: 8343 | smooth loss: 2.02449\n",
      "Best model at iteration: 8344 | smooth loss: 2.02437\n",
      "Best model at iteration: 8347 | smooth loss: 2.02419\n",
      "Best model at iteration: 8348 | smooth loss: 2.02351\n",
      "Best model at iteration: 8351 | smooth loss: 2.02345\n",
      "Best model at iteration: 8352 | smooth loss: 2.02311\n",
      "Best model at iteration: 8353 | smooth loss: 2.02291\n",
      "Best model at iteration: 8354 | smooth loss: 2.02264\n",
      "Best model at iteration: 8366 | smooth loss: 2.02215\n",
      "Best model at iteration: 8367 | smooth loss: 2.02214\n",
      "Best model at iteration: 8368 | smooth loss: 2.02209\n",
      "Best model at iteration: 8369 | smooth loss: 2.02175\n",
      "Best model at iteration: 8377 | smooth loss: 2.02134\n",
      "Best model at iteration: 8378 | smooth loss: 2.02119\n",
      "Best model at iteration: 8379 | smooth loss: 2.02094\n",
      "Best model at iteration: 8380 | smooth loss: 2.02076\n",
      "Best model at iteration: 8381 | smooth loss: 2.02026\n",
      "Best model at iteration: 8382 | smooth loss: 2.02018\n",
      "Best model at iteration: 8384 | smooth loss: 2.01978\n",
      "Best model at iteration: 8385 | smooth loss: 2.01961\n",
      "Best model at iteration: 8387 | smooth loss: 2.01894\n",
      "Best model at iteration: 8388 | smooth loss: 2.01871\n",
      "Best model at iteration: 8389 | smooth loss: 2.01810\n",
      "Best model at iteration: 8390 | smooth loss: 2.01761\n",
      "Best model at iteration: 8391 | smooth loss: 2.01700\n",
      "Best model at iteration: 8392 | smooth loss: 2.01671\n",
      "Best model at iteration: 8393 | smooth loss: 2.01603\n",
      "Best model at iteration: 8394 | smooth loss: 2.01528\n",
      "Best model at iteration: 8395 | smooth loss: 2.01451\n",
      "Best model at iteration: 8396 | smooth loss: 2.01386\n",
      "Best model at iteration: 8398 | smooth loss: 2.01353\n",
      "Best model at iteration: 8399 | smooth loss: 2.01342\n",
      "Best model at iteration: 8400 | smooth loss: 2.01331\n",
      "Best model at iteration: 8401 | smooth loss: 2.01326\n",
      "Best model at iteration: 8402 | smooth loss: 2.01316\n",
      "Best model at iteration: 8404 | smooth loss: 2.01316\n",
      "Best model at iteration: 8406 | smooth loss: 2.01286\n",
      "Best model at iteration: 8407 | smooth loss: 2.01233\n",
      "Best model at iteration: 8408 | smooth loss: 2.01221\n",
      "Best model at iteration: 8410 | smooth loss: 2.01201\n",
      "Best model at iteration: 8411 | smooth loss: 2.01159\n",
      "Best model at iteration: 8412 | smooth loss: 2.01065\n",
      "Best model at iteration: 8413 | smooth loss: 2.01036\n",
      "Best model at iteration: 8414 | smooth loss: 2.01030\n",
      "Best model at iteration: 8416 | smooth loss: 2.00984\n",
      "Best model at iteration: 8418 | smooth loss: 2.00973\n",
      "Best model at iteration: 8419 | smooth loss: 2.00966\n",
      "Best model at iteration: 8420 | smooth loss: 2.00930\n",
      "Best model at iteration: 8424 | smooth loss: 2.00916\n",
      "Best model at iteration: 8425 | smooth loss: 2.00854\n",
      "Best model at iteration: 8426 | smooth loss: 2.00814\n",
      "Best model at iteration: 8432 | smooth loss: 2.00763\n",
      "Best model at iteration: 8433 | smooth loss: 2.00702\n",
      "Best model at iteration: 8435 | smooth loss: 2.00629\n",
      "Best model at iteration: 8436 | smooth loss: 2.00611\n",
      "Best model at iteration: 8446 | smooth loss: 2.00588\n",
      "Best model at iteration: 8447 | smooth loss: 2.00561\n",
      "Best model at iteration: 8448 | smooth loss: 2.00550\n",
      "Best model at iteration: 8449 | smooth loss: 2.00511\n",
      "Best model at iteration: 8455 | smooth loss: 2.00498\n",
      "Best model at iteration: 8458 | smooth loss: 2.00479\n",
      "Best model at iteration: 8459 | smooth loss: 2.00373\n",
      "Best model at iteration: 8460 | smooth loss: 2.00353\n",
      "Best model at iteration: 8461 | smooth loss: 2.00283\n",
      "Best model at iteration: 8462 | smooth loss: 2.00227\n",
      "Best model at iteration: 8464 | smooth loss: 2.00217\n",
      "Best model at iteration: 8466 | smooth loss: 2.00213\n",
      "Best model at iteration: 8476 | smooth loss: 2.00120\n",
      "Best model at iteration: 8477 | smooth loss: 2.00101\n",
      "Best model at iteration: 8478 | smooth loss: 2.00045\n",
      "Best model at iteration: 8482 | smooth loss: 2.00045\n",
      "Best model at iteration: 8483 | smooth loss: 2.00003\n",
      "Best model at iteration: 8484 | smooth loss: 1.99911\n",
      "Best model at iteration: 8485 | smooth loss: 1.99902\n",
      "Best model at iteration: 8488 | smooth loss: 1.99832\n",
      "Best model at iteration: 8489 | smooth loss: 1.99796\n",
      "Best model at iteration: 8490 | smooth loss: 1.99761\n",
      "Best model at iteration: 8491 | smooth loss: 1.99750\n",
      "Best model at iteration: 8492 | smooth loss: 1.99739\n",
      "Best model at iteration: 8493 | smooth loss: 1.99721\n",
      "Best model at iteration: 8494 | smooth loss: 1.99708\n",
      "Best model at iteration: 8502 | smooth loss: 1.99695\n",
      "Best model at iteration: 8503 | smooth loss: 1.99695\n",
      "Best model at iteration: 8506 | smooth loss: 1.99679\n",
      "Best model at iteration: 8507 | smooth loss: 1.99640\n",
      "Best model at iteration: 8508 | smooth loss: 1.99632\n",
      "Best model at iteration: 8509 | smooth loss: 1.99590\n",
      "Best model at iteration: 8514 | smooth loss: 1.99579\n",
      "Best model at iteration: 8515 | smooth loss: 1.99498\n",
      "Best model at iteration: 8557 | smooth loss: 1.99493\n",
      "Best model at iteration: 8558 | smooth loss: 1.99447\n",
      "Best model at iteration: 8559 | smooth loss: 1.99434\n",
      "Best model at iteration: 8649 | smooth loss: 1.99433\n",
      "Best model at iteration: 8650 | smooth loss: 1.99331\n",
      "Best model at iteration: 8651 | smooth loss: 1.99300\n",
      "Best model at iteration: 8652 | smooth loss: 1.99285\n",
      "Best model at iteration: 8653 | smooth loss: 1.99276\n",
      "Best model at iteration: 8654 | smooth loss: 1.99274\n",
      "Best model at iteration: 8656 | smooth loss: 1.99177\n",
      "Best model at iteration: 8657 | smooth loss: 1.99169\n",
      "Best model at iteration: 8658 | smooth loss: 1.99125\n",
      "Best model at iteration: 8659 | smooth loss: 1.99121\n",
      "Best model at iteration: 8661 | smooth loss: 1.99107\n",
      "iter = 9000, smooth loss=2.0106959456291813\n",
      " lither Pertian, I'vered rumoret o aingain, butmome, ma to the ofont to hus wered rmoon'werese hisher not heroadn't rowvory wermamighon toins.  Harn's pprrister.  Harry Prored atadre?\"\n",
      "\"Ano rand the a\n",
      "\n",
      "iter = 10000, smooth loss=2.004544408068831\n",
      "sarse see af sook sway, wat ynum surong to herm.  Deds.  I with cact gete pare in Magrtlying Gond lferid bait dor s read ie sswist a klow- th ho lines afp rught han thay sar, wik sairs, en carise if k\n",
      "\n",
      "iter = 11000, smooth loss=2.040672716197565\n",
      "uls le andan Frumfen of on aar.  \"Harr, hlay lit on.  Thast, fluling the kstel Gulithure the hare y head saichils wes sof you duntid wis as pum, mide thoured ness kid?\" faid Hel te tre ont- oo the tou\n",
      "\n",
      "iter = 12000, smooth loss=2.021289931306156\n",
      "I innow lalla pragbars the sloullend mise was ely was wh, ssee ctang fos up aing lamuley, I aumplextlrouned wast that wfom toitk thete nowsto the dempilled thoulled anctstitniy  thr ficl oe sulfer wyo\n",
      "\n",
      "Best model at iteration: 12799 | smooth loss: 1.99101\n",
      "Best model at iteration: 12803 | smooth loss: 1.99061\n",
      "Best model at iteration: 12804 | smooth loss: 1.99003\n",
      "Best model at iteration: 12805 | smooth loss: 1.98930\n",
      "Best model at iteration: 12806 | smooth loss: 1.98918\n",
      "Best model at iteration: 12807 | smooth loss: 1.98887\n",
      "Best model at iteration: 12808 | smooth loss: 1.98875\n",
      "Best model at iteration: 12809 | smooth loss: 1.98860\n",
      "Best model at iteration: 12812 | smooth loss: 1.98830\n",
      "Best model at iteration: 12817 | smooth loss: 1.98827\n",
      "Best model at iteration: 12818 | smooth loss: 1.98778\n",
      "Best model at iteration: 12819 | smooth loss: 1.98741\n",
      "Best model at iteration: 12820 | smooth loss: 1.98726\n",
      "Best model at iteration: 12832 | smooth loss: 1.98699\n",
      "Best model at iteration: 12833 | smooth loss: 1.98678\n",
      "Best model at iteration: 12835 | smooth loss: 1.98617\n",
      "Best model at iteration: 12836 | smooth loss: 1.98564\n",
      "Best model at iteration: 12842 | smooth loss: 1.98537\n",
      "Best model at iteration: 12844 | smooth loss: 1.98513\n",
      "Best model at iteration: 12858 | smooth loss: 1.98506\n",
      "Best model at iteration: 12859 | smooth loss: 1.98424\n",
      "Best model at iteration: 12877 | smooth loss: 1.98399\n",
      "Best model at iteration: 12878 | smooth loss: 1.98351\n",
      "Best model at iteration: 12879 | smooth loss: 1.98337\n",
      "Best model at iteration: 12891 | smooth loss: 1.98291\n",
      "Best model at iteration: 12892 | smooth loss: 1.98230\n",
      "Best model at iteration: 12898 | smooth loss: 1.98225\n",
      "Best model at iteration: 12899 | smooth loss: 1.98215\n",
      "Best model at iteration: 12902 | smooth loss: 1.98191\n",
      "Best model at iteration: 12907 | smooth loss: 1.98189\n",
      "Best model at iteration: 12912 | smooth loss: 1.98139\n",
      "iter = 13000, smooth loss=1.9833649935582731\n",
      "g fermane th.  Hom the whote tedbfeaving and ppotend.  I moty nemped, you's togrts. . . whed wast sow progut lacVed;.  They had naws.  \"Ot,  he snay could sead of the gatthe gack a to the rlinged the \n",
      "\n",
      "Best model at iteration: 13011 | smooth loss: 1.98117\n",
      "Best model at iteration: 13012 | smooth loss: 1.98078\n",
      "Best model at iteration: 13014 | smooth loss: 1.98054\n",
      "Best model at iteration: 13015 | smooth loss: 1.97990\n",
      "Best model at iteration: 13016 | smooth loss: 1.97977\n",
      "Best model at iteration: 13017 | smooth loss: 1.97950\n",
      "Best model at iteration: 13021 | smooth loss: 1.97946\n",
      "Best model at iteration: 13025 | smooth loss: 1.97923\n",
      "Best model at iteration: 13027 | smooth loss: 1.97903\n",
      "Best model at iteration: 13028 | smooth loss: 1.97900\n",
      "Best model at iteration: 13033 | smooth loss: 1.97889\n",
      "Best model at iteration: 13034 | smooth loss: 1.97858\n",
      "Best model at iteration: 13035 | smooth loss: 1.97837\n",
      "Best model at iteration: 13036 | smooth loss: 1.97801\n",
      "Best model at iteration: 13041 | smooth loss: 1.97796\n",
      "Best model at iteration: 13042 | smooth loss: 1.97758\n",
      "Best model at iteration: 13044 | smooth loss: 1.97750\n",
      "Best model at iteration: 13045 | smooth loss: 1.97721\n",
      "Best model at iteration: 13046 | smooth loss: 1.97677\n",
      "Best model at iteration: 13047 | smooth loss: 1.97672\n",
      "Best model at iteration: 13048 | smooth loss: 1.97658\n",
      "Best model at iteration: 13049 | smooth loss: 1.97652\n",
      "Best model at iteration: 13050 | smooth loss: 1.97649\n",
      "Best model at iteration: 13052 | smooth loss: 1.97632\n",
      "Best model at iteration: 13053 | smooth loss: 1.97624\n",
      "Best model at iteration: 13054 | smooth loss: 1.97583\n",
      "Best model at iteration: 13058 | smooth loss: 1.97574\n",
      "Best model at iteration: 13059 | smooth loss: 1.97571\n",
      "Best model at iteration: 13060 | smooth loss: 1.97553\n",
      "Best model at iteration: 13062 | smooth loss: 1.97543\n",
      "Best model at iteration: 13063 | smooth loss: 1.97522\n",
      "Best model at iteration: 13064 | smooth loss: 1.97499\n",
      "Best model at iteration: 13065 | smooth loss: 1.97438\n",
      "Best model at iteration: 13070 | smooth loss: 1.97399\n",
      "Best model at iteration: 13072 | smooth loss: 1.97376\n",
      "Best model at iteration: 13073 | smooth loss: 1.97366\n",
      "Best model at iteration: 13077 | smooth loss: 1.97327\n",
      "Best model at iteration: 13078 | smooth loss: 1.97217\n",
      "Best model at iteration: 13079 | smooth loss: 1.97210\n",
      "Best model at iteration: 13080 | smooth loss: 1.97184\n",
      "Best model at iteration: 13084 | smooth loss: 1.97154\n",
      "Best model at iteration: 13085 | smooth loss: 1.97098\n",
      "Best model at iteration: 13086 | smooth loss: 1.97035\n",
      "Best model at iteration: 13087 | smooth loss: 1.96991\n",
      "Best model at iteration: 13088 | smooth loss: 1.96902\n",
      "Best model at iteration: 13089 | smooth loss: 1.96872\n",
      "Best model at iteration: 13090 | smooth loss: 1.96864\n",
      "Best model at iteration: 13091 | smooth loss: 1.96774\n",
      "Best model at iteration: 13093 | smooth loss: 1.96768\n",
      "Best model at iteration: 13094 | smooth loss: 1.96732\n",
      "Best model at iteration: 13104 | smooth loss: 1.96711\n",
      "Best model at iteration: 13105 | smooth loss: 1.96664\n",
      "Best model at iteration: 13111 | smooth loss: 1.96651\n",
      "Best model at iteration: 13112 | smooth loss: 1.96646\n",
      "Best model at iteration: 13113 | smooth loss: 1.96643\n",
      "Best model at iteration: 13115 | smooth loss: 1.96612\n",
      "Best model at iteration: 13117 | smooth loss: 1.96568\n",
      "Best model at iteration: 13118 | smooth loss: 1.96560\n",
      "Best model at iteration: 13119 | smooth loss: 1.96544\n",
      "Best model at iteration: 13120 | smooth loss: 1.96537\n",
      "Best model at iteration: 13122 | smooth loss: 1.96530\n",
      "Best model at iteration: 13123 | smooth loss: 1.96518\n",
      "Best model at iteration: 13124 | smooth loss: 1.96467\n",
      "Best model at iteration: 13126 | smooth loss: 1.96428\n",
      "Best model at iteration: 13129 | smooth loss: 1.96393\n",
      "Best model at iteration: 13131 | smooth loss: 1.96382\n",
      "Best model at iteration: 13132 | smooth loss: 1.96353\n",
      "Best model at iteration: 13152 | smooth loss: 1.96340\n",
      "Best model at iteration: 13153 | smooth loss: 1.96307\n",
      "Best model at iteration: 13156 | smooth loss: 1.96263\n",
      "Best model at iteration: 13157 | smooth loss: 1.96187\n",
      "Best model at iteration: 13159 | smooth loss: 1.96165\n",
      "Best model at iteration: 13160 | smooth loss: 1.96113\n",
      "Best model at iteration: 13167 | smooth loss: 1.96111\n",
      "Best model at iteration: 13169 | smooth loss: 1.96095\n",
      "Best model at iteration: 13170 | smooth loss: 1.96060\n",
      "Best model at iteration: 13171 | smooth loss: 1.96037\n",
      "Best model at iteration: 13172 | smooth loss: 1.96020\n",
      "Best model at iteration: 13173 | smooth loss: 1.95957\n",
      "Best model at iteration: 13174 | smooth loss: 1.95885\n",
      "Best model at iteration: 13177 | smooth loss: 1.95877\n",
      "Best model at iteration: 13178 | smooth loss: 1.95862\n",
      "Best model at iteration: 13179 | smooth loss: 1.95813\n",
      "Best model at iteration: 13180 | smooth loss: 1.95782\n",
      "Best model at iteration: 13181 | smooth loss: 1.95726\n",
      "Best model at iteration: 13188 | smooth loss: 1.95707\n",
      "Best model at iteration: 13190 | smooth loss: 1.95687\n",
      "Best model at iteration: 13195 | smooth loss: 1.95640\n",
      "Best model at iteration: 13196 | smooth loss: 1.95622\n",
      "Best model at iteration: 13197 | smooth loss: 1.95620\n",
      "Best model at iteration: 13200 | smooth loss: 1.95600\n",
      "Best model at iteration: 13202 | smooth loss: 1.95522\n",
      "Best model at iteration: 13207 | smooth loss: 1.95512\n",
      "Best model at iteration: 13210 | smooth loss: 1.95494\n",
      "Best model at iteration: 13212 | smooth loss: 1.95488\n",
      "Best model at iteration: 13213 | smooth loss: 1.95476\n",
      "Best model at iteration: 13224 | smooth loss: 1.95474\n",
      "Best model at iteration: 13227 | smooth loss: 1.95447\n",
      "Best model at iteration: 13228 | smooth loss: 1.95441\n",
      "Best model at iteration: 13229 | smooth loss: 1.95433\n",
      "Best model at iteration: 13307 | smooth loss: 1.95419\n",
      "Best model at iteration: 13308 | smooth loss: 1.95394\n",
      "Best model at iteration: 13316 | smooth loss: 1.95392\n",
      "Best model at iteration: 13317 | smooth loss: 1.95370\n",
      "Best model at iteration: 13319 | smooth loss: 1.95328\n",
      "Best model at iteration: 13325 | smooth loss: 1.95321\n",
      "Best model at iteration: 13328 | smooth loss: 1.95298\n",
      "Best model at iteration: 13339 | smooth loss: 1.95297\n",
      "Best model at iteration: 13340 | smooth loss: 1.95263\n",
      "Best model at iteration: 13344 | smooth loss: 1.95247\n",
      "Best model at iteration: 13345 | smooth loss: 1.95219\n",
      "Best model at iteration: 13346 | smooth loss: 1.95205\n",
      "Best model at iteration: 13347 | smooth loss: 1.95204\n",
      "Best model at iteration: 13351 | smooth loss: 1.95194\n",
      "Best model at iteration: 13352 | smooth loss: 1.95189\n",
      "Best model at iteration: 13353 | smooth loss: 1.95138\n",
      "Best model at iteration: 13354 | smooth loss: 1.95131\n",
      "Best model at iteration: 13355 | smooth loss: 1.95108\n",
      "Best model at iteration: 13362 | smooth loss: 1.95063\n",
      "Best model at iteration: 13366 | smooth loss: 1.95024\n",
      "Best model at iteration: 13367 | smooth loss: 1.95017\n",
      "Best model at iteration: 13368 | smooth loss: 1.94999\n",
      "Best model at iteration: 13369 | smooth loss: 1.94997\n",
      "Best model at iteration: 13370 | smooth loss: 1.94992\n",
      "Best model at iteration: 13373 | smooth loss: 1.94931\n",
      "Best model at iteration: 13374 | smooth loss: 1.94922\n",
      "Best model at iteration: 13375 | smooth loss: 1.94906\n",
      "Best model at iteration: 13376 | smooth loss: 1.94872\n",
      "Best model at iteration: 13398 | smooth loss: 1.94857\n",
      "Best model at iteration: 13531 | smooth loss: 1.94836\n",
      "Best model at iteration: 13534 | smooth loss: 1.94818\n",
      "Best model at iteration: 13535 | smooth loss: 1.94798\n",
      "Best model at iteration: 13538 | smooth loss: 1.94768\n",
      "Best model at iteration: 13539 | smooth loss: 1.94733\n",
      "Best model at iteration: 13548 | smooth loss: 1.94730\n",
      "Best model at iteration: 13549 | smooth loss: 1.94705\n",
      "Best model at iteration: 13551 | smooth loss: 1.94658\n",
      "Best model at iteration: 13552 | smooth loss: 1.94637\n",
      "iter = 14000, smooth loss=1.9563237839246497\n",
      "pabmets.\"\n",
      "This,\"\n",
      "\"Keruve sets tiansly lloont hlisbed a porthave cin,\" hag ow the HealaMoffeogrsteally urbed with a deane do tor cumall neligaly pros poritable fiviont the.  \" GroP.\n",
      "Jhis blly paves his\n",
      "\n",
      "iter = 15000, smooth loss=1.94819852545322\n",
      " arrake,\" said Roncaund at of wice, caust was and his atbat!\" \n",
      "\"Omating a desten.  \"It eated a winle, woffiction wasks has.\n",
      "\"Lous atly he wase hay goons al agoll.  \"Whed by a stild to sitt efarsorn, w\n",
      "\n",
      "Best model at iteration: 15031 | smooth loss: 1.94635\n",
      "Best model at iteration: 15034 | smooth loss: 1.94620\n",
      "Best model at iteration: 15035 | smooth loss: 1.94554\n",
      "Best model at iteration: 15036 | smooth loss: 1.94487\n",
      "Best model at iteration: 15037 | smooth loss: 1.94417\n",
      "Best model at iteration: 15043 | smooth loss: 1.94381\n",
      "Best model at iteration: 15044 | smooth loss: 1.94364\n",
      "Best model at iteration: 15045 | smooth loss: 1.94358\n",
      "Best model at iteration: 15083 | smooth loss: 1.94327\n",
      "Best model at iteration: 15087 | smooth loss: 1.94304\n",
      "Best model at iteration: 15093 | smooth loss: 1.94290\n",
      "Best model at iteration: 15098 | smooth loss: 1.94255\n",
      "Best model at iteration: 15100 | smooth loss: 1.94161\n",
      "Best model at iteration: 15102 | smooth loss: 1.94157\n",
      "Best model at iteration: 15104 | smooth loss: 1.94123\n",
      "Best model at iteration: 15105 | smooth loss: 1.94113\n",
      "Best model at iteration: 15106 | smooth loss: 1.94093\n",
      "Best model at iteration: 15107 | smooth loss: 1.94047\n",
      "Best model at iteration: 15108 | smooth loss: 1.94025\n",
      "Best model at iteration: 15109 | smooth loss: 1.94015\n",
      "Best model at iteration: 15110 | smooth loss: 1.93967\n",
      "Best model at iteration: 15119 | smooth loss: 1.93964\n",
      "Best model at iteration: 15122 | smooth loss: 1.93933\n",
      "Best model at iteration: 15123 | smooth loss: 1.93887\n",
      "Best model at iteration: 15124 | smooth loss: 1.93840\n",
      "Best model at iteration: 15125 | smooth loss: 1.93827\n",
      "Best model at iteration: 15126 | smooth loss: 1.93818\n",
      "Best model at iteration: 15150 | smooth loss: 1.93815\n",
      "Best model at iteration: 15151 | smooth loss: 1.93807\n",
      "Best model at iteration: 15152 | smooth loss: 1.93775\n",
      "Best model at iteration: 15153 | smooth loss: 1.93710\n",
      "Best model at iteration: 15154 | smooth loss: 1.93707\n",
      "Best model at iteration: 15155 | smooth loss: 1.93681\n",
      "Best model at iteration: 15157 | smooth loss: 1.93654\n",
      "Best model at iteration: 15159 | smooth loss: 1.93632\n",
      "Best model at iteration: 15272 | smooth loss: 1.93603\n",
      "Best model at iteration: 15274 | smooth loss: 1.93578\n",
      "Best model at iteration: 15275 | smooth loss: 1.93542\n",
      "Best model at iteration: 15277 | smooth loss: 1.93533\n",
      "Best model at iteration: 15279 | smooth loss: 1.93516\n",
      "Best model at iteration: 15282 | smooth loss: 1.93515\n",
      "Best model at iteration: 15283 | smooth loss: 1.93506\n",
      "Best model at iteration: 15284 | smooth loss: 1.93475\n",
      "Best model at iteration: 15292 | smooth loss: 1.93467\n",
      "Best model at iteration: 15293 | smooth loss: 1.93415\n",
      "Best model at iteration: 15302 | smooth loss: 1.93374\n",
      "Best model at iteration: 15304 | smooth loss: 1.93368\n",
      "Best model at iteration: 15305 | smooth loss: 1.93359\n",
      "Best model at iteration: 15309 | smooth loss: 1.93312\n",
      "Best model at iteration: 15312 | smooth loss: 1.93307\n",
      "Best model at iteration: 15315 | smooth loss: 1.93279\n",
      "Best model at iteration: 15318 | smooth loss: 1.93261\n",
      "Best model at iteration: 15319 | smooth loss: 1.93255\n",
      "Best model at iteration: 15320 | smooth loss: 1.93205\n",
      "Best model at iteration: 15321 | smooth loss: 1.93203\n",
      "Best model at iteration: 15322 | smooth loss: 1.93184\n",
      "Best model at iteration: 15324 | smooth loss: 1.93182\n",
      "Best model at iteration: 15325 | smooth loss: 1.93110\n",
      "Best model at iteration: 15326 | smooth loss: 1.93053\n",
      "Best model at iteration: 15327 | smooth loss: 1.93044\n",
      "Best model at iteration: 15329 | smooth loss: 1.93039\n",
      "Best model at iteration: 15330 | smooth loss: 1.93018\n",
      "Best model at iteration: 15331 | smooth loss: 1.92990\n",
      "Best model at iteration: 15335 | smooth loss: 1.92988\n",
      "Best model at iteration: 15336 | smooth loss: 1.92949\n",
      "Best model at iteration: 15343 | smooth loss: 1.92945\n",
      "Best model at iteration: 15344 | smooth loss: 1.92911\n",
      "Best model at iteration: 15348 | smooth loss: 1.92903\n",
      "Best model at iteration: 15353 | smooth loss: 1.92890\n",
      "Best model at iteration: 15354 | smooth loss: 1.92877\n",
      "Best model at iteration: 15355 | smooth loss: 1.92874\n",
      "Best model at iteration: 15356 | smooth loss: 1.92863\n",
      "Best model at iteration: 15358 | smooth loss: 1.92834\n",
      "Best model at iteration: 15359 | smooth loss: 1.92773\n",
      "Best model at iteration: 15360 | smooth loss: 1.92738\n",
      "Best model at iteration: 15361 | smooth loss: 1.92724\n",
      "Best model at iteration: 15363 | smooth loss: 1.92683\n",
      "Best model at iteration: 15366 | smooth loss: 1.92683\n",
      "Best model at iteration: 15367 | smooth loss: 1.92679\n",
      "Best model at iteration: 15368 | smooth loss: 1.92653\n",
      "Best model at iteration: 15370 | smooth loss: 1.92645\n",
      "Best model at iteration: 15372 | smooth loss: 1.92628\n",
      "Best model at iteration: 15373 | smooth loss: 1.92613\n",
      "Best model at iteration: 15374 | smooth loss: 1.92601\n",
      "Best model at iteration: 15375 | smooth loss: 1.92574\n",
      "Best model at iteration: 15376 | smooth loss: 1.92519\n",
      "Best model at iteration: 15377 | smooth loss: 1.92494\n",
      "Best model at iteration: 15378 | smooth loss: 1.92452\n",
      "Best model at iteration: 15379 | smooth loss: 1.92451\n",
      "Best model at iteration: 15386 | smooth loss: 1.92440\n",
      "Best model at iteration: 15395 | smooth loss: 1.92439\n",
      "Best model at iteration: 15396 | smooth loss: 1.92402\n",
      "Best model at iteration: 15411 | smooth loss: 1.92364\n",
      "Best model at iteration: 15412 | smooth loss: 1.92353\n",
      "Best model at iteration: 15413 | smooth loss: 1.92309\n",
      "Best model at iteration: 15415 | smooth loss: 1.92272\n",
      "Best model at iteration: 15416 | smooth loss: 1.92270\n",
      "Best model at iteration: 15417 | smooth loss: 1.92200\n",
      "Best model at iteration: 15418 | smooth loss: 1.92148\n",
      "Best model at iteration: 15419 | smooth loss: 1.92147\n",
      "Best model at iteration: 15421 | smooth loss: 1.92117\n",
      "Best model at iteration: 15426 | smooth loss: 1.92093\n",
      "Best model at iteration: 15427 | smooth loss: 1.92072\n",
      "Best model at iteration: 15428 | smooth loss: 1.92047\n",
      "Best model at iteration: 15429 | smooth loss: 1.92031\n",
      "Best model at iteration: 15433 | smooth loss: 1.92024\n",
      "Best model at iteration: 15435 | smooth loss: 1.92004\n",
      "Best model at iteration: 15438 | smooth loss: 1.91990\n",
      "Best model at iteration: 15439 | smooth loss: 1.91951\n",
      "Best model at iteration: 15441 | smooth loss: 1.91946\n",
      "Best model at iteration: 15442 | smooth loss: 1.91944\n",
      "Best model at iteration: 15444 | smooth loss: 1.91943\n",
      "Best model at iteration: 15446 | smooth loss: 1.91928\n",
      "Best model at iteration: 15447 | smooth loss: 1.91885\n",
      "Best model at iteration: 15450 | smooth loss: 1.91858\n",
      "Best model at iteration: 15451 | smooth loss: 1.91819\n",
      "Best model at iteration: 15452 | smooth loss: 1.91810\n",
      "Best model at iteration: 15453 | smooth loss: 1.91782\n",
      "Best model at iteration: 15456 | smooth loss: 1.91762\n",
      "Best model at iteration: 15460 | smooth loss: 1.91745\n",
      "Best model at iteration: 15461 | smooth loss: 1.91697\n",
      "Best model at iteration: 15462 | smooth loss: 1.91679\n",
      "Best model at iteration: 15463 | smooth loss: 1.91649\n",
      "Best model at iteration: 15465 | smooth loss: 1.91644\n",
      "Best model at iteration: 15466 | smooth loss: 1.91613\n",
      "Best model at iteration: 15467 | smooth loss: 1.91589\n",
      "Best model at iteration: 15468 | smooth loss: 1.91584\n",
      "Best model at iteration: 15469 | smooth loss: 1.91560\n",
      "Best model at iteration: 15470 | smooth loss: 1.91557\n",
      "Best model at iteration: 15486 | smooth loss: 1.91555\n",
      "Best model at iteration: 15487 | smooth loss: 1.91508\n",
      "Best model at iteration: 15488 | smooth loss: 1.91450\n",
      "Best model at iteration: 15489 | smooth loss: 1.91441\n",
      "Best model at iteration: 15492 | smooth loss: 1.91438\n",
      "Best model at iteration: 15506 | smooth loss: 1.91432\n",
      "Best model at iteration: 15507 | smooth loss: 1.91393\n",
      "Best model at iteration: 15508 | smooth loss: 1.91387\n",
      "Best model at iteration: 15510 | smooth loss: 1.91371\n",
      "Best model at iteration: 15514 | smooth loss: 1.91365\n",
      "Best model at iteration: 15518 | smooth loss: 1.91329\n",
      "Best model at iteration: 15519 | smooth loss: 1.91272\n",
      "Best model at iteration: 15521 | smooth loss: 1.91269\n",
      "Best model at iteration: 15522 | smooth loss: 1.91243\n",
      "Best model at iteration: 15534 | smooth loss: 1.91219\n",
      "Best model at iteration: 15535 | smooth loss: 1.91180\n",
      "Best model at iteration: 15536 | smooth loss: 1.91179\n",
      "Best model at iteration: 15546 | smooth loss: 1.91171\n",
      "Best model at iteration: 15547 | smooth loss: 1.91134\n",
      "Best model at iteration: 15548 | smooth loss: 1.91115\n",
      "Best model at iteration: 15554 | smooth loss: 1.91114\n",
      "Best model at iteration: 15555 | smooth loss: 1.91096\n",
      "Best model at iteration: 15556 | smooth loss: 1.91089\n",
      "Best model at iteration: 15557 | smooth loss: 1.91056\n",
      "Best model at iteration: 15584 | smooth loss: 1.91007\n",
      "Best model at iteration: 15585 | smooth loss: 1.90959\n",
      "Best model at iteration: 15586 | smooth loss: 1.90934\n",
      "Best model at iteration: 15733 | smooth loss: 1.90910\n",
      "Best model at iteration: 15734 | smooth loss: 1.90866\n",
      "Best model at iteration: 15735 | smooth loss: 1.90824\n",
      "Best model at iteration: 15736 | smooth loss: 1.90793\n",
      "Best model at iteration: 15967 | smooth loss: 1.90788\n",
      "Best model at iteration: 15968 | smooth loss: 1.90779\n",
      "Best model at iteration: 15969 | smooth loss: 1.90749\n",
      "Best model at iteration: 15970 | smooth loss: 1.90730\n",
      "Best model at iteration: 15971 | smooth loss: 1.90723\n",
      "Best model at iteration: 15972 | smooth loss: 1.90706\n",
      "Best model at iteration: 15973 | smooth loss: 1.90700\n",
      "Best model at iteration: 15974 | smooth loss: 1.90647\n",
      "Best model at iteration: 15975 | smooth loss: 1.90588\n",
      "Best model at iteration: 15976 | smooth loss: 1.90561\n",
      "Best model at iteration: 15983 | smooth loss: 1.90534\n",
      "Best model at iteration: 15984 | smooth loss: 1.90488\n",
      "Best model at iteration: 15985 | smooth loss: 1.90427\n",
      "Best model at iteration: 15990 | smooth loss: 1.90399\n",
      "Best model at iteration: 15992 | smooth loss: 1.90371\n",
      "Best model at iteration: 15996 | smooth loss: 1.90364\n",
      "Best model at iteration: 15997 | smooth loss: 1.90351\n",
      "iter = 16000, smooth loss=1.90373453225295\n",
      "nd saigen sidering it oge leaknd quich, ADlEFgena.  Hagrid foo wait, thoughteed,\" said Half cid go them the gatred afrout ee to heading a?\"\n",
      "\"Tuth ther comingncubacull hepl oup and neait - sucking up b\n",
      "\n",
      "Best model at iteration: 16010 | smooth loss: 1.90346\n",
      "Best model at iteration: 16012 | smooth loss: 1.90319\n",
      "Best model at iteration: 16013 | smooth loss: 1.90312\n",
      "Best model at iteration: 16016 | smooth loss: 1.90299\n",
      "Best model at iteration: 16017 | smooth loss: 1.90256\n",
      "Best model at iteration: 16018 | smooth loss: 1.90211\n",
      "Best model at iteration: 16020 | smooth loss: 1.90207\n",
      "Best model at iteration: 16021 | smooth loss: 1.90161\n",
      "Best model at iteration: 16023 | smooth loss: 1.90082\n",
      "Best model at iteration: 16024 | smooth loss: 1.90049\n",
      "Best model at iteration: 16025 | smooth loss: 1.89998\n",
      "Best model at iteration: 16029 | smooth loss: 1.89992\n",
      "Best model at iteration: 16030 | smooth loss: 1.89962\n",
      "Best model at iteration: 16032 | smooth loss: 1.89946\n",
      "Best model at iteration: 16039 | smooth loss: 1.89934\n",
      "Best model at iteration: 16058 | smooth loss: 1.89921\n",
      "Best model at iteration: 16059 | smooth loss: 1.89891\n",
      "Best model at iteration: 16060 | smooth loss: 1.89873\n",
      "Best model at iteration: 16070 | smooth loss: 1.89870\n",
      "Best model at iteration: 16071 | smooth loss: 1.89828\n",
      "Best model at iteration: 16072 | smooth loss: 1.89793\n",
      "Best model at iteration: 16087 | smooth loss: 1.89766\n",
      "Best model at iteration: 16088 | smooth loss: 1.89743\n",
      "Best model at iteration: 16089 | smooth loss: 1.89712\n",
      "Best model at iteration: 16097 | smooth loss: 1.89706\n",
      "Best model at iteration: 16098 | smooth loss: 1.89695\n",
      "Best model at iteration: 16214 | smooth loss: 1.89677\n",
      "Best model at iteration: 16215 | smooth loss: 1.89673\n",
      "Best model at iteration: 16223 | smooth loss: 1.89641\n",
      "Best model at iteration: 16225 | smooth loss: 1.89629\n",
      "Best model at iteration: 16228 | smooth loss: 1.89606\n",
      "Best model at iteration: 16229 | smooth loss: 1.89558\n",
      "Best model at iteration: 16232 | smooth loss: 1.89528\n",
      "Best model at iteration: 16233 | smooth loss: 1.89522\n",
      "Best model at iteration: 16234 | smooth loss: 1.89492\n",
      "Best model at iteration: 16236 | smooth loss: 1.89481\n",
      "Best model at iteration: 16240 | smooth loss: 1.89455\n",
      "Best model at iteration: 16243 | smooth loss: 1.89435\n",
      "Best model at iteration: 16244 | smooth loss: 1.89416\n",
      "Best model at iteration: 16246 | smooth loss: 1.89341\n",
      "Best model at iteration: 16247 | smooth loss: 1.89339\n",
      "Best model at iteration: 16248 | smooth loss: 1.89284\n",
      "Best model at iteration: 16249 | smooth loss: 1.89198\n",
      "Best model at iteration: 16250 | smooth loss: 1.89126\n",
      "Best model at iteration: 16251 | smooth loss: 1.89124\n",
      "Best model at iteration: 16267 | smooth loss: 1.89118\n",
      "Best model at iteration: 16270 | smooth loss: 1.89104\n",
      "Best model at iteration: 16274 | smooth loss: 1.89078\n",
      "Best model at iteration: 16277 | smooth loss: 1.89003\n",
      "Best model at iteration: 16279 | smooth loss: 1.88987\n",
      "Best model at iteration: 16286 | smooth loss: 1.88943\n",
      "Best model at iteration: 16287 | smooth loss: 1.88876\n",
      "Best model at iteration: 16288 | smooth loss: 1.88858\n",
      "Best model at iteration: 16289 | smooth loss: 1.88804\n",
      "Best model at iteration: 16290 | smooth loss: 1.88798\n",
      "Best model at iteration: 16296 | smooth loss: 1.88789\n",
      "Best model at iteration: 16297 | smooth loss: 1.88745\n",
      "Best model at iteration: 16299 | smooth loss: 1.88724\n",
      "Best model at iteration: 16313 | smooth loss: 1.88668\n",
      "Best model at iteration: 16314 | smooth loss: 1.88653\n",
      "Best model at iteration: 16315 | smooth loss: 1.88611\n",
      "Best model at iteration: 16316 | smooth loss: 1.88509\n",
      "Best model at iteration: 16317 | smooth loss: 1.88507\n",
      "Best model at iteration: 16318 | smooth loss: 1.88487\n",
      "Best model at iteration: 16319 | smooth loss: 1.88421\n",
      "Best model at iteration: 16320 | smooth loss: 1.88371\n",
      "Best model at iteration: 16321 | smooth loss: 1.88363\n",
      "Best model at iteration: 16322 | smooth loss: 1.88359\n",
      "Best model at iteration: 16325 | smooth loss: 1.88343\n",
      "Best model at iteration: 16326 | smooth loss: 1.88310\n",
      "Best model at iteration: 16327 | smooth loss: 1.88263\n",
      "Best model at iteration: 16328 | smooth loss: 1.88194\n",
      "Best model at iteration: 16329 | smooth loss: 1.88159\n",
      "Best model at iteration: 16337 | smooth loss: 1.88124\n",
      "Best model at iteration: 16339 | smooth loss: 1.88110\n",
      "Best model at iteration: 16340 | smooth loss: 1.88093\n",
      "Best model at iteration: 16344 | smooth loss: 1.88079\n",
      "Best model at iteration: 16369 | smooth loss: 1.88078\n",
      "Best model at iteration: 16370 | smooth loss: 1.88012\n",
      "Best model at iteration: 16374 | smooth loss: 1.87987\n",
      "Best model at iteration: 16376 | smooth loss: 1.87963\n",
      "Best model at iteration: 16908 | smooth loss: 1.87941\n",
      "Best model at iteration: 16910 | smooth loss: 1.87913\n",
      "Best model at iteration: 16911 | smooth loss: 1.87893\n",
      "Best model at iteration: 16912 | smooth loss: 1.87866\n",
      "Best model at iteration: 16913 | smooth loss: 1.87802\n",
      "Best model at iteration: 16914 | smooth loss: 1.87776\n",
      "Best model at iteration: 16915 | smooth loss: 1.87762\n",
      "Best model at iteration: 16917 | smooth loss: 1.87716\n",
      "Best model at iteration: 16918 | smooth loss: 1.87704\n",
      "Best model at iteration: 16919 | smooth loss: 1.87701\n",
      "Best model at iteration: 16925 | smooth loss: 1.87685\n",
      "Best model at iteration: 16931 | smooth loss: 1.87646\n",
      "Best model at iteration: 16934 | smooth loss: 1.87639\n",
      "Best model at iteration: 16935 | smooth loss: 1.87623\n",
      "iter = 17000, smooth loss=1.876746719531251\n",
      "sht has about.  The ther andone hive't have ho'p hat Durbom Pringed.  Bell turtincu diluth hime of your ham with rave bageven coup.\n",
      "\"Have to nowning wast batee no cednous. . . Cupred, whot dowe, hid a\n",
      "\n",
      "Best model at iteration: 17101 | smooth loss: 1.87613\n",
      "Best model at iteration: 17107 | smooth loss: 1.87595\n",
      "Best model at iteration: 17108 | smooth loss: 1.87587\n",
      "Best model at iteration: 17117 | smooth loss: 1.87559\n",
      "Best model at iteration: 17118 | smooth loss: 1.87544\n",
      "Best model at iteration: 17119 | smooth loss: 1.87510\n",
      "Best model at iteration: 17122 | smooth loss: 1.87491\n",
      "Best model at iteration: 17123 | smooth loss: 1.87452\n",
      "Best model at iteration: 17124 | smooth loss: 1.87450\n",
      "Best model at iteration: 17125 | smooth loss: 1.87432\n",
      "Best model at iteration: 17126 | smooth loss: 1.87419\n",
      "Best model at iteration: 17127 | smooth loss: 1.87399\n",
      "Best model at iteration: 17132 | smooth loss: 1.87389\n",
      "Best model at iteration: 17133 | smooth loss: 1.87362\n",
      "Best model at iteration: 17157 | smooth loss: 1.87362\n",
      "Best model at iteration: 17158 | smooth loss: 1.87322\n",
      "Best model at iteration: 17159 | smooth loss: 1.87293\n",
      "Best model at iteration: 17163 | smooth loss: 1.87288\n",
      "Best model at iteration: 17164 | smooth loss: 1.87251\n",
      "Best model at iteration: 17176 | smooth loss: 1.87229\n",
      "Best model at iteration: 17177 | smooth loss: 1.87209\n",
      "Best model at iteration: 17189 | smooth loss: 1.87209\n",
      "Best model at iteration: 17193 | smooth loss: 1.87194\n",
      "Best model at iteration: 17194 | smooth loss: 1.87173\n",
      "Best model at iteration: 17195 | smooth loss: 1.87151\n",
      "Best model at iteration: 17196 | smooth loss: 1.87120\n",
      "Best model at iteration: 17199 | smooth loss: 1.87049\n",
      "Best model at iteration: 17200 | smooth loss: 1.87031\n",
      "Best model at iteration: 17203 | smooth loss: 1.87008\n",
      "Best model at iteration: 17286 | smooth loss: 1.87005\n",
      "Best model at iteration: 17287 | smooth loss: 1.86967\n",
      "Best model at iteration: 17288 | smooth loss: 1.86932\n",
      "Best model at iteration: 17289 | smooth loss: 1.86921\n",
      "Best model at iteration: 17292 | smooth loss: 1.86908\n",
      "Best model at iteration: 17293 | smooth loss: 1.86899\n",
      "Best model at iteration: 17294 | smooth loss: 1.86876\n",
      "Best model at iteration: 17295 | smooth loss: 1.86850\n",
      "Best model at iteration: 17296 | smooth loss: 1.86813\n",
      "Best model at iteration: 17297 | smooth loss: 1.86796\n",
      "Best model at iteration: 17298 | smooth loss: 1.86768\n",
      "Best model at iteration: 17305 | smooth loss: 1.86750\n",
      "Best model at iteration: 17306 | smooth loss: 1.86741\n",
      "Best model at iteration: 17325 | smooth loss: 1.86726\n",
      "Best model at iteration: 17326 | smooth loss: 1.86708\n",
      "Best model at iteration: 17328 | smooth loss: 1.86682\n",
      "Best model at iteration: 17329 | smooth loss: 1.86672\n",
      "Best model at iteration: 17334 | smooth loss: 1.86652\n",
      "Best model at iteration: 17337 | smooth loss: 1.86648\n",
      "Best model at iteration: 17338 | smooth loss: 1.86644\n",
      "Best model at iteration: 17372 | smooth loss: 1.86644\n",
      "Best model at iteration: 17373 | smooth loss: 1.86599\n",
      "Best model at iteration: 17375 | smooth loss: 1.86592\n",
      "Best model at iteration: 17376 | smooth loss: 1.86579\n",
      "Best model at iteration: 17378 | smooth loss: 1.86573\n",
      "Best model at iteration: 17379 | smooth loss: 1.86478\n",
      "Best model at iteration: 17380 | smooth loss: 1.86454\n",
      "Best model at iteration: 17382 | smooth loss: 1.86435\n",
      "Best model at iteration: 17384 | smooth loss: 1.86433\n",
      "Best model at iteration: 17391 | smooth loss: 1.86393\n",
      "Best model at iteration: 17392 | smooth loss: 1.86343\n",
      "Best model at iteration: 17394 | smooth loss: 1.86305\n",
      "Best model at iteration: 17396 | smooth loss: 1.86222\n",
      "Best model at iteration: 17397 | smooth loss: 1.86176\n",
      "Best model at iteration: 17398 | smooth loss: 1.86168\n",
      "Best model at iteration: 17399 | smooth loss: 1.86159\n",
      "Best model at iteration: 17403 | smooth loss: 1.86156\n",
      "Best model at iteration: 17414 | smooth loss: 1.86150\n",
      "Best model at iteration: 17418 | smooth loss: 1.86150\n",
      "Best model at iteration: 17420 | smooth loss: 1.86120\n",
      "Best model at iteration: 17421 | smooth loss: 1.86101\n",
      "Best model at iteration: 17426 | smooth loss: 1.86082\n",
      "Best model at iteration: 17427 | smooth loss: 1.86061\n",
      "Best model at iteration: 17430 | smooth loss: 1.86021\n",
      "Best model at iteration: 17431 | smooth loss: 1.85992\n",
      "Best model at iteration: 17452 | smooth loss: 1.85991\n",
      "Best model at iteration: 17506 | smooth loss: 1.85982\n",
      "Best model at iteration: 17507 | smooth loss: 1.85909\n",
      "Best model at iteration: 17508 | smooth loss: 1.85849\n",
      "Best model at iteration: 17509 | smooth loss: 1.85840\n",
      "Best model at iteration: 17510 | smooth loss: 1.85820\n",
      "iter = 18000, smooth loss=1.8762408996455584\n",
      "ve gait, and bown neatingwarch attherin. .n ampllincustly and extums at ag t wask to he, the voup e-chime.  Thet and Harry, scappsent!  The liGet bnat verying.  Bet noichanches ast to Pontact term.\"\n",
      "\t\n",
      "\n",
      "iter = 19000, smooth loss=1.8718782408689398\n",
      "o sugh if sharta fariary she justion lither Mongirgen thromen in gole hampienst Exmenter?\"\n",
      "He misn't atabeee sisirins onhe master a mevinget blarmond Harry sheme ali, fee on shembaits hen his not's wi\n",
      "\n",
      "Best model at iteration: 19375 | smooth loss: 1.85800\n",
      "Best model at iteration: 19376 | smooth loss: 1.85790\n",
      "Best model at iteration: 19377 | smooth loss: 1.85771\n",
      "Best model at iteration: 19379 | smooth loss: 1.85759\n",
      "Best model at iteration: 19380 | smooth loss: 1.85717\n",
      "Best model at iteration: 19381 | smooth loss: 1.85678\n",
      "Best model at iteration: 19382 | smooth loss: 1.85657\n",
      "Best model at iteration: 19383 | smooth loss: 1.85656\n",
      "Best model at iteration: 19385 | smooth loss: 1.85638\n",
      "Best model at iteration: 19387 | smooth loss: 1.85637\n",
      "Best model at iteration: 19388 | smooth loss: 1.85626\n",
      "Best model at iteration: 19389 | smooth loss: 1.85602\n",
      "Best model at iteration: 19390 | smooth loss: 1.85562\n",
      "Best model at iteration: 19391 | smooth loss: 1.85540\n",
      "Best model at iteration: 19392 | smooth loss: 1.85492\n",
      "Best model at iteration: 19393 | smooth loss: 1.85464\n",
      "Best model at iteration: 19394 | smooth loss: 1.85419\n",
      "Best model at iteration: 19395 | smooth loss: 1.85396\n",
      "Best model at iteration: 19430 | smooth loss: 1.85388\n",
      "Best model at iteration: 19437 | smooth loss: 1.85337\n",
      "Best model at iteration: 19439 | smooth loss: 1.85275\n",
      "Best model at iteration: 19440 | smooth loss: 1.85220\n",
      "Best model at iteration: 19441 | smooth loss: 1.85196\n",
      "Best model at iteration: 19442 | smooth loss: 1.85165\n",
      "Best model at iteration: 19443 | smooth loss: 1.85155\n",
      "Best model at iteration: 19445 | smooth loss: 1.85149\n",
      "Best model at iteration: 19447 | smooth loss: 1.85100\n",
      "Best model at iteration: 19448 | smooth loss: 1.85093\n",
      "Best model at iteration: 19487 | smooth loss: 1.85059\n",
      "Best model at iteration: 19497 | smooth loss: 1.85056\n",
      "Best model at iteration: 19498 | smooth loss: 1.85013\n",
      "Best model at iteration: 19499 | smooth loss: 1.85011\n",
      "Best model at iteration: 19500 | smooth loss: 1.84983\n",
      "Best model at iteration: 19502 | smooth loss: 1.84970\n",
      "Best model at iteration: 19503 | smooth loss: 1.84958\n",
      "Best model at iteration: 19506 | smooth loss: 1.84949\n",
      "Best model at iteration: 19507 | smooth loss: 1.84895\n",
      "Best model at iteration: 19508 | smooth loss: 1.84888\n",
      "Best model at iteration: 19509 | smooth loss: 1.84886\n",
      "Best model at iteration: 19510 | smooth loss: 1.84875\n",
      "Best model at iteration: 19511 | smooth loss: 1.84813\n",
      "Best model at iteration: 19512 | smooth loss: 1.84781\n",
      "Best model at iteration: 19513 | smooth loss: 1.84776\n",
      "Best model at iteration: 19543 | smooth loss: 1.84755\n",
      "Best model at iteration: 19544 | smooth loss: 1.84715\n",
      "Best model at iteration: 19545 | smooth loss: 1.84684\n",
      "Best model at iteration: 19546 | smooth loss: 1.84683\n",
      "Best model at iteration: 19547 | smooth loss: 1.84680\n",
      "Best model at iteration: 19556 | smooth loss: 1.84666\n",
      "Best model at iteration: 19557 | smooth loss: 1.84648\n",
      "Best model at iteration: 19854 | smooth loss: 1.84644\n",
      "Best model at iteration: 19855 | smooth loss: 1.84610\n",
      "Best model at iteration: 19856 | smooth loss: 1.84581\n",
      "Best model at iteration: 19857 | smooth loss: 1.84559\n",
      "Best model at iteration: 19861 | smooth loss: 1.84538\n",
      "Best model at iteration: 19863 | smooth loss: 1.84530\n",
      "Best model at iteration: 19864 | smooth loss: 1.84505\n",
      "Best model at iteration: 19865 | smooth loss: 1.84495\n",
      "Best model at iteration: 19866 | smooth loss: 1.84481\n",
      "Best model at iteration: 19867 | smooth loss: 1.84462\n",
      "Best model at iteration: 19915 | smooth loss: 1.84416\n",
      "Best model at iteration: 19923 | smooth loss: 1.84408\n",
      "Best model at iteration: 19924 | smooth loss: 1.84401\n",
      "Best model at iteration: 19938 | smooth loss: 1.84398\n",
      "Best model at iteration: 19940 | smooth loss: 1.84369\n",
      "Best model at iteration: 19942 | smooth loss: 1.84344\n",
      "Best model at iteration: 19945 | smooth loss: 1.84338\n",
      "Best model at iteration: 19948 | smooth loss: 1.84309\n",
      "Best model at iteration: 19949 | smooth loss: 1.84299\n",
      "Best model at iteration: 19950 | smooth loss: 1.84279\n",
      "Best model at iteration: 19952 | smooth loss: 1.84273\n",
      "Best model at iteration: 19966 | smooth loss: 1.84252\n",
      "Best model at iteration: 19967 | smooth loss: 1.84189\n",
      "Best model at iteration: 19968 | smooth loss: 1.84163\n",
      "Best model at iteration: 19970 | smooth loss: 1.84125\n",
      "Best model at iteration: 19976 | smooth loss: 1.84059\n",
      "iter = 20000, smooth loss=1.841154108345016\n",
      ",\" she a lackn gite, anting. \n",
      "Wuaking an Treakins brublf.\"\n",
      "\"What sand fingehand the cittle fot kionilins  overing homist you Extrontowned with Eo \tas he soundr's anound the furittod Hogwartacoust He.\"\n",
      "\n",
      "Best model at iteration: 20010 | smooth loss: 1.84048\n",
      "Best model at iteration: 20014 | smooth loss: 1.84011\n",
      "Best model at iteration: 20015 | smooth loss: 1.83986\n",
      "Best model at iteration: 20016 | smooth loss: 1.83969\n",
      "Best model at iteration: 20020 | smooth loss: 1.83942\n",
      "Best model at iteration: 20021 | smooth loss: 1.83941\n",
      "Best model at iteration: 20023 | smooth loss: 1.83890\n",
      "Best model at iteration: 20029 | smooth loss: 1.83859\n",
      "Best model at iteration: 20030 | smooth loss: 1.83821\n",
      "Best model at iteration: 20032 | smooth loss: 1.83806\n",
      "Best model at iteration: 20036 | smooth loss: 1.83798\n",
      "Best model at iteration: 20037 | smooth loss: 1.83772\n",
      "Best model at iteration: 20100 | smooth loss: 1.83769\n",
      "Best model at iteration: 20101 | smooth loss: 1.83733\n",
      "Best model at iteration: 20102 | smooth loss: 1.83720\n",
      "Best model at iteration: 20103 | smooth loss: 1.83667\n",
      "Best model at iteration: 20124 | smooth loss: 1.83617\n",
      "Best model at iteration: 20125 | smooth loss: 1.83587\n",
      "Best model at iteration: 20126 | smooth loss: 1.83551\n",
      "Best model at iteration: 20137 | smooth loss: 1.83527\n",
      "Best model at iteration: 20143 | smooth loss: 1.83514\n",
      "Best model at iteration: 20145 | smooth loss: 1.83503\n",
      "Best model at iteration: 20176 | smooth loss: 1.83482\n",
      "Best model at iteration: 20177 | smooth loss: 1.83433\n",
      "Best model at iteration: 20179 | smooth loss: 1.83409\n",
      "Best model at iteration: 20182 | smooth loss: 1.83408\n",
      "Best model at iteration: 20183 | smooth loss: 1.83376\n",
      "Best model at iteration: 20184 | smooth loss: 1.83352\n",
      "Best model at iteration: 20185 | smooth loss: 1.83320\n",
      "Best model at iteration: 20191 | smooth loss: 1.83282\n",
      "Best model at iteration: 20192 | smooth loss: 1.83266\n",
      "Best model at iteration: 20547 | smooth loss: 1.83231\n",
      "Best model at iteration: 20552 | smooth loss: 1.83217\n",
      "Best model at iteration: 20553 | smooth loss: 1.83160\n",
      "Best model at iteration: 20554 | smooth loss: 1.83139\n",
      "Best model at iteration: 20555 | smooth loss: 1.83135\n",
      "Best model at iteration: 20556 | smooth loss: 1.83119\n",
      "Best model at iteration: 20557 | smooth loss: 1.83066\n",
      "Best model at iteration: 20558 | smooth loss: 1.83039\n",
      "Best model at iteration: 20932 | smooth loss: 1.83039\n",
      "Best model at iteration: 20933 | smooth loss: 1.83005\n",
      "Best model at iteration: 20934 | smooth loss: 1.82985\n",
      "Best model at iteration: 20936 | smooth loss: 1.82981\n",
      "Best model at iteration: 20937 | smooth loss: 1.82973\n",
      "Best model at iteration: 20939 | smooth loss: 1.82949\n",
      "Best model at iteration: 20942 | smooth loss: 1.82938\n",
      "Best model at iteration: 20946 | smooth loss: 1.82909\n",
      "Best model at iteration: 20947 | smooth loss: 1.82854\n",
      "Best model at iteration: 20949 | smooth loss: 1.82825\n",
      "Best model at iteration: 20950 | smooth loss: 1.82774\n",
      "Best model at iteration: 20951 | smooth loss: 1.82715\n",
      "Best model at iteration: 20952 | smooth loss: 1.82695\n",
      "Best model at iteration: 20953 | smooth loss: 1.82664\n",
      "Best model at iteration: 20955 | smooth loss: 1.82652\n",
      "Best model at iteration: 20956 | smooth loss: 1.82648\n",
      "Best model at iteration: 20957 | smooth loss: 1.82631\n",
      "Best model at iteration: 20960 | smooth loss: 1.82630\n",
      "Best model at iteration: 20961 | smooth loss: 1.82602\n",
      "Best model at iteration: 20962 | smooth loss: 1.82533\n",
      "Best model at iteration: 20963 | smooth loss: 1.82507\n",
      "Best model at iteration: 20970 | smooth loss: 1.82476\n",
      "Best model at iteration: 20973 | smooth loss: 1.82454\n",
      "Best model at iteration: 20990 | smooth loss: 1.82447\n",
      "Best model at iteration: 20991 | smooth loss: 1.82443\n",
      "Best model at iteration: 20993 | smooth loss: 1.82441\n",
      "iter = 21000, smooth loss=1.8245259012121997\n",
      "ng sconmrorssoring veroong, what's factully ragnot.\n",
      "Ol quidner thearienped, his aikione, with outhore, worch thengric o heary\n",
      "\"Oh.. Vart then wasle he was woicarragaing as oner, pooll, he couched un e\n",
      "\n",
      "Best model at iteration: 21101 | smooth loss: 1.82421\n",
      "Best model at iteration: 21102 | smooth loss: 1.82381\n",
      "Best model at iteration: 21103 | smooth loss: 1.82377\n",
      "Best model at iteration: 21104 | smooth loss: 1.82342\n",
      "Best model at iteration: 21105 | smooth loss: 1.82286\n",
      "Best model at iteration: 21108 | smooth loss: 1.82275\n",
      "Best model at iteration: 21109 | smooth loss: 1.82272\n",
      "Best model at iteration: 21110 | smooth loss: 1.82235\n",
      "Best model at iteration: 21112 | smooth loss: 1.82218\n",
      "Best model at iteration: 21122 | smooth loss: 1.82213\n",
      "Best model at iteration: 21125 | smooth loss: 1.82208\n",
      "Best model at iteration: 21127 | smooth loss: 1.82152\n",
      "Best model at iteration: 21128 | smooth loss: 1.82104\n",
      "Best model at iteration: 21129 | smooth loss: 1.82022\n",
      "Best model at iteration: 21131 | smooth loss: 1.82014\n",
      "Best model at iteration: 21132 | smooth loss: 1.81974\n",
      "Best model at iteration: 21133 | smooth loss: 1.81939\n",
      "Best model at iteration: 21139 | smooth loss: 1.81927\n",
      "Best model at iteration: 21142 | smooth loss: 1.81914\n",
      "Best model at iteration: 21143 | smooth loss: 1.81905\n",
      "Best model at iteration: 21144 | smooth loss: 1.81877\n",
      "Best model at iteration: 21145 | smooth loss: 1.81845\n",
      "Best model at iteration: 21151 | smooth loss: 1.81828\n",
      "Best model at iteration: 21155 | smooth loss: 1.81822\n",
      "Best model at iteration: 21157 | smooth loss: 1.81805\n",
      "Best model at iteration: 21158 | smooth loss: 1.81799\n",
      "Best model at iteration: 21269 | smooth loss: 1.81759\n",
      "Best model at iteration: 21272 | smooth loss: 1.81727\n",
      "Best model at iteration: 21274 | smooth loss: 1.81716\n",
      "Best model at iteration: 21310 | smooth loss: 1.81692\n",
      "Best model at iteration: 21311 | smooth loss: 1.81689\n",
      "Best model at iteration: 21312 | smooth loss: 1.81626\n",
      "Best model at iteration: 21316 | smooth loss: 1.81604\n",
      "Best model at iteration: 21317 | smooth loss: 1.81547\n",
      "Best model at iteration: 21318 | smooth loss: 1.81485\n",
      "Best model at iteration: 21319 | smooth loss: 1.81465\n",
      "Best model at iteration: 21329 | smooth loss: 1.81452\n",
      "Best model at iteration: 21503 | smooth loss: 1.81431\n",
      "Best model at iteration: 21507 | smooth loss: 1.81416\n",
      "Best model at iteration: 21513 | smooth loss: 1.81380\n",
      "Best model at iteration: 21522 | smooth loss: 1.81326\n",
      "Best model at iteration: 21523 | smooth loss: 1.81319\n",
      "Best model at iteration: 21524 | smooth loss: 1.81283\n",
      "Best model at iteration: 21532 | smooth loss: 1.81241\n",
      "Best model at iteration: 21533 | smooth loss: 1.81218\n",
      "Best model at iteration: 21535 | smooth loss: 1.81200\n",
      "Best model at iteration: 21536 | smooth loss: 1.81186\n",
      "Best model at iteration: 21539 | smooth loss: 1.81176\n",
      "Best model at iteration: 21540 | smooth loss: 1.81168\n",
      "Best model at iteration: 21544 | smooth loss: 1.81157\n",
      "Best model at iteration: 21545 | smooth loss: 1.81143\n",
      "Best model at iteration: 21551 | smooth loss: 1.81115\n",
      "Best model at iteration: 21552 | smooth loss: 1.81099\n",
      "Best model at iteration: 21562 | smooth loss: 1.81075\n",
      "Best model at iteration: 21568 | smooth loss: 1.81068\n",
      "Best model at iteration: 21575 | smooth loss: 1.81040\n",
      "Best model at iteration: 21576 | smooth loss: 1.80996\n",
      "Best model at iteration: 21577 | smooth loss: 1.80990\n",
      "Best model at iteration: 21578 | smooth loss: 1.80985\n",
      "Best model at iteration: 21579 | smooth loss: 1.80975\n",
      "Best model at iteration: 21588 | smooth loss: 1.80956\n",
      "Best model at iteration: 21589 | smooth loss: 1.80923\n",
      "Best model at iteration: 21590 | smooth loss: 1.80848\n",
      "Best model at iteration: 21591 | smooth loss: 1.80843\n",
      "Best model at iteration: 21592 | smooth loss: 1.80812\n",
      "Best model at iteration: 21596 | smooth loss: 1.80802\n",
      "Best model at iteration: 21599 | smooth loss: 1.80787\n",
      "Best model at iteration: 21600 | smooth loss: 1.80774\n",
      "Best model at iteration: 21603 | smooth loss: 1.80768\n",
      "Best model at iteration: 21606 | smooth loss: 1.80756\n",
      "Best model at iteration: 21607 | smooth loss: 1.80747\n",
      "Best model at iteration: 21609 | smooth loss: 1.80709\n",
      "Best model at iteration: 21614 | smooth loss: 1.80702\n",
      "Best model at iteration: 21620 | smooth loss: 1.80698\n",
      "Best model at iteration: 21621 | smooth loss: 1.80670\n",
      "Best model at iteration: 21622 | smooth loss: 1.80653\n",
      "Best model at iteration: 21650 | smooth loss: 1.80608\n",
      "Best model at iteration: 21651 | smooth loss: 1.80602\n",
      "Best model at iteration: 21681 | smooth loss: 1.80593\n",
      "Best model at iteration: 21688 | smooth loss: 1.80562\n",
      "Best model at iteration: 21689 | smooth loss: 1.80533\n",
      "Best model at iteration: 21690 | smooth loss: 1.80522\n",
      "Best model at iteration: 21691 | smooth loss: 1.80460\n",
      "Best model at iteration: 21692 | smooth loss: 1.80435\n",
      "Best model at iteration: 21693 | smooth loss: 1.80395\n",
      "Best model at iteration: 21698 | smooth loss: 1.80394\n",
      "Best model at iteration: 21699 | smooth loss: 1.80376\n",
      "Best model at iteration: 21700 | smooth loss: 1.80350\n",
      "Best model at iteration: 21701 | smooth loss: 1.80343\n",
      "Best model at iteration: 21727 | smooth loss: 1.80337\n",
      "Best model at iteration: 21728 | smooth loss: 1.80324\n",
      "Best model at iteration: 21742 | smooth loss: 1.80299\n",
      "Best model at iteration: 21744 | smooth loss: 1.80282\n",
      "Best model at iteration: 21745 | smooth loss: 1.80263\n",
      "Best model at iteration: 21746 | smooth loss: 1.80238\n",
      "Best model at iteration: 21747 | smooth loss: 1.80198\n",
      "Best model at iteration: 21748 | smooth loss: 1.80170\n",
      "Best model at iteration: 21749 | smooth loss: 1.80157\n",
      "Best model at iteration: 21750 | smooth loss: 1.80094\n",
      "Best model at iteration: 21752 | smooth loss: 1.80072\n",
      "Best model at iteration: 21753 | smooth loss: 1.80051\n",
      "Best model at iteration: 21754 | smooth loss: 1.80048\n",
      "Best model at iteration: 21755 | smooth loss: 1.80014\n",
      "Best model at iteration: 21758 | smooth loss: 1.79994\n",
      "Best model at iteration: 21759 | smooth loss: 1.79982\n",
      "Best model at iteration: 21760 | smooth loss: 1.79976\n",
      "Best model at iteration: 21777 | smooth loss: 1.79974\n",
      "Best model at iteration: 21778 | smooth loss: 1.79956\n",
      "Best model at iteration: 21779 | smooth loss: 1.79936\n",
      "Best model at iteration: 21791 | smooth loss: 1.79907\n",
      "Best model at iteration: 21792 | smooth loss: 1.79876\n",
      "Best model at iteration: 21794 | smooth loss: 1.79850\n",
      "Best model at iteration: 21795 | smooth loss: 1.79829\n",
      "Best model at iteration: 21796 | smooth loss: 1.79811\n",
      "Best model at iteration: 21797 | smooth loss: 1.79777\n",
      "Best model at iteration: 21798 | smooth loss: 1.79740\n",
      "Best model at iteration: 21824 | smooth loss: 1.79715\n",
      "Best model at iteration: 21825 | smooth loss: 1.79682\n",
      "Best model at iteration: 21827 | smooth loss: 1.79680\n",
      "Best model at iteration: 21828 | smooth loss: 1.79640\n",
      "Best model at iteration: 21829 | smooth loss: 1.79620\n",
      "iter = 22000, smooth loss=1.8006603322231578\n",
      "t in ow kerthan the ward aud soe theg, fotet.\n",
      "Tourd tulloustac, he dadge to have bech inse the wiseed shees sor\n",
      "He bild yourntine ey.  He boowong if, too down whicG?\"  and MrGoming acoughn'vir, \"Lough\n",
      "\n",
      "iter = 23000, smooth loss=1.799295828221999\n",
      "see ghaning draic Dombys al ssartiod off, bet in reviped.\n",
      "\"Whink look he hear very be armecter leo ever wey siup intitior the plin them that he eme fren he deam the beand!  Dibby wiren it, thoik on th\n",
      "\n",
      "Best model at iteration: 23026 | smooth loss: 1.79615\n",
      "Best model at iteration: 23027 | smooth loss: 1.79611\n",
      "Best model at iteration: 23028 | smooth loss: 1.79609\n",
      "Best model at iteration: 23035 | smooth loss: 1.79595\n",
      "Best model at iteration: 23037 | smooth loss: 1.79557\n",
      "Best model at iteration: 23038 | smooth loss: 1.79546\n",
      "Best model at iteration: 23042 | smooth loss: 1.79531\n",
      "Best model at iteration: 23045 | smooth loss: 1.79526\n",
      "Best model at iteration: 23047 | smooth loss: 1.79513\n",
      "Best model at iteration: 23048 | smooth loss: 1.79498\n",
      "Best model at iteration: 23049 | smooth loss: 1.79464\n",
      "Best model at iteration: 23050 | smooth loss: 1.79438\n",
      "Best model at iteration: 23077 | smooth loss: 1.79434\n",
      "Best model at iteration: 23081 | smooth loss: 1.79393\n",
      "Best model at iteration: 23082 | smooth loss: 1.79362\n",
      "Best model at iteration: 23083 | smooth loss: 1.79305\n",
      "Best model at iteration: 23084 | smooth loss: 1.79290\n",
      "Best model at iteration: 23085 | smooth loss: 1.79265\n",
      "Best model at iteration: 23086 | smooth loss: 1.79260\n",
      "Best model at iteration: 23090 | smooth loss: 1.79250\n",
      "Best model at iteration: 23094 | smooth loss: 1.79241\n",
      "Best model at iteration: 23095 | smooth loss: 1.79211\n",
      "Best model at iteration: 23096 | smooth loss: 1.79189\n",
      "Best model at iteration: 23097 | smooth loss: 1.79102\n",
      "Best model at iteration: 23098 | smooth loss: 1.79056\n",
      "Best model at iteration: 23099 | smooth loss: 1.79043\n",
      "Best model at iteration: 23100 | smooth loss: 1.79019\n",
      "Best model at iteration: 23101 | smooth loss: 1.79007\n",
      "Best model at iteration: 23106 | smooth loss: 1.79005\n",
      "Best model at iteration: 23107 | smooth loss: 1.78941\n",
      "Best model at iteration: 23108 | smooth loss: 1.78922\n",
      "Best model at iteration: 23109 | smooth loss: 1.78819\n",
      "Best model at iteration: 23122 | smooth loss: 1.78805\n",
      "Best model at iteration: 23123 | smooth loss: 1.78804\n",
      "Best model at iteration: 23124 | smooth loss: 1.78786\n",
      "Best model at iteration: 23126 | smooth loss: 1.78768\n",
      "Best model at iteration: 23130 | smooth loss: 1.78749\n",
      "Best model at iteration: 23135 | smooth loss: 1.78735\n",
      "Best model at iteration: 23136 | smooth loss: 1.78727\n",
      "Best model at iteration: 23139 | smooth loss: 1.78687\n",
      "iter = 24000, smooth loss=1.8119973936735587\n",
      "rest sord wank to pans of clork then goth agoon ham go - the sang malping onarmalom syout, )omete wall,\" said Harry to gind hir Can harr it Harry ol - ske\"  Ono that make even Horw, in ullubick aid.  \n",
      "\n",
      "iter = 25000, smooth loss=1.82302526359947\n",
      "il over tobed of conered iny urup wneviored and bes elfiwnseriss PEjacu?\" she Harry and them of squearly befaring finsed hhrouch saud. Thenchis; cearmedming were, whon welloweshere fust bown, Permione\n",
      "\n",
      "iter = 26000, smooth loss=1.8114628170209095\n",
      "ine wal? . . . . Harmy the just coor yot the Win.  \"Do, what, he cousr.  shind said foog.\n",
      "\"She bauls.  Bouping whatiof.  Chasl hangiog to sig to arattan, he wast if bue he's make to me one.  Harry con\n",
      "\n",
      "iter = 27000, smooth loss=1.7997757925360918\n",
      "had row.\"\n",
      "He was bact a with Harry, pur wo head istor,\" siup Her'y were you year, glonked timist pit me.  Be deich, the ofly dan't domblan, and Gearens, pit looked par drom arrins, sied trudge rowsin,\n",
      "\n",
      "Best model at iteration: 27291 | smooth loss: 1.78659\n",
      "Best model at iteration: 27292 | smooth loss: 1.78594\n",
      "Best model at iteration: 27293 | smooth loss: 1.78579\n",
      "Best model at iteration: 27295 | smooth loss: 1.78570\n",
      "Best model at iteration: 27296 | smooth loss: 1.78520\n",
      "Best model at iteration: 27299 | smooth loss: 1.78470\n",
      "Best model at iteration: 27302 | smooth loss: 1.78446\n",
      "Best model at iteration: 27313 | smooth loss: 1.78375\n",
      "Best model at iteration: 27314 | smooth loss: 1.78361\n",
      "Best model at iteration: 27315 | smooth loss: 1.78333\n",
      "Best model at iteration: 27316 | smooth loss: 1.78308\n",
      "iter = 28000, smooth loss=1.812503573378199\n",
      "erled firee . . a don't feet toiseveding of fore.  Myrclly?\"\n",
      "My the satessed apyomes....\"\n",
      "\"Beakid tordoed for off then Prot!\" shive you guin out the vurfom; in had got a h, Rnoulfs!  Thick of eny.  He\n",
      "\n",
      "Best model at iteration: 28683 | smooth loss: 1.78293\n",
      "Best model at iteration: 28684 | smooth loss: 1.78224\n",
      "Best model at iteration: 28685 | smooth loss: 1.78198\n",
      "Best model at iteration: 28686 | smooth loss: 1.78154\n",
      "Best model at iteration: 28687 | smooth loss: 1.78148\n",
      "Best model at iteration: 28707 | smooth loss: 1.78139\n",
      "Best model at iteration: 28727 | smooth loss: 1.78092\n",
      "Best model at iteration: 28731 | smooth loss: 1.78021\n",
      "Best model at iteration: 28732 | smooth loss: 1.77995\n",
      "Best model at iteration: 28733 | smooth loss: 1.77972\n",
      "Best model at iteration: 28748 | smooth loss: 1.77932\n",
      "Best model at iteration: 28750 | smooth loss: 1.77910\n",
      "Best model at iteration: 28751 | smooth loss: 1.77861\n",
      "Best model at iteration: 28760 | smooth loss: 1.77845\n",
      "Best model at iteration: 28761 | smooth loss: 1.77821\n",
      "Best model at iteration: 28762 | smooth loss: 1.77794\n",
      "Best model at iteration: 28763 | smooth loss: 1.77732\n",
      "Best model at iteration: 28765 | smooth loss: 1.77706\n",
      "Best model at iteration: 28766 | smooth loss: 1.77663\n",
      "Best model at iteration: 28767 | smooth loss: 1.77645\n",
      "Best model at iteration: 28768 | smooth loss: 1.77586\n",
      "Best model at iteration: 28769 | smooth loss: 1.77553\n",
      "Best model at iteration: 28770 | smooth loss: 1.77485\n",
      "Best model at iteration: 28772 | smooth loss: 1.77464\n",
      "Best model at iteration: 28774 | smooth loss: 1.77430\n",
      "Best model at iteration: 28775 | smooth loss: 1.77389\n",
      "Best model at iteration: 28776 | smooth loss: 1.77371\n",
      "Best model at iteration: 28784 | smooth loss: 1.77343\n",
      "Best model at iteration: 28789 | smooth loss: 1.77320\n",
      "Best model at iteration: 28790 | smooth loss: 1.77274\n",
      "Best model at iteration: 28791 | smooth loss: 1.77230\n",
      "Best model at iteration: 28793 | smooth loss: 1.77216\n",
      "Best model at iteration: 28794 | smooth loss: 1.77192\n",
      "Best model at iteration: 28796 | smooth loss: 1.77147\n",
      "Best model at iteration: 28797 | smooth loss: 1.77116\n",
      "Best model at iteration: 28798 | smooth loss: 1.77108\n",
      "Best model at iteration: 28800 | smooth loss: 1.77101\n",
      "Best model at iteration: 28801 | smooth loss: 1.77045\n",
      "Best model at iteration: 28802 | smooth loss: 1.77040\n",
      "Best model at iteration: 28803 | smooth loss: 1.76998\n",
      "Best model at iteration: 28804 | smooth loss: 1.76960\n",
      "Best model at iteration: 28805 | smooth loss: 1.76933\n",
      "Best model at iteration: 28931 | smooth loss: 1.76907\n",
      "Best model at iteration: 28932 | smooth loss: 1.76878\n",
      "Best model at iteration: 28933 | smooth loss: 1.76862\n",
      "Best model at iteration: 28934 | smooth loss: 1.76847\n",
      "Best model at iteration: 28935 | smooth loss: 1.76786\n",
      "Best model at iteration: 28943 | smooth loss: 1.76783\n",
      "Best model at iteration: 28944 | smooth loss: 1.76781\n",
      "Best model at iteration: 28945 | smooth loss: 1.76744\n",
      "Best model at iteration: 28946 | smooth loss: 1.76730\n",
      "Best model at iteration: 28952 | smooth loss: 1.76726\n",
      "Best model at iteration: 28953 | smooth loss: 1.76711\n",
      "Best model at iteration: 28954 | smooth loss: 1.76705\n",
      "Best model at iteration: 28955 | smooth loss: 1.76670\n",
      "Best model at iteration: 28956 | smooth loss: 1.76660\n",
      "Best model at iteration: 28957 | smooth loss: 1.76611\n",
      "Best model at iteration: 28958 | smooth loss: 1.76571\n",
      "Best model at iteration: 28959 | smooth loss: 1.76473\n",
      "Best model at iteration: 28961 | smooth loss: 1.76466\n",
      "Best model at iteration: 28962 | smooth loss: 1.76421\n",
      "Best model at iteration: 28963 | smooth loss: 1.76400\n",
      "Best model at iteration: 28964 | smooth loss: 1.76391\n",
      "Best model at iteration: 28965 | smooth loss: 1.76330\n",
      "Best model at iteration: 28966 | smooth loss: 1.76312\n",
      "Best model at iteration: 28977 | smooth loss: 1.76245\n",
      "Best model at iteration: 28978 | smooth loss: 1.76242\n",
      "Best model at iteration: 28979 | smooth loss: 1.76192\n",
      "iter = 29000, smooth loss=1.7670601320842423\n",
      "p at Harry well bare muthting.\n",
      "\"Crof Durbly asso'd \"I pueffice, searthing MarsuE, was . . . .. The'k Poto the malle tree smoming. \"I've id for a fellht fyweroves of weak loum Holly what Shished to thu\n",
      "\n",
      "iter = 30000, smooth loss=1.7799236496001978\n",
      "uaning whime thrunge stooked the antosslase scome mutwiovel weared the toge..\n",
      "Ho the peecouts to darnfacelisse are as wanded that \"e snow herared stelloly.  Harry and seat we eldem with there frearing\n",
      "\n",
      "Best model at iteration: 30280 | smooth loss: 1.76166\n",
      "Best model at iteration: 30281 | smooth loss: 1.76156\n",
      "Best model at iteration: 30292 | smooth loss: 1.76135\n",
      "Best model at iteration: 30293 | smooth loss: 1.76114\n",
      "Best model at iteration: 30294 | smooth loss: 1.76105\n",
      "Best model at iteration: 30295 | smooth loss: 1.76054\n",
      "Best model at iteration: 30301 | smooth loss: 1.76048\n",
      "Best model at iteration: 30302 | smooth loss: 1.76002\n",
      "Best model at iteration: 30304 | smooth loss: 1.75952\n",
      "Best model at iteration: 30305 | smooth loss: 1.75930\n",
      "Best model at iteration: 30306 | smooth loss: 1.75908\n",
      "Best model at iteration: 30307 | smooth loss: 1.75878\n",
      "Best model at iteration: 30309 | smooth loss: 1.75848\n",
      "Best model at iteration: 30310 | smooth loss: 1.75812\n",
      "Best model at iteration: 30311 | smooth loss: 1.75799\n",
      "Best model at iteration: 30312 | smooth loss: 1.75770\n",
      "Best model at iteration: 30313 | smooth loss: 1.75764\n",
      "Best model at iteration: 30325 | smooth loss: 1.75759\n",
      "Best model at iteration: 30327 | smooth loss: 1.75718\n",
      "Best model at iteration: 30328 | smooth loss: 1.75689\n",
      "Best model at iteration: 30329 | smooth loss: 1.75679\n",
      "Best model at iteration: 30331 | smooth loss: 1.75639\n",
      "Best model at iteration: 30332 | smooth loss: 1.75569\n",
      "Best model at iteration: 30333 | smooth loss: 1.75521\n",
      "Best model at iteration: 30334 | smooth loss: 1.75487\n",
      "Best model at iteration: 30337 | smooth loss: 1.75467\n",
      "Best model at iteration: 30338 | smooth loss: 1.75431\n",
      "Best model at iteration: 30339 | smooth loss: 1.75394\n",
      "Best model at iteration: 30340 | smooth loss: 1.75354\n",
      "Best model at iteration: 30341 | smooth loss: 1.75305\n",
      "Best model at iteration: 30342 | smooth loss: 1.75280\n",
      "Best model at iteration: 30344 | smooth loss: 1.75252\n",
      "Best model at iteration: 30346 | smooth loss: 1.75246\n",
      "Best model at iteration: 30347 | smooth loss: 1.75213\n",
      "Best model at iteration: 30348 | smooth loss: 1.75199\n",
      "Best model at iteration: 30349 | smooth loss: 1.75186\n",
      "Best model at iteration: 30480 | smooth loss: 1.75160\n",
      "Best model at iteration: 30481 | smooth loss: 1.75147\n",
      "Best model at iteration: 30484 | smooth loss: 1.75088\n",
      "Best model at iteration: 30485 | smooth loss: 1.75050\n",
      "iter = 31000, smooth loss=1.7822782139002629\n",
      "rof sturned and was to h he could and the beaks noterrid, poanst ham, \"Ahawh, chask.  restle she Ron'oly.  They swhond  beet.\"\n",
      "\"Flaytall his laigeve that Ron. \"I onatured\" one up itarch at (hoursu,\" S\n",
      "\n",
      "Best model at iteration: 31902 | smooth loss: 1.75014\n",
      "Best model at iteration: 31903 | smooth loss: 1.75008\n",
      "Best model at iteration: 31904 | smooth loss: 1.74951\n",
      "Best model at iteration: 31907 | smooth loss: 1.74938\n",
      "Best model at iteration: 31908 | smooth loss: 1.74911\n",
      "Best model at iteration: 31909 | smooth loss: 1.74911\n",
      "Best model at iteration: 31910 | smooth loss: 1.74889\n",
      "Best model at iteration: 31911 | smooth loss: 1.74858\n",
      "Best model at iteration: 31939 | smooth loss: 1.74854\n",
      "iter = 32000, smooth loss=1.7516572126402952\n",
      "\" hat secobure but head with Mary at Snabus, patized protch to hus bectol nack soudevatel on Sirey.\"\n",
      "\"Gelch thing hitd rubbred he send whbed had the enereds wize to Dung fwen his allurt neach of Chori\n",
      "\n",
      "Best model at iteration: 32028 | smooth loss: 1.74774\n",
      "Best model at iteration: 32029 | smooth loss: 1.74744\n",
      "Best model at iteration: 32031 | smooth loss: 1.74727\n",
      "Best model at iteration: 32032 | smooth loss: 1.74725\n",
      "Best model at iteration: 32033 | smooth loss: 1.74697\n",
      "Best model at iteration: 32085 | smooth loss: 1.74646\n",
      "Best model at iteration: 32086 | smooth loss: 1.74604\n",
      "Best model at iteration: 32096 | smooth loss: 1.74585\n",
      "Best model at iteration: 32097 | smooth loss: 1.74570\n",
      "Best model at iteration: 32099 | smooth loss: 1.74568\n",
      "Best model at iteration: 32100 | smooth loss: 1.74521\n",
      "Best model at iteration: 32101 | smooth loss: 1.74443\n",
      "Best model at iteration: 32103 | smooth loss: 1.74406\n",
      "Best model at iteration: 32104 | smooth loss: 1.74405\n",
      "Best model at iteration: 32105 | smooth loss: 1.74375\n",
      "Best model at iteration: 32106 | smooth loss: 1.74338\n",
      "Best model at iteration: 32108 | smooth loss: 1.74307\n",
      "Best model at iteration: 32109 | smooth loss: 1.74277\n",
      "Best model at iteration: 32111 | smooth loss: 1.74197\n",
      "Best model at iteration: 32112 | smooth loss: 1.74174\n",
      "Best model at iteration: 32124 | smooth loss: 1.74166\n",
      "Best model at iteration: 32129 | smooth loss: 1.74146\n",
      "Best model at iteration: 32130 | smooth loss: 1.74084\n",
      "Best model at iteration: 32131 | smooth loss: 1.74076\n",
      "Best model at iteration: 32136 | smooth loss: 1.74076\n",
      "Best model at iteration: 32138 | smooth loss: 1.74060\n",
      "Best model at iteration: 32139 | smooth loss: 1.74058\n",
      "Best model at iteration: 32140 | smooth loss: 1.74042\n",
      "Best model at iteration: 32142 | smooth loss: 1.74010\n",
      "Best model at iteration: 32143 | smooth loss: 1.74000\n",
      "Best model at iteration: 32145 | smooth loss: 1.73959\n",
      "Best model at iteration: 32147 | smooth loss: 1.73950\n",
      "Best model at iteration: 32148 | smooth loss: 1.73944\n",
      "Best model at iteration: 32149 | smooth loss: 1.73928\n",
      "Best model at iteration: 32171 | smooth loss: 1.73897\n",
      "Best model at iteration: 32172 | smooth loss: 1.73875\n",
      "Best model at iteration: 32205 | smooth loss: 1.73854\n",
      "Best model at iteration: 32206 | smooth loss: 1.73838\n",
      "Best model at iteration: 32211 | smooth loss: 1.73807\n",
      "Best model at iteration: 32212 | smooth loss: 1.73805\n",
      "Best model at iteration: 32267 | smooth loss: 1.73805\n",
      "Best model at iteration: 32268 | smooth loss: 1.73767\n",
      "Best model at iteration: 32271 | smooth loss: 1.73728\n",
      "Best model at iteration: 32272 | smooth loss: 1.73698\n",
      "Best model at iteration: 32273 | smooth loss: 1.73677\n",
      "Best model at iteration: 32279 | smooth loss: 1.73669\n",
      "Best model at iteration: 32280 | smooth loss: 1.73661\n",
      "Best model at iteration: 32341 | smooth loss: 1.73606\n",
      "Best model at iteration: 32344 | smooth loss: 1.73580\n",
      "Best model at iteration: 32345 | smooth loss: 1.73561\n",
      "Best model at iteration: 32375 | smooth loss: 1.73537\n",
      "Best model at iteration: 32377 | smooth loss: 1.73505\n",
      "Best model at iteration: 32379 | smooth loss: 1.73451\n",
      "Best model at iteration: 32381 | smooth loss: 1.73437\n",
      "Best model at iteration: 32382 | smooth loss: 1.73432\n",
      "Best model at iteration: 32383 | smooth loss: 1.73376\n",
      "Best model at iteration: 32385 | smooth loss: 1.73375\n",
      "Best model at iteration: 32386 | smooth loss: 1.73356\n",
      "Best model at iteration: 32434 | smooth loss: 1.73341\n",
      "Best model at iteration: 32435 | smooth loss: 1.73293\n",
      "Best model at iteration: 32436 | smooth loss: 1.73262\n",
      "Best model at iteration: 32437 | smooth loss: 1.73212\n",
      "Best model at iteration: 32438 | smooth loss: 1.73181\n",
      "Best model at iteration: 32442 | smooth loss: 1.73148\n",
      "Best model at iteration: 32443 | smooth loss: 1.73136\n",
      "Best model at iteration: 32444 | smooth loss: 1.73129\n",
      "Best model at iteration: 32445 | smooth loss: 1.73086\n",
      "Best model at iteration: 32446 | smooth loss: 1.73080\n",
      "Best model at iteration: 32509 | smooth loss: 1.73069\n",
      "Best model at iteration: 32510 | smooth loss: 1.73049\n",
      "Best model at iteration: 32514 | smooth loss: 1.73003\n",
      "Best model at iteration: 32516 | smooth loss: 1.72968\n",
      "Best model at iteration: 32517 | smooth loss: 1.72952\n",
      "Best model at iteration: 32521 | smooth loss: 1.72904\n",
      "Best model at iteration: 32522 | smooth loss: 1.72872\n",
      "Best model at iteration: 32528 | smooth loss: 1.72850\n",
      "Best model at iteration: 32530 | smooth loss: 1.72810\n",
      "Best model at iteration: 32531 | smooth loss: 1.72785\n",
      "Best model at iteration: 32536 | smooth loss: 1.72763\n",
      "Best model at iteration: 32538 | smooth loss: 1.72748\n",
      "Best model at iteration: 32539 | smooth loss: 1.72729\n",
      "iter = 33000, smooth loss=1.747801264900099\n",
      " sidd.  \"I hers phomeding solls.\" said Hermawining the stelf.   Ozlry neskous deablefes,\" soid Hermione is for Herbondigg the plasemperipplalee stimli..  It was itowes've anyo Magh ail open woth and t\n",
      "\n",
      "Best model at iteration: 33649 | smooth loss: 1.72718\n",
      "Best model at iteration: 33652 | smooth loss: 1.72697\n",
      "Best model at iteration: 33653 | smooth loss: 1.72648\n",
      "Best model at iteration: 33654 | smooth loss: 1.72628\n",
      "Best model at iteration: 33662 | smooth loss: 1.72591\n",
      "Best model at iteration: 33665 | smooth loss: 1.72589\n",
      "Best model at iteration: 33667 | smooth loss: 1.72554\n",
      "Best model at iteration: 33669 | smooth loss: 1.72506\n",
      "Best model at iteration: 33670 | smooth loss: 1.72501\n",
      "Best model at iteration: 33671 | smooth loss: 1.72495\n",
      "Best model at iteration: 33673 | smooth loss: 1.72423\n",
      "Best model at iteration: 33677 | smooth loss: 1.72422\n",
      "Best model at iteration: 33678 | smooth loss: 1.72415\n",
      "Best model at iteration: 33679 | smooth loss: 1.72347\n",
      "Best model at iteration: 33682 | smooth loss: 1.72296\n",
      "Best model at iteration: 33683 | smooth loss: 1.72231\n",
      "Best model at iteration: 33684 | smooth loss: 1.72211\n",
      "Best model at iteration: 33685 | smooth loss: 1.72164\n",
      "Best model at iteration: 33691 | smooth loss: 1.72152\n",
      "Best model at iteration: 33692 | smooth loss: 1.72096\n",
      "Best model at iteration: 33695 | smooth loss: 1.72087\n",
      "Best model at iteration: 33696 | smooth loss: 1.71994\n",
      "Best model at iteration: 33697 | smooth loss: 1.71975\n",
      "Best model at iteration: 33698 | smooth loss: 1.71941\n",
      "Best model at iteration: 33700 | smooth loss: 1.71915\n",
      "Best model at iteration: 33701 | smooth loss: 1.71879\n",
      "Best model at iteration: 33702 | smooth loss: 1.71852\n",
      "Best model at iteration: 33707 | smooth loss: 1.71841\n",
      "Best model at iteration: 33709 | smooth loss: 1.71814\n",
      "Best model at iteration: 33713 | smooth loss: 1.71797\n",
      "Best model at iteration: 33714 | smooth loss: 1.71764\n",
      "Best model at iteration: 33717 | smooth loss: 1.71753\n",
      "Best model at iteration: 33718 | smooth loss: 1.71745\n",
      "Best model at iteration: 33719 | smooth loss: 1.71739\n",
      "Best model at iteration: 33721 | smooth loss: 1.71665\n",
      "Best model at iteration: 33722 | smooth loss: 1.71650\n",
      "Best model at iteration: 33725 | smooth loss: 1.71636\n",
      "Best model at iteration: 33727 | smooth loss: 1.71606\n",
      "Best model at iteration: 33728 | smooth loss: 1.71572\n",
      "Best model at iteration: 33729 | smooth loss: 1.71532\n",
      "Best model at iteration: 33731 | smooth loss: 1.71522\n",
      "Best model at iteration: 33732 | smooth loss: 1.71485\n",
      "Best model at iteration: 33733 | smooth loss: 1.71430\n",
      "Best model at iteration: 33734 | smooth loss: 1.71385\n",
      "Best model at iteration: 33735 | smooth loss: 1.71305\n",
      "Best model at iteration: 33736 | smooth loss: 1.71305\n",
      "Best model at iteration: 33739 | smooth loss: 1.71276\n",
      "Best model at iteration: 33740 | smooth loss: 1.71200\n",
      "Best model at iteration: 33741 | smooth loss: 1.71161\n",
      "Best model at iteration: 33742 | smooth loss: 1.71145\n",
      "Best model at iteration: 33744 | smooth loss: 1.71119\n",
      "Best model at iteration: 33745 | smooth loss: 1.71088\n",
      "Best model at iteration: 33747 | smooth loss: 1.71066\n",
      "Best model at iteration: 33748 | smooth loss: 1.71060\n",
      "Best model at iteration: 33750 | smooth loss: 1.71029\n",
      "Best model at iteration: 33751 | smooth loss: 1.70938\n",
      "Best model at iteration: 33752 | smooth loss: 1.70935\n",
      "Best model at iteration: 33753 | smooth loss: 1.70902\n",
      "Best model at iteration: 33754 | smooth loss: 1.70849\n",
      "Best model at iteration: 33755 | smooth loss: 1.70823\n",
      "Best model at iteration: 33782 | smooth loss: 1.70818\n",
      "Best model at iteration: 33787 | smooth loss: 1.70808\n",
      "Best model at iteration: 33788 | smooth loss: 1.70766\n",
      "Best model at iteration: 33792 | smooth loss: 1.70725\n",
      "Best model at iteration: 33793 | smooth loss: 1.70681\n",
      "Best model at iteration: 33794 | smooth loss: 1.70649\n",
      "Best model at iteration: 33797 | smooth loss: 1.70629\n",
      "Best model at iteration: 33801 | smooth loss: 1.70624\n",
      "Best model at iteration: 33803 | smooth loss: 1.70599\n",
      "Best model at iteration: 33804 | smooth loss: 1.70547\n",
      "Best model at iteration: 33810 | smooth loss: 1.70491\n",
      "Best model at iteration: 33811 | smooth loss: 1.70474\n",
      "Best model at iteration: 33815 | smooth loss: 1.70389\n",
      "Best model at iteration: 33816 | smooth loss: 1.70380\n",
      "Best model at iteration: 33817 | smooth loss: 1.70338\n",
      "Best model at iteration: 33823 | smooth loss: 1.70307\n",
      "Best model at iteration: 33828 | smooth loss: 1.70294\n",
      "Best model at iteration: 33829 | smooth loss: 1.70225\n",
      "Best model at iteration: 33831 | smooth loss: 1.70141\n",
      "Best model at iteration: 33834 | smooth loss: 1.70091\n",
      "Best model at iteration: 33835 | smooth loss: 1.70078\n",
      "Best model at iteration: 33836 | smooth loss: 1.70052\n",
      "Best model at iteration: 33837 | smooth loss: 1.70038\n",
      "Best model at iteration: 33838 | smooth loss: 1.70008\n",
      "Best model at iteration: 33839 | smooth loss: 1.69967\n",
      "Best model at iteration: 33844 | smooth loss: 1.69960\n",
      "Best model at iteration: 33845 | smooth loss: 1.69946\n",
      "Best model at iteration: 33851 | smooth loss: 1.69920\n",
      "Best model at iteration: 33853 | smooth loss: 1.69905\n",
      "Best model at iteration: 33856 | smooth loss: 1.69839\n",
      "Best model at iteration: 33857 | smooth loss: 1.69773\n",
      "Best model at iteration: 33858 | smooth loss: 1.69744\n",
      "Best model at iteration: 33859 | smooth loss: 1.69732\n",
      "Best model at iteration: 33860 | smooth loss: 1.69718\n",
      "Best model at iteration: 33861 | smooth loss: 1.69653\n",
      "Best model at iteration: 33863 | smooth loss: 1.69625\n",
      "Best model at iteration: 33864 | smooth loss: 1.69588\n",
      "Best model at iteration: 33867 | smooth loss: 1.69575\n",
      "Best model at iteration: 33868 | smooth loss: 1.69567\n",
      "Best model at iteration: 33869 | smooth loss: 1.69538\n",
      "Best model at iteration: 33871 | smooth loss: 1.69490\n",
      "iter = 34000, smooth loss=1.701028799465836\n",
      "aone uptryyoo mutt tarin, an her it to to hee raumbaber, and quize foom, pout talking his throak Dumbledore!\"  snim.  Harry herewned in weramind anow. . sair an.  \"Fle, an farmanu tarrin' anp he weter\n",
      "\n",
      "Best model at iteration: 34068 | smooth loss: 1.69483\n",
      "Best model at iteration: 34069 | smooth loss: 1.69483\n",
      "Best model at iteration: 34070 | smooth loss: 1.69431\n",
      "Best model at iteration: 34071 | smooth loss: 1.69407\n",
      "Best model at iteration: 34072 | smooth loss: 1.69343\n",
      "Best model at iteration: 34073 | smooth loss: 1.69311\n",
      "Best model at iteration: 34074 | smooth loss: 1.69289\n",
      "Best model at iteration: 34076 | smooth loss: 1.69261\n",
      "Best model at iteration: 34077 | smooth loss: 1.69260\n",
      "Best model at iteration: 34079 | smooth loss: 1.69212\n",
      "Best model at iteration: 34092 | smooth loss: 1.69201\n",
      "Best model at iteration: 34093 | smooth loss: 1.69134\n",
      "Best model at iteration: 34094 | smooth loss: 1.69098\n",
      "Best model at iteration: 34095 | smooth loss: 1.69067\n",
      "Best model at iteration: 34098 | smooth loss: 1.69017\n",
      "Best model at iteration: 34099 | smooth loss: 1.68955\n",
      "Best model at iteration: 34100 | smooth loss: 1.68903\n",
      "Best model at iteration: 34102 | smooth loss: 1.68829\n",
      "Best model at iteration: 34103 | smooth loss: 1.68812\n",
      "Best model at iteration: 34104 | smooth loss: 1.68795\n",
      "Best model at iteration: 34133 | smooth loss: 1.68794\n",
      "Best model at iteration: 34134 | smooth loss: 1.68726\n",
      "Best model at iteration: 34135 | smooth loss: 1.68705\n",
      "Best model at iteration: 34136 | smooth loss: 1.68667\n",
      "Best model at iteration: 34147 | smooth loss: 1.68651\n",
      "Best model at iteration: 34214 | smooth loss: 1.68628\n",
      "Best model at iteration: 34227 | smooth loss: 1.68623\n",
      "Best model at iteration: 34228 | smooth loss: 1.68612\n",
      "Best model at iteration: 34231 | smooth loss: 1.68542\n",
      "Best model at iteration: 34232 | smooth loss: 1.68515\n",
      "Best model at iteration: 34233 | smooth loss: 1.68489\n",
      "Best model at iteration: 34234 | smooth loss: 1.68425\n",
      "Best model at iteration: 34235 | smooth loss: 1.68402\n",
      "Best model at iteration: 34236 | smooth loss: 1.68395\n",
      "Best model at iteration: 34237 | smooth loss: 1.68348\n",
      "Best model at iteration: 34238 | smooth loss: 1.68299\n",
      "Best model at iteration: 34239 | smooth loss: 1.68286\n",
      "Best model at iteration: 34241 | smooth loss: 1.68274\n",
      "Best model at iteration: 34242 | smooth loss: 1.68265\n",
      "Best model at iteration: 34243 | smooth loss: 1.68227\n",
      "Best model at iteration: 34244 | smooth loss: 1.68172\n",
      "Best model at iteration: 34245 | smooth loss: 1.68128\n",
      "Best model at iteration: 34247 | smooth loss: 1.68098\n",
      "Best model at iteration: 34250 | smooth loss: 1.68090\n",
      "Best model at iteration: 34251 | smooth loss: 1.68046\n",
      "Best model at iteration: 34252 | smooth loss: 1.68028\n",
      "Best model at iteration: 34253 | smooth loss: 1.68022\n",
      "Best model at iteration: 34257 | smooth loss: 1.67999\n",
      "Best model at iteration: 34258 | smooth loss: 1.67966\n",
      "Best model at iteration: 34263 | smooth loss: 1.67951\n",
      "Best model at iteration: 34264 | smooth loss: 1.67927\n",
      "Best model at iteration: 34265 | smooth loss: 1.67926\n",
      "Best model at iteration: 34267 | smooth loss: 1.67923\n",
      "Best model at iteration: 34268 | smooth loss: 1.67865\n",
      "Best model at iteration: 34269 | smooth loss: 1.67862\n",
      "Best model at iteration: 34270 | smooth loss: 1.67838\n",
      "Best model at iteration: 34271 | smooth loss: 1.67819\n",
      "Best model at iteration: 34273 | smooth loss: 1.67731\n",
      "Best model at iteration: 34275 | smooth loss: 1.67712\n",
      "Best model at iteration: 34278 | smooth loss: 1.67682\n",
      "Best model at iteration: 34279 | smooth loss: 1.67659\n",
      "Best model at iteration: 34281 | smooth loss: 1.67646\n",
      "Best model at iteration: 34290 | smooth loss: 1.67639\n",
      "Best model at iteration: 34298 | smooth loss: 1.67603\n",
      "Best model at iteration: 34393 | smooth loss: 1.67568\n",
      "Best model at iteration: 34394 | smooth loss: 1.67558\n",
      "Best model at iteration: 34395 | smooth loss: 1.67550\n",
      "Best model at iteration: 34396 | smooth loss: 1.67550\n",
      "Best model at iteration: 34421 | smooth loss: 1.67516\n",
      "Best model at iteration: 34422 | smooth loss: 1.67468\n",
      "Best model at iteration: 34427 | smooth loss: 1.67455\n",
      "Best model at iteration: 34428 | smooth loss: 1.67392\n",
      "Best model at iteration: 34429 | smooth loss: 1.67378\n",
      "Best model at iteration: 34431 | smooth loss: 1.67349\n",
      "Best model at iteration: 34432 | smooth loss: 1.67325\n",
      "Best model at iteration: 34570 | smooth loss: 1.67312\n",
      "Best model at iteration: 34571 | smooth loss: 1.67270\n",
      "Best model at iteration: 34572 | smooth loss: 1.67259\n",
      "Best model at iteration: 34573 | smooth loss: 1.67229\n",
      "Best model at iteration: 34576 | smooth loss: 1.67224\n",
      "Best model at iteration: 34577 | smooth loss: 1.67219\n",
      "Best model at iteration: 34578 | smooth loss: 1.67208\n",
      "Best model at iteration: 34579 | smooth loss: 1.67208\n",
      "Best model at iteration: 34580 | smooth loss: 1.67173\n",
      "iter = 35000, smooth loss=1.7126422646590702\n",
      "d!\"  said asters, stureamen out stoy steplying cout nextered, buch done, lould saind, all Baxadyent the siret asvessy downe juster ext ann teiling her copleave .. .\"\n",
      "\"I daden the porg. . . .\n",
      "\n",
      "Not Crck\n",
      "\n",
      "iter = 36000, smooth loss=1.7046215393179083\n",
      "n about bulked down formul, and mane snep.  Mony cogeden, theved to a now!\"\n",
      " he woid it for the bay, formenfly.  I chaired steasn't mined and evory arm wereve it!\"\n",
      "\"Your nust nexinged agrings; the too\n",
      "\n",
      "Best model at iteration: 36535 | smooth loss: 1.67164\n",
      "Best model at iteration: 36537 | smooth loss: 1.67086\n",
      "Best model at iteration: 36538 | smooth loss: 1.67085\n",
      "Best model at iteration: 36539 | smooth loss: 1.67069\n",
      "Best model at iteration: 36542 | smooth loss: 1.67053\n",
      "Best model at iteration: 36543 | smooth loss: 1.67009\n",
      "Best model at iteration: 36544 | smooth loss: 1.66969\n",
      "iter = 37000, smooth loss=1.7143131095905366\n",
      "e, say tag sthered kewn chimpenitaty what you be heyer thout laug the fryt of the enst in the Dumbledore a olfod Hermuond a Krus?  Sow she way were - your That thes very what berouse, from matemle, he\n",
      "\n",
      "iter = 38000, smooth loss=1.7123343717657311\n",
      "as at Hermy the end Rous, hadry.\n",
      "\"Crouch, and and over, Durathous ewornggely, tor tre was stidatal, loulds waich.\n",
      "Harry.  The cluan Krum.\n",
      "Harris, made in timet her age And, ling, the plows updah at he\n",
      "\n",
      "iter = 39000, smooth loss=1.6952751572353215\n",
      "hep, masped him, the past.\n",
      "The surs sill . . . you store more ef for  arm, shimmenc a starons becon whit . . in moised pomengiigway getents, and but ruch, wettan't caumurgln, what he can keet get rep \n",
      "\n",
      "iter = 40000, smooth loss=1.6837556949421844\n",
      " at rited in.  Vold on them anche keeted to the cladg traing Harry wand that feye sCrulvepakion.  But goong thottont congetiring so theach could gerred, and and Harry weistack threars teble crambers f\n",
      "\n",
      "Best model at iteration: 40120 | smooth loss: 1.66964\n",
      "Best model at iteration: 40121 | smooth loss: 1.66932\n",
      "Best model at iteration: 40122 | smooth loss: 1.66909\n",
      "Best model at iteration: 40124 | smooth loss: 1.66893\n",
      "Best model at iteration: 40131 | smooth loss: 1.66879\n",
      "Best model at iteration: 40132 | smooth loss: 1.66864\n",
      "Best model at iteration: 40134 | smooth loss: 1.66857\n",
      "Best model at iteration: 40136 | smooth loss: 1.66804\n",
      "Best model at iteration: 40139 | smooth loss: 1.66791\n",
      "Best model at iteration: 40143 | smooth loss: 1.66787\n",
      "Best model at iteration: 40144 | smooth loss: 1.66739\n",
      "Best model at iteration: 40147 | smooth loss: 1.66694\n",
      "Best model at iteration: 40160 | smooth loss: 1.66644\n",
      "Best model at iteration: 40164 | smooth loss: 1.66627\n",
      "Best model at iteration: 40165 | smooth loss: 1.66620\n",
      "Best model at iteration: 40166 | smooth loss: 1.66584\n",
      "Best model at iteration: 40167 | smooth loss: 1.66556\n",
      "Best model at iteration: 40168 | smooth loss: 1.66488\n",
      "Best model at iteration: 40171 | smooth loss: 1.66475\n",
      "Best model at iteration: 40172 | smooth loss: 1.66433\n",
      "Best model at iteration: 40174 | smooth loss: 1.66427\n",
      "Best model at iteration: 40175 | smooth loss: 1.66344\n",
      "Best model at iteration: 40184 | smooth loss: 1.66335\n",
      "Best model at iteration: 40185 | smooth loss: 1.66307\n",
      "Best model at iteration: 40189 | smooth loss: 1.66227\n",
      "Best model at iteration: 40198 | smooth loss: 1.66227\n",
      "Best model at iteration: 40200 | smooth loss: 1.66215\n",
      "Best model at iteration: 40208 | smooth loss: 1.66196\n",
      "Best model at iteration: 40230 | smooth loss: 1.66174\n",
      "Best model at iteration: 40231 | smooth loss: 1.66163\n",
      "Best model at iteration: 40232 | smooth loss: 1.66081\n",
      "Best model at iteration: 40233 | smooth loss: 1.66031\n",
      "Best model at iteration: 40234 | smooth loss: 1.66029\n",
      "Best model at iteration: 40235 | smooth loss: 1.65978\n",
      "Best model at iteration: 40236 | smooth loss: 1.65933\n",
      "Best model at iteration: 40237 | smooth loss: 1.65926\n",
      "Best model at iteration: 40238 | smooth loss: 1.65849\n",
      "Best model at iteration: 40241 | smooth loss: 1.65829\n",
      "Best model at iteration: 40242 | smooth loss: 1.65776\n",
      "Best model at iteration: 40247 | smooth loss: 1.65728\n",
      "Best model at iteration: 40248 | smooth loss: 1.65690\n",
      "Best model at iteration: 40249 | smooth loss: 1.65686\n",
      "Best model at iteration: 40250 | smooth loss: 1.65644\n",
      "Best model at iteration: 40251 | smooth loss: 1.65630\n",
      "Best model at iteration: 40252 | smooth loss: 1.65598\n",
      "Best model at iteration: 40253 | smooth loss: 1.65585\n",
      "Best model at iteration: 40254 | smooth loss: 1.65577\n",
      "Best model at iteration: 40255 | smooth loss: 1.65560\n",
      "Best model at iteration: 40256 | smooth loss: 1.65553\n",
      "Best model at iteration: 40262 | smooth loss: 1.65538\n",
      "Best model at iteration: 40263 | smooth loss: 1.65477\n",
      "Best model at iteration: 40266 | smooth loss: 1.65431\n",
      "Best model at iteration: 40269 | smooth loss: 1.65427\n",
      "Best model at iteration: 40270 | smooth loss: 1.65365\n",
      "Best model at iteration: 40284 | smooth loss: 1.65339\n",
      "Best model at iteration: 40286 | smooth loss: 1.65328\n",
      "Best model at iteration: 40287 | smooth loss: 1.65313\n",
      "Best model at iteration: 40291 | smooth loss: 1.65302\n",
      "Best model at iteration: 40293 | smooth loss: 1.65268\n",
      "Best model at iteration: 40404 | smooth loss: 1.65266\n",
      "Best model at iteration: 40405 | smooth loss: 1.65254\n",
      "Best model at iteration: 40414 | smooth loss: 1.65247\n",
      "Best model at iteration: 40415 | smooth loss: 1.65233\n",
      "Best model at iteration: 40430 | smooth loss: 1.65205\n",
      "Best model at iteration: 40436 | smooth loss: 1.65183\n",
      "Best model at iteration: 40437 | smooth loss: 1.65140\n",
      "Best model at iteration: 40441 | smooth loss: 1.65105\n",
      "Best model at iteration: 40445 | smooth loss: 1.65086\n",
      "Best model at iteration: 40446 | smooth loss: 1.65041\n",
      "Best model at iteration: 40447 | smooth loss: 1.65010\n",
      "Best model at iteration: 40448 | smooth loss: 1.64971\n",
      "Best model at iteration: 40449 | smooth loss: 1.64956\n",
      "Best model at iteration: 40450 | smooth loss: 1.64917\n",
      "Best model at iteration: 40453 | smooth loss: 1.64910\n",
      "Best model at iteration: 40476 | smooth loss: 1.64907\n",
      "Best model at iteration: 40477 | smooth loss: 1.64876\n",
      "Best model at iteration: 40488 | smooth loss: 1.64834\n",
      "Best model at iteration: 40489 | smooth loss: 1.64805\n",
      "Best model at iteration: 40573 | smooth loss: 1.64771\n",
      "Best model at iteration: 40574 | smooth loss: 1.64715\n",
      "Best model at iteration: 40577 | smooth loss: 1.64702\n",
      "Best model at iteration: 40581 | smooth loss: 1.64683\n",
      "Best model at iteration: 40582 | smooth loss: 1.64679\n",
      "Best model at iteration: 40583 | smooth loss: 1.64648\n",
      "Best model at iteration: 40584 | smooth loss: 1.64626\n",
      "Best model at iteration: 40585 | smooth loss: 1.64596\n",
      "Best model at iteration: 40627 | smooth loss: 1.64591\n",
      "Best model at iteration: 40628 | smooth loss: 1.64548\n",
      "Best model at iteration: 40631 | smooth loss: 1.64510\n",
      "Best model at iteration: 40633 | smooth loss: 1.64481\n",
      "Best model at iteration: 40634 | smooth loss: 1.64438\n",
      "Best model at iteration: 40636 | smooth loss: 1.64422\n",
      "Best model at iteration: 40637 | smooth loss: 1.64393\n",
      "Best model at iteration: 40641 | smooth loss: 1.64374\n",
      "Best model at iteration: 40642 | smooth loss: 1.64363\n",
      "Best model at iteration: 40643 | smooth loss: 1.64357\n",
      "Best model at iteration: 40644 | smooth loss: 1.64285\n",
      "Best model at iteration: 40645 | smooth loss: 1.64278\n",
      "Best model at iteration: 40646 | smooth loss: 1.64252\n",
      "Best model at iteration: 40649 | smooth loss: 1.64247\n",
      "Best model at iteration: 40652 | smooth loss: 1.64239\n",
      "Best model at iteration: 40666 | smooth loss: 1.64223\n",
      "Best model at iteration: 40668 | smooth loss: 1.64201\n",
      "Best model at iteration: 40669 | smooth loss: 1.64167\n",
      "Best model at iteration: 40670 | smooth loss: 1.64142\n",
      "Best model at iteration: 40671 | smooth loss: 1.64134\n",
      "Best model at iteration: 40672 | smooth loss: 1.64117\n",
      "Best model at iteration: 40674 | smooth loss: 1.64110\n",
      "Best model at iteration: 40675 | smooth loss: 1.64071\n",
      "Best model at iteration: 40676 | smooth loss: 1.64066\n",
      "Best model at iteration: 40677 | smooth loss: 1.64057\n",
      "Best model at iteration: 40678 | smooth loss: 1.64031\n",
      "Best model at iteration: 40679 | smooth loss: 1.64003\n",
      "Best model at iteration: 40680 | smooth loss: 1.63963\n",
      "Best model at iteration: 40681 | smooth loss: 1.63882\n",
      "Best model at iteration: 40685 | smooth loss: 1.63861\n",
      "Best model at iteration: 40686 | smooth loss: 1.63810\n",
      "Best model at iteration: 40687 | smooth loss: 1.63809\n",
      "Best model at iteration: 40688 | smooth loss: 1.63801\n",
      "Best model at iteration: 40689 | smooth loss: 1.63748\n",
      "Best model at iteration: 40690 | smooth loss: 1.63743\n",
      "Best model at iteration: 40691 | smooth loss: 1.63734\n",
      "iter = 41000, smooth loss=1.6429302765918439\n",
      "nor.. said Hotretshipus it hey faitht wize - Harry'nnssing indor lacked to world gave into the amplas sid!\"\n",
      "Harry his liped to finupatinn.\n",
      "\"Wolyedelowe distred as so winck Harry..  \"I relay what me di\n",
      "\n",
      "Best model at iteration: 41698 | smooth loss: 1.63704\n",
      "Best model at iteration: 41699 | smooth loss: 1.63642\n",
      "Best model at iteration: 41700 | smooth loss: 1.63590\n",
      "Best model at iteration: 41701 | smooth loss: 1.63541\n",
      "Best model at iteration: 41702 | smooth loss: 1.63540\n",
      "Best model at iteration: 41703 | smooth loss: 1.63516\n",
      "Best model at iteration: 41704 | smooth loss: 1.63462\n",
      "Best model at iteration: 41705 | smooth loss: 1.63427\n",
      "Best model at iteration: 41706 | smooth loss: 1.63368\n",
      "Best model at iteration: 41707 | smooth loss: 1.63319\n",
      "Best model at iteration: 41708 | smooth loss: 1.63261\n",
      "Best model at iteration: 41709 | smooth loss: 1.63240\n",
      "Best model at iteration: 41710 | smooth loss: 1.63211\n",
      "Best model at iteration: 41990 | smooth loss: 1.63207\n",
      "Best model at iteration: 41991 | smooth loss: 1.63193\n",
      "Best model at iteration: 41992 | smooth loss: 1.63168\n",
      "Best model at iteration: 41994 | smooth loss: 1.63166\n",
      "iter = 42000, smooth loss=1.6322418373414846\n",
      "menpent, his had bory iearier, and Dumbledore.  He wal evered Dumbledore over where hew with shone said me at though the refaclous was had ores and dnewh, weced eft, thought yesshu hem.  Fado Harry Du\n",
      "\n",
      "Best model at iteration: 42002 | smooth loss: 1.63154\n",
      "Best model at iteration: 42003 | smooth loss: 1.63124\n",
      "Best model at iteration: 42004 | smooth loss: 1.63090\n",
      "Best model at iteration: 42005 | smooth loss: 1.63056\n",
      "Best model at iteration: 42006 | smooth loss: 1.62993\n",
      "Best model at iteration: 42008 | smooth loss: 1.62977\n",
      "Best model at iteration: 42009 | smooth loss: 1.62976\n",
      "Best model at iteration: 42010 | smooth loss: 1.62950\n",
      "Best model at iteration: 42011 | smooth loss: 1.62907\n",
      "Best model at iteration: 42015 | smooth loss: 1.62863\n",
      "Best model at iteration: 42016 | smooth loss: 1.62821\n",
      "Best model at iteration: 42017 | smooth loss: 1.62776\n",
      "Best model at iteration: 42024 | smooth loss: 1.62754\n",
      "Best model at iteration: 42025 | smooth loss: 1.62693\n",
      "Best model at iteration: 42026 | smooth loss: 1.62660\n",
      "Best model at iteration: 42027 | smooth loss: 1.62605\n",
      "Best model at iteration: 42106 | smooth loss: 1.62545\n",
      "Best model at iteration: 42111 | smooth loss: 1.62491\n",
      "Best model at iteration: 42131 | smooth loss: 1.62478\n",
      "Best model at iteration: 42132 | smooth loss: 1.62429\n",
      "Best model at iteration: 42133 | smooth loss: 1.62406\n",
      "Best model at iteration: 42175 | smooth loss: 1.62361\n",
      "Best model at iteration: 42187 | smooth loss: 1.62342\n",
      "Best model at iteration: 42188 | smooth loss: 1.62323\n",
      "Best model at iteration: 42189 | smooth loss: 1.62253\n",
      "Best model at iteration: 42195 | smooth loss: 1.62251\n",
      "Best model at iteration: 42199 | smooth loss: 1.62220\n",
      "Best model at iteration: 42201 | smooth loss: 1.62204\n",
      "Best model at iteration: 42202 | smooth loss: 1.62198\n",
      "Best model at iteration: 42205 | smooth loss: 1.62170\n",
      "Best model at iteration: 42206 | smooth loss: 1.62149\n",
      "Best model at iteration: 42207 | smooth loss: 1.62114\n",
      "Best model at iteration: 42237 | smooth loss: 1.62099\n",
      "Best model at iteration: 42238 | smooth loss: 1.62079\n",
      "Best model at iteration: 42241 | smooth loss: 1.62058\n",
      "Best model at iteration: 42244 | smooth loss: 1.61967\n",
      "Best model at iteration: 42245 | smooth loss: 1.61932\n",
      "iter = 43000, smooth loss=1.6440288298023915\n",
      "is heard,\" said Durgite could can's toored.  ToorAr.  On yekriean!\"\n",
      "\"My been you cand his refess frost,\" Her Stand fack to dire, buthas wead yip. \n",
      "He face, stroyed Porflione aroundg arepetidur tave in\n",
      "\n",
      "iter = 44000, smooth loss=1.657163348661239\n",
      "ahore.  Now Crosce.  How coull.\n",
      "I consedly herrep.\n",
      "\"Vilach, saw Thar hard sont you you conflling wrore nexpenterbit hissesp ... yef storends,\" said neall seested termitterning in that. Moody.  Arem!\"\n",
      "\n",
      "\n",
      "Completed epoch 1\n",
      "Epoch: 2\n",
      "iter = 45000, smooth loss=1.7096828705754183\n",
      "\"\n",
      "\t\"Oh? I loughst of dir,\" se hoverying quedamed you.\". Won't for my mormalise to anys has pour, is merinamowlisticlys comp ie meeed it had to killowlise, you, realles.  I mook would yey man lesssreld\n",
      "\n",
      "iter = 46000, smooth loss=1.7245985607063687\n",
      "ot a toilded budge acome to been sincted might thay said eeplertatonacthing waveel - shople of.  A will eveers ond the beeved a murt tayice Priving to that, could be beaned fecent Prat Eagh then Are i\n",
      "\n",
      "iter = 47000, smooth loss=1.7094243517170147\n",
      "ecked exclesinings on and Muggle now Uncle Vart Peting triging at Unclow.\"\n",
      "They're Ladg, onairsh it weur's it the and Mr. Netwarxagetly.  \"Ind the rot at the Quidsw and a bbang ticid,\" said heroors. W\n",
      "\n",
      "iter = 48000, smooth loss=1.699626450468593\n",
      "p rome \"I tryent mort gowants and be farks.\n",
      "\"Where asprest At's capeve to didn't's benting on - and to be been hered te nemte tean Crutchicars.  \"Mighriate if the refeed wattred treated asserry nerniv\n",
      "\n",
      "iter = 49000, smooth loss=1.7134513167080827\n",
      "ontin a cout un caugh the stall go.  A Potter'ad in here parting where I fechen lookss sucy, a blowele her tent a se'd at we rettor.  It I purse a lire sbee she way a fept we chister to key head a cor\n",
      "\n",
      "iter = 50000, smooth loss=1.7427167141647266\n",
      "er and GroHd wond it's dign, tabligmportt, buddelt.  They handly the Routing and still doweteritt alought themetigainn macizar amout.\n",
      "\"I tenars in potmected of Ron, any head they wears yoll word Timng\n",
      "\n",
      "iter = 51000, smooth loss=1.7740584446504037\n",
      "aresst's having out be to have and frow. . . Harry!  he coulder, he surfer gea ald showhing Houmbling of the veaced no hon's venost-latwen attettroughing whit chosed to dien, and the Bagged hore Marma\n",
      "\n",
      "iter = 52000, smooth loss=1.7092667004275413\n",
      "ast timas of Lucfrised hermoly foun that'om anfors.  \"Whis fure thect three very said \"Himuslared, from foure was stist, \"Ead, sextriess. \"I swit Fraccocted bight the niamed,\" said Hermione of Moward \n",
      "\n",
      "iter = 53000, smooth loss=1.6404947610064788\n",
      "n.\"S veem,\" said Harry's it,\" said Hermione slooded it it for in the moon in in kinct ow them ghan.\n",
      "\"It was sturt, and the grouse groused and hourd out bm whith was the Deplauten who have had been mop\n",
      "\n",
      "iter = 54000, smooth loss=1.6750110159457972\n",
      "ask to her cairs defthore's reartilius.\"\n",
      "\"Dop,\" on theye, slapted aloop, as conjo had its to nisells their offawe reberow, but liefitden to I durtroud ne toll, Ron,\" said Darmy,\" he Oy, \"I'vi?\"\n",
      "Mr. Sl\n",
      "\n",
      "iter = 55000, smooth loss=1.7375053734800705\n",
      "frieldl's haddor; they his was greece blother at the tol stark stuppiatcars corred at Lise-dren, Madgor Nearmact it.  Then!  Ron.\n",
      "\"Dand Potress heagle, when Hall, was parted gosted Hallg into it was t\n",
      "\n",
      "iter = 56000, smooth loss=1.7433860767349114\n",
      "y collages,\" said George bedoning the ond gwould Nos to othered tonvosing propeed.\n",
      "\t\t\" he head s idled Fasply up, And sucgrdees. \"whering himst fore to achbounds had every poor Scofmed was in who was \n",
      "\n",
      "iter = 57000, smooth loss=1.7189472713286842\n",
      "el lasachage's arrspes and her of cailligg, and spelfo' lead andishidars - then me, Ind hard then hard wigh look whraugh his pull then have I der hag just clating that it tarred his deme to - He delky\n",
      "\n",
      "iter = 58000, smooth loss=1.6926148811987176\n",
      "louds and Harky his lys noined Harry she dissor\" said Ron lould olk and as to in the glough could night a bet, his light half a come for suck at bear, and bed the?\"\n",
      "\"I I didn'?\"\n",
      "\"Dopter seawing in his\n",
      "\n",
      "iter = 59000, smooth loss=1.7213207436930218\n",
      "ere you rejush the greens a thouse.  \"I's being Harky, and, Harry?\" say labe usned now invataied toaring up than'en they've donifrBul having of handsestles,\" Are and katurred in a somes.\"  Sn'fflename\n",
      "\n",
      "iter = 60000, smooth loss=1.6636434625747036\n",
      ".\n",
      "\"Weached in stild as that's set not Herdioped made nexited. \"Well'ke is?\"\n",
      "\"I'm htamed a lith the Gryffish for then pearil \n",
      "\"Whisht carsed fiors lowd, ampreddraugeenfi.\"\n",
      "\"Ron a glie'd plet.\"\n",
      "\"I Luke\"\n",
      "\n",
      "iter = 61000, smooth loss=1.6445059465144962\n",
      "iden eave is thos betondissed.  Sneece lele, \"o dimp, flamease!  He daor a sendieled to ghough-vercess gettely swill were youns en, plates un finp.  Overy conce rowse zatches were alments him pagont. \n",
      "\n",
      "iter = 62000, smooth loss=1.6435913983296497\n",
      "ur were this wheke and wound was manted.\n",
      "\n",
      "\tketurars you, let the sacu farso, has unnor art you call as plike best tain said ersiesn looked of just of the behord them are sregled rust ceald renter shis\n",
      "\n",
      "iter = 63000, smooth loss=1.661846283013173\n",
      "d,\" he sabed, his was evamess freer to anten hurry's show amitter frre.  \"I the enfoye we they wand.\n",
      "\"Yis straybled wournny.  He shoke.  \"Anved Hapring the champious.\n",
      "Harvioness fen of ciptereher quie\n",
      "\n",
      "iter = 64000, smooth loss=1.6582231402148133\n",
      "rnoush, Hadrer wizing;  \"I large the entinace Mood; the chart, Harry cour-bove pinith a bew.\"\n",
      "\"Then, and Suduch he set light - weath the ere snagent clacet clue leage, and his darrong - soone but the \n",
      "\n",
      "iter = 65000, smooth loss=1.6417844715743224\n",
      "eaking Harry I wang closing Hogwlib.....\n",
      "\n",
      "Sthin, wely Pobter.\n",
      "\"\n",
      "But the've wousens.  \"That'm gover that becambes, Karking towaghs det the rragos.  \"That wander.\"\n",
      "Chon'd spare.  He want butt them going\n",
      "\n",
      "iter = 66000, smooth loss=1.6209280100121675\n",
      "MoRon... I nexe and butiony - to uply into thamp in her fly out ying again itting is it was deread mortuver to knen prited it was raggent, it his okno he doder inTit.  He said Dudblagont!  Harry stike\n",
      "\n",
      "Best model at iteration: 66048 | smooth loss: 1.61919\n",
      "Best model at iteration: 66049 | smooth loss: 1.61881\n",
      "Best model at iteration: 66050 | smooth loss: 1.61876\n",
      "Best model at iteration: 66051 | smooth loss: 1.61825\n",
      "Best model at iteration: 66052 | smooth loss: 1.61811\n",
      "Best model at iteration: 66053 | smooth loss: 1.61795\n",
      "Best model at iteration: 66054 | smooth loss: 1.61793\n",
      "Best model at iteration: 66055 | smooth loss: 1.61787\n",
      "Best model at iteration: 66056 | smooth loss: 1.61737\n",
      "Best model at iteration: 66058 | smooth loss: 1.61728\n",
      "Best model at iteration: 66059 | smooth loss: 1.61695\n",
      "Best model at iteration: 66060 | smooth loss: 1.61675\n",
      "Best model at iteration: 66061 | smooth loss: 1.61660\n",
      "Best model at iteration: 66080 | smooth loss: 1.61641\n",
      "Best model at iteration: 66093 | smooth loss: 1.61614\n",
      "Best model at iteration: 66094 | smooth loss: 1.61613\n",
      "Best model at iteration: 66095 | smooth loss: 1.61564\n",
      "Best model at iteration: 66096 | smooth loss: 1.61544\n",
      "Best model at iteration: 66097 | smooth loss: 1.61537\n",
      "Best model at iteration: 66098 | smooth loss: 1.61511\n",
      "Best model at iteration: 66099 | smooth loss: 1.61479\n",
      "Best model at iteration: 66123 | smooth loss: 1.61478\n",
      "Best model at iteration: 66124 | smooth loss: 1.61441\n",
      "Best model at iteration: 66125 | smooth loss: 1.61408\n",
      "Best model at iteration: 66126 | smooth loss: 1.61369\n",
      "Best model at iteration: 66129 | smooth loss: 1.61332\n",
      "Best model at iteration: 66130 | smooth loss: 1.61295\n",
      "Best model at iteration: 66218 | smooth loss: 1.61270\n",
      "iter = 67000, smooth loss=1.6518242073394496\n",
      "ave bittant pocled soint the noomed over of lijuse.  Now the pit, and chases wince morny pornif clewizes the Dably formed the I badword the pives of channef read-the swords's reps.  Fireins, an would \n",
      "\n",
      "iter = 68000, smooth loss=1.6526317388426857\n",
      "s, why didned all and to...\"\n",
      "\"Chournsubes task's,\" said Strout,\" said Ron.\n",
      "\"Went, and Harry!\" Ron sudde sharound, days had gold thing we knows qeemter too, pant.\"\n",
      "\"Good beingly up then,\" said Ron her \n",
      "\n",
      "iter = 69000, smooth loss=1.671946355122788\n",
      "should anved firtiling their looking back exchlesed.   Hurry, who Doist were didlens!  Pand the was just icross the looked durners to the s'ce let there after the stously, wure two trave.  Hermione do\n",
      "\n",
      "iter = 70000, smooth loss=1.6694771772310988\n",
      "tters enty.\n",
      "Ron ereshess batce was a to trieg.\n",
      "\"What have gining eximed on got was this goin.\" Ron anded joon the camortied Ron and the taghan from Snape in from the with Harry sight?\"  you satcr had \n",
      "\n",
      "iter = 71000, smooth loss=1.674998467768777\n",
      "s watting fay. . . . he way thoughchaks troniy. \"He tush- and Hagrid wever morting, linner topess's some. Yeid-\"y wattertout-nod.  The sey grong,\"\n",
      "Ron she's hadn't gauds, wese inct and tworgg agaugali\n",
      "\n",
      "iter = 72000, smooth loss=1.6767234335083694\n",
      "oses on fast affiard, one was off the wants, whot-oussed her thriedso Maved loubshowals and muce jere and as feplesided oued a rearop Hood was lobbusalled up the look rigcing upent than end fuces pott\n",
      "\n",
      "iter = 73000, smooth loss=1.6451227719867845\n",
      " Faiced this wizor you to Bardy, sourd.\n",
      "Harry scornometoters, He bars.  He galed to more.  \"Heglcarry, you?\"\n",
      "\"Iowr, ways Moody.  \"Ere moed to him and Hurridong?\"\n",
      "\"It's handens weet then - forvelt.\n",
      "\"Ye\n",
      "\n",
      "iter = 74000, smooth loss=1.6471987221756288\n",
      "n ann gonightly.  Harry side that that the librs, I that he heall, be abritting to say of his heappes about hear, book exawal the Yibe Dubble sursuce.\" Harry sun levers?  He watching all who take the \n",
      "\n",
      "iter = 75000, smooth loss=1.6320857182758526\n",
      "ime his heud sign to aftering to Ron astened ounds. Dolely outioull - your was been them to be sfid just above -.. weam of thered and lire be giving,\" said Hermawover.  He hadreafing was the cotteded \n",
      "\n",
      "iter = 76000, smooth loss=1.6318541734979766\n",
      " the seriously at the get showly hand wold arfing usness.\n",
      "\"The hard to the dor herselfo somster beech dooks approver bach interes.\"\n",
      "\"Kher,\" said Ron,\" Harry didoble taff, the Trange but ath him with s\n",
      "\n",
      "Best model at iteration: 76412 | smooth loss: 1.61236\n",
      "Best model at iteration: 76413 | smooth loss: 1.61212\n",
      "Best model at iteration: 76418 | smooth loss: 1.61210\n",
      "Best model at iteration: 76421 | smooth loss: 1.61195\n",
      "Best model at iteration: 76422 | smooth loss: 1.61192\n",
      "Best model at iteration: 76424 | smooth loss: 1.61168\n",
      "Best model at iteration: 76425 | smooth loss: 1.61142\n",
      "Best model at iteration: 76430 | smooth loss: 1.61099\n",
      "Best model at iteration: 76431 | smooth loss: 1.61042\n",
      "Best model at iteration: 76432 | smooth loss: 1.61041\n",
      "Best model at iteration: 76437 | smooth loss: 1.61017\n",
      "Best model at iteration: 76438 | smooth loss: 1.61012\n",
      "Best model at iteration: 76439 | smooth loss: 1.60979\n",
      "Best model at iteration: 76441 | smooth loss: 1.60966\n",
      "Best model at iteration: 76443 | smooth loss: 1.60922\n",
      "Best model at iteration: 76444 | smooth loss: 1.60895\n",
      "Best model at iteration: 76446 | smooth loss: 1.60855\n",
      "Best model at iteration: 76472 | smooth loss: 1.60853\n",
      "Best model at iteration: 76473 | smooth loss: 1.60847\n",
      "Best model at iteration: 76503 | smooth loss: 1.60836\n",
      "Best model at iteration: 76506 | smooth loss: 1.60806\n",
      "Best model at iteration: 76507 | smooth loss: 1.60787\n",
      "Best model at iteration: 76512 | smooth loss: 1.60786\n",
      "Best model at iteration: 76513 | smooth loss: 1.60784\n",
      "Best model at iteration: 76567 | smooth loss: 1.60782\n",
      "Best model at iteration: 76568 | smooth loss: 1.60760\n",
      "Best model at iteration: 76569 | smooth loss: 1.60715\n",
      "Best model at iteration: 76572 | smooth loss: 1.60666\n",
      "Best model at iteration: 76573 | smooth loss: 1.60638\n",
      "Best model at iteration: 76574 | smooth loss: 1.60614\n",
      "Best model at iteration: 76640 | smooth loss: 1.60598\n",
      "Best model at iteration: 76642 | smooth loss: 1.60537\n",
      "Best model at iteration: 76645 | smooth loss: 1.60518\n",
      "Best model at iteration: 76646 | smooth loss: 1.60489\n",
      "Best model at iteration: 76647 | smooth loss: 1.60486\n",
      "Best model at iteration: 76678 | smooth loss: 1.60451\n",
      "Best model at iteration: 76680 | smooth loss: 1.60407\n",
      "Best model at iteration: 76682 | smooth loss: 1.60402\n",
      "Best model at iteration: 76683 | smooth loss: 1.60387\n",
      "Best model at iteration: 76684 | smooth loss: 1.60334\n",
      "Best model at iteration: 76687 | smooth loss: 1.60332\n",
      "Best model at iteration: 76688 | smooth loss: 1.60314\n",
      "Best model at iteration: 76739 | smooth loss: 1.60310\n",
      "Best model at iteration: 76743 | smooth loss: 1.60291\n",
      "Best model at iteration: 76744 | smooth loss: 1.60273\n",
      "Best model at iteration: 76745 | smooth loss: 1.60260\n",
      "Best model at iteration: 76746 | smooth loss: 1.60221\n",
      "Best model at iteration: 76747 | smooth loss: 1.60199\n",
      "Best model at iteration: 76810 | smooth loss: 1.60167\n",
      "Best model at iteration: 76811 | smooth loss: 1.60160\n",
      "Best model at iteration: 76812 | smooth loss: 1.60156\n",
      "Best model at iteration: 76813 | smooth loss: 1.60154\n",
      "Best model at iteration: 76815 | smooth loss: 1.60098\n",
      "Best model at iteration: 76817 | smooth loss: 1.60065\n",
      "Best model at iteration: 76818 | smooth loss: 1.60044\n",
      "Best model at iteration: 76822 | smooth loss: 1.60003\n",
      "Best model at iteration: 76823 | smooth loss: 1.59975\n",
      "Best model at iteration: 76829 | smooth loss: 1.59952\n",
      "Best model at iteration: 76831 | smooth loss: 1.59916\n",
      "Best model at iteration: 76832 | smooth loss: 1.59889\n",
      "Best model at iteration: 76836 | smooth loss: 1.59887\n",
      "Best model at iteration: 76837 | smooth loss: 1.59864\n",
      "Best model at iteration: 76839 | smooth loss: 1.59853\n",
      "Best model at iteration: 76841 | smooth loss: 1.59848\n",
      "iter = 77000, smooth loss=1.61281011983275\n",
      "e stowling peerrugs a barget insurt a coached allor, as the kabben I donere the and point there throt jush bewor.\n",
      "\n",
      "Hell, frown recken.\"\n",
      "\"We edle keefly lyegrid up to Hermione on enorth moketh out of t\n",
      "\n",
      "iter = 78000, smooth loss=1.6000440869376669\n",
      " ssart pupton the varred.  Tes.  He say myse, it's talk Hermione, Harry!\"\n",
      "\"Relted any fined tight termors and that wasn of stroxken one indire pupply to wizards?\" Dady quike nome toward the add appele\n",
      "\n",
      "Best model at iteration: 78015 | smooth loss: 1.59840\n",
      "Best model at iteration: 78016 | smooth loss: 1.59839\n",
      "Best model at iteration: 78018 | smooth loss: 1.59814\n",
      "Best model at iteration: 78020 | smooth loss: 1.59804\n",
      "Best model at iteration: 78021 | smooth loss: 1.59802\n",
      "Best model at iteration: 78022 | smooth loss: 1.59715\n",
      "Best model at iteration: 78023 | smooth loss: 1.59695\n",
      "Best model at iteration: 78025 | smooth loss: 1.59685\n",
      "Best model at iteration: 78026 | smooth loss: 1.59662\n",
      "Best model at iteration: 78028 | smooth loss: 1.59610\n",
      "Best model at iteration: 78029 | smooth loss: 1.59596\n",
      "Best model at iteration: 78030 | smooth loss: 1.59552\n",
      "Best model at iteration: 78032 | smooth loss: 1.59550\n",
      "Best model at iteration: 78033 | smooth loss: 1.59516\n",
      "Best model at iteration: 78034 | smooth loss: 1.59457\n",
      "Best model at iteration: 78035 | smooth loss: 1.59420\n",
      "Best model at iteration: 78036 | smooth loss: 1.59351\n",
      "Best model at iteration: 78037 | smooth loss: 1.59335\n",
      "Best model at iteration: 78039 | smooth loss: 1.59322\n",
      "Best model at iteration: 78040 | smooth loss: 1.59287\n",
      "Best model at iteration: 78041 | smooth loss: 1.59215\n",
      "Best model at iteration: 78042 | smooth loss: 1.59177\n",
      "Best model at iteration: 78043 | smooth loss: 1.59170\n",
      "Best model at iteration: 78044 | smooth loss: 1.59152\n",
      "Best model at iteration: 78045 | smooth loss: 1.59126\n",
      "Best model at iteration: 78046 | smooth loss: 1.59099\n",
      "Best model at iteration: 78047 | smooth loss: 1.59086\n",
      "Best model at iteration: 78048 | smooth loss: 1.59058\n",
      "Best model at iteration: 78051 | smooth loss: 1.59040\n",
      "Best model at iteration: 78052 | smooth loss: 1.58953\n",
      "Best model at iteration: 78054 | smooth loss: 1.58940\n",
      "Best model at iteration: 78055 | smooth loss: 1.58886\n",
      "Best model at iteration: 78056 | smooth loss: 1.58851\n",
      "Best model at iteration: 78089 | smooth loss: 1.58833\n",
      "Best model at iteration: 78093 | smooth loss: 1.58782\n",
      "Best model at iteration: 78094 | smooth loss: 1.58735\n",
      "Best model at iteration: 78095 | smooth loss: 1.58693\n",
      "Best model at iteration: 78112 | smooth loss: 1.58675\n",
      "Best model at iteration: 78115 | smooth loss: 1.58659\n",
      "Best model at iteration: 78116 | smooth loss: 1.58573\n",
      "Best model at iteration: 78117 | smooth loss: 1.58565\n",
      "Best model at iteration: 78118 | smooth loss: 1.58526\n",
      "Best model at iteration: 78120 | smooth loss: 1.58514\n",
      "Best model at iteration: 78129 | smooth loss: 1.58483\n",
      "Best model at iteration: 78130 | smooth loss: 1.58431\n",
      "Best model at iteration: 78131 | smooth loss: 1.58418\n",
      "Best model at iteration: 78132 | smooth loss: 1.58345\n",
      "Best model at iteration: 78135 | smooth loss: 1.58332\n",
      "Best model at iteration: 78136 | smooth loss: 1.58308\n",
      "Best model at iteration: 78137 | smooth loss: 1.58286\n",
      "Best model at iteration: 78138 | smooth loss: 1.58275\n",
      "Best model at iteration: 78139 | smooth loss: 1.58248\n",
      "Best model at iteration: 78140 | smooth loss: 1.58218\n",
      "Best model at iteration: 78145 | smooth loss: 1.58215\n",
      "Best model at iteration: 78146 | smooth loss: 1.58202\n",
      "Best model at iteration: 78151 | smooth loss: 1.58191\n",
      "Best model at iteration: 78152 | smooth loss: 1.58150\n",
      "Best model at iteration: 78154 | smooth loss: 1.58143\n",
      "Best model at iteration: 78155 | smooth loss: 1.58128\n",
      "Best model at iteration: 78156 | smooth loss: 1.58116\n",
      "Best model at iteration: 78157 | smooth loss: 1.58050\n",
      "Best model at iteration: 78158 | smooth loss: 1.57988\n",
      "Best model at iteration: 78159 | smooth loss: 1.57952\n",
      "Best model at iteration: 78161 | smooth loss: 1.57949\n",
      "Best model at iteration: 78162 | smooth loss: 1.57891\n",
      "Best model at iteration: 78164 | smooth loss: 1.57881\n",
      "Best model at iteration: 78165 | smooth loss: 1.57845\n",
      "Best model at iteration: 78167 | smooth loss: 1.57843\n",
      "Best model at iteration: 78168 | smooth loss: 1.57810\n",
      "Best model at iteration: 78169 | smooth loss: 1.57809\n",
      "Best model at iteration: 78170 | smooth loss: 1.57791\n",
      "Best model at iteration: 78172 | smooth loss: 1.57773\n",
      "Best model at iteration: 78174 | smooth loss: 1.57769\n",
      "Best model at iteration: 78373 | smooth loss: 1.57735\n",
      "Best model at iteration: 78374 | smooth loss: 1.57690\n",
      "Best model at iteration: 78375 | smooth loss: 1.57681\n",
      "Best model at iteration: 78377 | smooth loss: 1.57642\n",
      "Best model at iteration: 78379 | smooth loss: 1.57641\n",
      "Best model at iteration: 78380 | smooth loss: 1.57595\n",
      "Best model at iteration: 78384 | smooth loss: 1.57587\n",
      "Best model at iteration: 78386 | smooth loss: 1.57584\n",
      "Best model at iteration: 78393 | smooth loss: 1.57557\n",
      "Best model at iteration: 78394 | smooth loss: 1.57495\n",
      "Best model at iteration: 78395 | smooth loss: 1.57453\n",
      "Best model at iteration: 78396 | smooth loss: 1.57428\n",
      "Best model at iteration: 78398 | smooth loss: 1.57419\n",
      "Best model at iteration: 78399 | smooth loss: 1.57363\n",
      "Best model at iteration: 78400 | smooth loss: 1.57305\n",
      "Best model at iteration: 78401 | smooth loss: 1.57259\n",
      "Best model at iteration: 78403 | smooth loss: 1.57201\n",
      "Best model at iteration: 78404 | smooth loss: 1.57175\n",
      "Best model at iteration: 78405 | smooth loss: 1.57154\n",
      "Best model at iteration: 78433 | smooth loss: 1.57127\n",
      "Best model at iteration: 78434 | smooth loss: 1.57060\n",
      "Best model at iteration: 78435 | smooth loss: 1.57008\n",
      "Best model at iteration: 78436 | smooth loss: 1.56989\n",
      "Best model at iteration: 78437 | smooth loss: 1.56948\n",
      "Best model at iteration: 78442 | smooth loss: 1.56929\n",
      "Best model at iteration: 78443 | smooth loss: 1.56924\n",
      "Best model at iteration: 78448 | smooth loss: 1.56918\n",
      "Best model at iteration: 78533 | smooth loss: 1.56885\n",
      "Best model at iteration: 78534 | smooth loss: 1.56858\n",
      "Best model at iteration: 78535 | smooth loss: 1.56790\n",
      "Best model at iteration: 78536 | smooth loss: 1.56752\n",
      "Best model at iteration: 78537 | smooth loss: 1.56736\n",
      "Best model at iteration: 78538 | smooth loss: 1.56683\n",
      "Best model at iteration: 78539 | smooth loss: 1.56627\n",
      "Best model at iteration: 78540 | smooth loss: 1.56611\n",
      "Best model at iteration: 78541 | smooth loss: 1.56611\n",
      "Best model at iteration: 78542 | smooth loss: 1.56591\n",
      "Best model at iteration: 78543 | smooth loss: 1.56575\n",
      "Best model at iteration: 78544 | smooth loss: 1.56539\n",
      "Best model at iteration: 78545 | smooth loss: 1.56475\n",
      "Best model at iteration: 78546 | smooth loss: 1.56432\n",
      "Best model at iteration: 78547 | smooth loss: 1.56429\n",
      "Best model at iteration: 78548 | smooth loss: 1.56389\n",
      "Best model at iteration: 78552 | smooth loss: 1.56374\n",
      "Best model at iteration: 78553 | smooth loss: 1.56372\n",
      "Best model at iteration: 78554 | smooth loss: 1.56361\n",
      "Best model at iteration: 78558 | smooth loss: 1.56333\n",
      "Best model at iteration: 78559 | smooth loss: 1.56295\n",
      "Best model at iteration: 78564 | smooth loss: 1.56294\n",
      "Best model at iteration: 78565 | smooth loss: 1.56273\n",
      "Best model at iteration: 78566 | smooth loss: 1.56261\n",
      "Best model at iteration: 78569 | smooth loss: 1.56227\n",
      "Best model at iteration: 78570 | smooth loss: 1.56208\n",
      "Best model at iteration: 78571 | smooth loss: 1.56163\n",
      "Best model at iteration: 78572 | smooth loss: 1.56129\n",
      "Best model at iteration: 78574 | smooth loss: 1.56054\n",
      "Best model at iteration: 78575 | smooth loss: 1.56051\n",
      "Best model at iteration: 78576 | smooth loss: 1.56019\n",
      "Best model at iteration: 78579 | smooth loss: 1.56005\n",
      "Best model at iteration: 78580 | smooth loss: 1.55988\n",
      "Best model at iteration: 78591 | smooth loss: 1.55975\n",
      "Best model at iteration: 78597 | smooth loss: 1.55915\n",
      "Best model at iteration: 78599 | smooth loss: 1.55866\n",
      "Best model at iteration: 78601 | smooth loss: 1.55865\n",
      "Best model at iteration: 78602 | smooth loss: 1.55864\n",
      "Best model at iteration: 78654 | smooth loss: 1.55848\n",
      "Best model at iteration: 78671 | smooth loss: 1.55810\n",
      "Best model at iteration: 78672 | smooth loss: 1.55797\n",
      "Best model at iteration: 78673 | smooth loss: 1.55779\n",
      "Best model at iteration: 78674 | smooth loss: 1.55740\n",
      "Best model at iteration: 78677 | smooth loss: 1.55728\n",
      "Best model at iteration: 78678 | smooth loss: 1.55714\n",
      "Best model at iteration: 78688 | smooth loss: 1.55697\n",
      "Best model at iteration: 78692 | smooth loss: 1.55693\n",
      "Best model at iteration: 78693 | smooth loss: 1.55683\n",
      "Best model at iteration: 78694 | smooth loss: 1.55618\n",
      "Best model at iteration: 78695 | smooth loss: 1.55608\n",
      "Best model at iteration: 78696 | smooth loss: 1.55607\n",
      "Best model at iteration: 78722 | smooth loss: 1.55584\n",
      "Best model at iteration: 78723 | smooth loss: 1.55545\n",
      "Best model at iteration: 78725 | smooth loss: 1.55544\n",
      "Best model at iteration: 78727 | smooth loss: 1.55537\n",
      "Best model at iteration: 78728 | smooth loss: 1.55495\n",
      "Best model at iteration: 78729 | smooth loss: 1.55429\n",
      "Best model at iteration: 78730 | smooth loss: 1.55414\n",
      "Best model at iteration: 78733 | smooth loss: 1.55414\n",
      "iter = 79000, smooth loss=1.5673615362664772\n",
      "enky lize anterstter the that her you chause his tasknabberE-, would eact trangis and him.  This the spisardowor!\"  said Ron, his clestelesares.  The have Torle.  \"All owl, rearbad ittagered on him an\n",
      "\n",
      "iter = 80000, smooth loss=1.5842812822527523\n",
      "oupherd clinting, withrouch in yessing to demp as Ald dealled \n",
      "Ttus unting, and made of up, you cleered to have andaring nowned, some Min's at at Karkince.\"\n",
      "Eveate,\" said Harry. \"\n",
      "Hy was corple, gied \n",
      "\n",
      "Best model at iteration: 80838 | smooth loss: 1.55394\n",
      "Best model at iteration: 80840 | smooth loss: 1.55370\n",
      "Best model at iteration: 80843 | smooth loss: 1.55335\n",
      "Best model at iteration: 80844 | smooth loss: 1.55307\n",
      "Best model at iteration: 80845 | smooth loss: 1.55272\n",
      "Best model at iteration: 80846 | smooth loss: 1.55268\n",
      "Best model at iteration: 80881 | smooth loss: 1.55241\n",
      "iter = 81000, smooth loss=1.5604657398820603\n",
      "tess eaking at micteming, and the could to Harry have there cencemaid hourd trapperess.. The arm . . .\"\n",
      "\"Welled to the' watce to rearmensbed with the would you learedly one only a.  The Jure said not \n",
      "\n",
      "iter = 82000, smooth loss=1.5905144416803936\n",
      "ide, bloated.\n",
      "\n",
      "The hear, and tints fliacion. 'lurgs its feles he had to the whaffed of fine into therest a dor sudds up a douse and to his jemine the remed to sampati and to appiored hood neos!  Ron a\n",
      "\n",
      "iter = 83000, smooth loss=1.5766318072119683\n",
      "t had night, unds of the thing, Harry -- was belt alonk oncieds, the pobled Harry oplaned that Harry now Weach caired a pan. . . oun holder dlamy of the bight, himble of regort ippaned stoold was beon\n",
      "\n",
      "iter = 84000, smooth loss=1.6070609356783763\n",
      "uef turrig.  Harry said, who you help me of . . . a spill sirclinted possurt silver as out molieveds to be a ninger light of his of, clackating to burnerg seins now while be mothing it and of Ene wim \n",
      "\n",
      "Best model at iteration: 84736 | smooth loss: 1.55237\n",
      "Best model at iteration: 84737 | smooth loss: 1.55216\n",
      "Best model at iteration: 84738 | smooth loss: 1.55183\n",
      "Best model at iteration: 84741 | smooth loss: 1.55178\n",
      "Best model at iteration: 84742 | smooth loss: 1.55136\n",
      "Best model at iteration: 84745 | smooth loss: 1.55125\n",
      "Best model at iteration: 84746 | smooth loss: 1.55104\n",
      "Best model at iteration: 84747 | smooth loss: 1.55076\n",
      "Best model at iteration: 84748 | smooth loss: 1.55052\n",
      "Best model at iteration: 84749 | smooth loss: 1.55017\n",
      "Best model at iteration: 84750 | smooth loss: 1.54998\n",
      "Best model at iteration: 84751 | smooth loss: 1.54958\n",
      "Best model at iteration: 84776 | smooth loss: 1.54935\n",
      "Best model at iteration: 84777 | smooth loss: 1.54893\n",
      "Best model at iteration: 84778 | smooth loss: 1.54871\n",
      "Best model at iteration: 84789 | smooth loss: 1.54842\n",
      "Best model at iteration: 84790 | smooth loss: 1.54813\n",
      "Best model at iteration: 84805 | smooth loss: 1.54811\n",
      "Best model at iteration: 84812 | smooth loss: 1.54806\n",
      "Best model at iteration: 84814 | smooth loss: 1.54796\n",
      "Best model at iteration: 84817 | smooth loss: 1.54783\n",
      "Best model at iteration: 84818 | smooth loss: 1.54770\n",
      "Best model at iteration: 84874 | smooth loss: 1.54722\n",
      "Best model at iteration: 84875 | smooth loss: 1.54675\n",
      "Best model at iteration: 84877 | smooth loss: 1.54671\n",
      "Best model at iteration: 84878 | smooth loss: 1.54650\n",
      "Best model at iteration: 84884 | smooth loss: 1.54646\n",
      "Best model at iteration: 84885 | smooth loss: 1.54633\n",
      "Best model at iteration: 84886 | smooth loss: 1.54611\n",
      "Best model at iteration: 84934 | smooth loss: 1.54601\n",
      "Best model at iteration: 84935 | smooth loss: 1.54554\n",
      "Best model at iteration: 84937 | smooth loss: 1.54532\n",
      "Best model at iteration: 84938 | smooth loss: 1.54491\n",
      "Best model at iteration: 84941 | smooth loss: 1.54481\n",
      "Best model at iteration: 84942 | smooth loss: 1.54454\n",
      "Best model at iteration: 84943 | smooth loss: 1.54435\n",
      "Best model at iteration: 84944 | smooth loss: 1.54418\n",
      "Best model at iteration: 84945 | smooth loss: 1.54350\n",
      "Best model at iteration: 84946 | smooth loss: 1.54326\n",
      "Best model at iteration: 84947 | smooth loss: 1.54308\n",
      "Best model at iteration: 84965 | smooth loss: 1.54301\n",
      "Best model at iteration: 84966 | smooth loss: 1.54296\n",
      "Best model at iteration: 84967 | smooth loss: 1.54274\n",
      "Best model at iteration: 84969 | smooth loss: 1.54243\n",
      "Best model at iteration: 84970 | smooth loss: 1.54203\n",
      "Best model at iteration: 84971 | smooth loss: 1.54201\n",
      "Best model at iteration: 84973 | smooth loss: 1.54196\n",
      "Best model at iteration: 84975 | smooth loss: 1.54184\n",
      "Best model at iteration: 84976 | smooth loss: 1.54149\n",
      "Best model at iteration: 84978 | smooth loss: 1.54146\n",
      "Best model at iteration: 84979 | smooth loss: 1.54127\n",
      "Best model at iteration: 84980 | smooth loss: 1.54099\n",
      "Best model at iteration: 84981 | smooth loss: 1.54078\n",
      "Best model at iteration: 84982 | smooth loss: 1.53994\n",
      "Best model at iteration: 84986 | smooth loss: 1.53985\n",
      "Best model at iteration: 84987 | smooth loss: 1.53939\n",
      "Best model at iteration: 84990 | smooth loss: 1.53896\n",
      "iter = 85000, smooth loss=1.5412413154208087\n",
      "t to well?\"\n",
      "Voldemen.\n",
      "\"At, his wait armpened hurning heard.  \"Treas?\"\n",
      "Fon'- I wook?\"  Bagas he an, figurever.\n",
      "He though \"rooming Uarea!\" he woldert wayminiciel who were a post.\n",
      "\"Wher. .. well over off\n",
      "\n",
      "iter = 86000, smooth loss=1.539191339189896\n",
      " your every hatherided Minid My Croffituer him of impat Dumblick queared.  He walked abady, whid you wal my fight.  I knew done who waugh saaking whem the mates to feen the matted this evelytsway Daka\n",
      "\n",
      "Best model at iteration: 86001 | smooth loss: 1.53864\n",
      "Best model at iteration: 86002 | smooth loss: 1.53813\n",
      "Best model at iteration: 86004 | smooth loss: 1.53810\n",
      "Best model at iteration: 86005 | smooth loss: 1.53753\n",
      "Best model at iteration: 86006 | smooth loss: 1.53711\n",
      "Best model at iteration: 86007 | smooth loss: 1.53650\n",
      "Best model at iteration: 86008 | smooth loss: 1.53594\n",
      "Best model at iteration: 86009 | smooth loss: 1.53546\n",
      "Best model at iteration: 86010 | smooth loss: 1.53523\n",
      "Best model at iteration: 86011 | smooth loss: 1.53490\n",
      "Best model at iteration: 86290 | smooth loss: 1.53438\n",
      "Best model at iteration: 86291 | smooth loss: 1.53421\n",
      "Best model at iteration: 86292 | smooth loss: 1.53403\n",
      "Best model at iteration: 86293 | smooth loss: 1.53378\n",
      "Best model at iteration: 86295 | smooth loss: 1.53366\n",
      "Best model at iteration: 86304 | smooth loss: 1.53345\n",
      "Best model at iteration: 86305 | smooth loss: 1.53309\n",
      "Best model at iteration: 86306 | smooth loss: 1.53268\n",
      "Best model at iteration: 86307 | smooth loss: 1.53205\n",
      "Best model at iteration: 86309 | smooth loss: 1.53190\n",
      "Best model at iteration: 86310 | smooth loss: 1.53182\n",
      "Best model at iteration: 86311 | smooth loss: 1.53155\n",
      "Best model at iteration: 86312 | smooth loss: 1.53118\n",
      "Best model at iteration: 86316 | smooth loss: 1.53104\n",
      "Best model at iteration: 86317 | smooth loss: 1.53055\n",
      "Best model at iteration: 86318 | smooth loss: 1.53024\n",
      "Best model at iteration: 86325 | smooth loss: 1.52998\n",
      "Best model at iteration: 86326 | smooth loss: 1.52938\n",
      "Best model at iteration: 86327 | smooth loss: 1.52902\n",
      "Best model at iteration: 86328 | smooth loss: 1.52840\n",
      "Best model at iteration: 86409 | smooth loss: 1.52839\n",
      "Best model at iteration: 86410 | smooth loss: 1.52837\n",
      "Best model at iteration: 86412 | smooth loss: 1.52769\n",
      "Best model at iteration: 86433 | smooth loss: 1.52736\n",
      "Best model at iteration: 86434 | smooth loss: 1.52706\n",
      "Best model at iteration: 86455 | smooth loss: 1.52692\n",
      "Best model at iteration: 86488 | smooth loss: 1.52677\n",
      "Best model at iteration: 86489 | smooth loss: 1.52654\n",
      "Best model at iteration: 86490 | smooth loss: 1.52586\n",
      "Best model at iteration: 86491 | smooth loss: 1.52584\n",
      "Best model at iteration: 86496 | smooth loss: 1.52558\n",
      "Best model at iteration: 86500 | smooth loss: 1.52529\n",
      "Best model at iteration: 86502 | smooth loss: 1.52524\n",
      "Best model at iteration: 86503 | smooth loss: 1.52523\n",
      "Best model at iteration: 86506 | smooth loss: 1.52511\n",
      "Best model at iteration: 86507 | smooth loss: 1.52504\n",
      "Best model at iteration: 86508 | smooth loss: 1.52465\n",
      "Best model at iteration: 86545 | smooth loss: 1.52403\n",
      "Best model at iteration: 86546 | smooth loss: 1.52380\n",
      "iter = 87000, smooth loss=1.545460219827306\n",
      " the raff, fromted the pain puther mo milislicure a dright,\" he folle - the houghts he saw Voldiss - But for proterused the feop the hounges, that not once to reverd, branked reeded the melus finding \n",
      "\n",
      "iter = 88000, smooth loss=1.5501013692877497\n",
      "waad to strode.  The Kris bolled face Voldemal, and the cance not wanap as make saver thun one insmouch of cire was Harry betalver the Dixbls was Lyop ack how,\"  said again.  Harry.  Ron moment of alg\n",
      "\n",
      "Completed epoch 2\n",
      "Epoch: 3\n",
      "iter = 89000, smooth loss=1.612697658611767\n",
      "cinging.  Fren down eny it though voicius back.  He seads canting him an and as a sames their a loun alooging though is his dack dight, diver that showled him eving on the paut.  I'ver, with the oft t\n",
      "\n",
      "iter = 90000, smooth loss=1.60975749678283\n",
      "n the knowjoss's about, for any farside itsised at Harry was staring. .Wore had'he will he ward would and had as fall gettlessed by his Atlered.  \"You the stild aly crow: said so parned to kistion.\n",
      "\tA\n",
      "\n",
      "iter = 91000, smooth loss=1.6365252660600569\n",
      "ic looked to ough I't seed, that he would has notting him saytlike he was and duevenred cugse, and exive it.  He lirlexagidation on whicr Maging turne the Dursleys indo was otcborny perely to intine w\n",
      "\n",
      "iter = 92000, smooth loss=1.6112499725956384\n",
      ".\n",
      "Harry a cared whother graze dlack.\n",
      "\" Mr. Weactat looking to Me....hay hearg and I mumping sholk,\" surnwad in on a pewnily to lullen.  Not heasing in ananed Peasions, the art his to Mook.\n",
      "\"They quase\n",
      "\n",
      "iter = 93000, smooth loss=1.628198566625403\n",
      "p, adus at the staying pimfated?\"\n",
      "\"Bles un.  \"\n",
      "\"Be rooming foy not faces, plaskinn't out any heldent hil out - of talling to been which.\n",
      "Mr. Weasl you?\" reade hold Mr. Weam, Harry?\"  Tinibul's foonty \n",
      "\n",
      "iter = 94000, smooth loss=1.6652043624428363\n",
      "ht any and say, and to drased at Hadry can't witchoase in the undiin of thank.  \"Will Bagman wizard, stall that man eacs, just a perton-finds about you kittly.  \"And curse, Mr. Weasley had excitedny s\n",
      "\n",
      "iter = 95000, smooth loss=1.7019293934766755\n",
      "r his tonstuld with said screate louds his scrairs, skinceving at the purpleted Eatien, was suddered to pitching on the qualkedly of the lifecal Harry had seatlf of the nimch of Malfoy want ittidn's a\n",
      "\n",
      "iter = 96000, smooth loss=1.655142540285885\n",
      "theis your wanting behed up anits had pater anlip tempents now Ron, \"We's Harry, it a days they\"\n",
      "The lookeds to th's just them. Harry skey me said, they gratch Winoust you somehly an here unsting alki\n",
      "\n",
      "iter = 97000, smooth loss=1.591320138507453\n",
      "wble sirtuying toing was over. . . .\" the Worltastly.\n",
      "\"This?  Wear?\" said Harry sumttrent cont erqualheyes?\" said Free was nizaxterwartulibart to, eving it blats agaved have wite stroush the Bounded i\n",
      "\n",
      "iter = 98000, smooth loss=1.6141305669582038\n",
      "psess you'll,\"\n",
      "Disioss, and repeoise quill youn of O press,\" said Hermione was act th. Hersions, pare. \"Disse of his haverly, marred Mrsty she's to heres crowing bohe?  hat aroum Gon's hand umpotting \n",
      "\n",
      "iter = 99000, smooth loss=1.6412879893865744\n",
      "oor her out of ropesting say,\" said Malfoy and puek of attreming out of the Creagrout was onated into Krus all shack now, his of. . . side inkory the off even, the sheld as juppit, looked on have-exco\n",
      "\n",
      "iter = 100000, smooth loss=1.656190348746499\n",
      "hick to Ron.\n",
      "\"Tole really.  \"The walknows.  \"You?\"  said it you ssuder taying the enter, nowe in!\" said at thoughted they's jead,\" said George wall reckat of ondo come a plaped Bert. \"It's excoparonn.\n",
      "\n",
      "iter = 101000, smooth loss=1.6574606228080544\n",
      "el,\" save.\n",
      "\"But Harry, were oncuaks, starce laig that fear, idsent on onting.\n",
      "\"Tatted looking ond of a mement.\n",
      "\"Mondong, Moody.  \"Who danding hom tefursn furries.\n",
      "\"I piccames.\n",
      "\"I glink a shorlch of th\n",
      "\n",
      "iter = 102000, smooth loss=1.6176044226976263\n",
      "arry jonger awank as something mole,  houry of Neville, you would Nevillats -- deckledoushagiculy, and Geeors and saw yod out of the bots - twell becaums Lood opprowing, in your sacking whissing.\n",
      "\"Abl\n",
      "\n",
      "iter = 103000, smooth loss=1.6567121408626402\n",
      "ory ros tourars, could asgeriously cabes widnous and not that abtilive flanged.  Dlagney, and Hermiont forges.  He then stopthicorsh A pleaming mossened to Ron, poined him for Mcanters for a loze dave\n",
      "\n",
      "iter = 104000, smooth loss=1.6065145202900615\n",
      "d to last than a norshed fins, the Trrcons oghed his will be it, nuriel, cheave gisn had sutmanving one to seft to see in the shomboss.  The good the Gobled oncint.  Dudll Sagst as he was accoble buer\n",
      "\n",
      "iter = 105000, smooth loss=1.566583767558795\n",
      "igamely alone and dectlouds and rese, whice at the chanfelined.  Bece, Buttly netory. .  And Harry!  Harry frompy tho!\" said said awized. \"It not happening his ent; porgled, boin about Harry, I'd not \n",
      "\n",
      "iter = 106000, smooth loss=1.561135947375669\n",
      "ff, do.  He gall blats in excorch. \"Ohe's fore -\"\n",
      "\"You know ence peacc on that not aloutcert--Siminned into the Snape as he greal got anyesting quenter of the tone into the Great Wil ound's not you gr\n",
      "\n",
      "iter = 107000, smooth loss=1.589868247871209\n",
      "ing?\"\n",
      "Harry remaed kits. .\"\n",
      "Harry armanich cyudgely.  I just insse hirk, so, we hours fingernate Rimame Rige and\n",
      "Harry's watchuding one.  Thent in by it wasn't he pable experely up you, he fully loost\n",
      "\n",
      "iter = 108000, smooth loss=1.5901657910672076\n",
      "ak Harry, ease Hogward to had it wasse times.  Ssame Fred ever doing to the Go-nothing. . . she would it Quiddnight.\"\n",
      "Harry matered. . . but the mat litters -\"\n",
      "\"Moody cously. . . drite Ind chetticle s\n",
      "\n",
      "iter = 109000, smooth loss=1.581118403965661\n",
      "ast him apribher beat the light gle finworgory gold fine to same be's was he greating the postle pomenty making died we was that alaging when heh with yeshe fell of the mines, as the pall.  Everyth Ha\n",
      "\n",
      "iter = 110000, smooth loss=1.5707587705822552\n",
      "ave that quieffice coming said breasy heard, Harry nit he lours, Humb eaming theite I trieacing hat had have just him conjure also, some strang we had benandow, and freed head - stonc, with hever spos\n",
      "\n",
      "iter = 111000, smooth loss=1.5901117234269233\n",
      "ff tells welchical Shrout swads breal yor. Harry her lent:ing mibsing one lint than had feelt of the remiching bur shing in there, Harry.  \"Bugh Deagays carenly mone fired each.  \"Though Genncestly.  \n",
      "\n",
      "iter = 112000, smooth loss=1.5754666816810272\n",
      "appy he coupigatelest forlone, she partcussed and fusponets drysfeernudering word up you don't it. . . . Though one of Moody's.\n",
      "Antary puecksent as a veryle aboar and stundertay.  Gettert, brittanger \n",
      "\n",
      "iter = 113000, smooth loss=1.5955131725417742\n",
      "foing instions that he punly - but they gifurly Woudd up the compectly do wild and sput moowly dragon--\"\n",
      "Be eyeworgh, who still.  \"Well. you knew as her ways want in his Fast put suddelway.\n",
      "Hermione m\n",
      "\n",
      "iter = 114000, smooth loss=1.6116070518950498\n",
      "e beaidn't watterts to with a whilig! n\"keyping to plop, who wall was Harry going!\"\n",
      "\"Wasking to were ince, the wen, and Ron we nimbying entrying him.  Hermigptr to with shorgilge.\n",
      "\"Did, the Gebletwhic\n",
      "\n",
      "iter = 115000, smooth loss=1.6245436689838983\n",
      "esenter students more mole, nasule in abeling, he has mysent stably at the when fliwr. Very was she stick.\n",
      "The cotsering crubbidasr them, Hagrid.\n",
      "\"She was begical, at he was circred his freamon a panu\n",
      "\n",
      "iter = 116000, smooth loss=1.5927826656294692\n",
      "e-of table, he had heve was said his boee are seck insently, \"She carefter aftle.  \"Is sfar you and of Hagrid back uablatembers had hears would him.  Ron sappous up.  Sacking; eath, you but caused any\n",
      "\n",
      "iter = 117000, smooth loss=1.6233336633110174\n",
      "ening throtring his clapen't Harry scovernize.  \"Sorgoing around sneeted furised!  Peoves frozing. \"MirtCroom the silves.  \"Herwilising amam, that Snapenssele. . . . . I come off?\"  said Flecody - inc\n",
      "\n",
      "iter = 118000, smooth loss=1.6084632897727957\n",
      "was good as Looked oveg arounded and have omewing an,\" said Ret than As Hogwarts, and You I'x into he it on my find his made Mig, Ront up to you You've and look he closer, and Karker on a mast that is\n",
      "\n",
      "iter = 119000, smooth loss=1.5778387670397274\n",
      " wabout then askingerly of surcome any cask plating now was there were airiate sure to that Weyt onit,\n",
      "\"ken was that their, he herce of DarKF .. Flister swimming shouted and the (turoast of the beway,\n",
      "\n",
      "iter = 120000, smooth loss=1.5935998548510564\n",
      " dupping toinge who stile-happithton his teal for of lauch o-gimins up and babels. \"The foot ween walking bag stoads oltime, I klims they cirsion.\n",
      "Harry was amish caping about the more went in themp b\n",
      "\n",
      "iter = 121000, smooth loss=1.5549555212743376\n",
      "ted to noirs shice.  Who' , nim,\" said Hermione?\" said Hermione outsiathansfors.\"\n",
      "\"gave behing was the wal.\n",
      "\"You've look had binly swaded fire with Skend surf a he ddont tryte ne uncing troved againgl\n",
      "\n",
      "iter = 122000, smooth loss=1.5812622469285085\n",
      "y of twe past lunth ust to kigutay, when at Harrying bean by standizard I hand.  Wore . . . if thought.\n",
      "\n",
      "It's and Hermione wele of viel, queperbed.\n",
      "\"Vermeal have the entort Harry, bit for have, \"I dar\n",
      "\n",
      "Best model at iteration: 122736 | smooth loss: 1.52351\n",
      "Best model at iteration: 122737 | smooth loss: 1.52332\n",
      "Best model at iteration: 122738 | smooth loss: 1.52281\n",
      "Best model at iteration: 122749 | smooth loss: 1.52262\n",
      "Best model at iteration: 122833 | smooth loss: 1.52256\n",
      "Best model at iteration: 122834 | smooth loss: 1.52217\n",
      "Best model at iteration: 122835 | smooth loss: 1.52184\n",
      "Best model at iteration: 122836 | smooth loss: 1.52122\n",
      "Best model at iteration: 122837 | smooth loss: 1.52092\n",
      "Best model at iteration: 122838 | smooth loss: 1.52077\n",
      "Best model at iteration: 122839 | smooth loss: 1.52018\n",
      "Best model at iteration: 122840 | smooth loss: 1.51963\n",
      "Best model at iteration: 122841 | smooth loss: 1.51943\n",
      "Best model at iteration: 122843 | smooth loss: 1.51907\n",
      "Best model at iteration: 122844 | smooth loss: 1.51892\n",
      "Best model at iteration: 122845 | smooth loss: 1.51859\n",
      "Best model at iteration: 122846 | smooth loss: 1.51805\n",
      "Best model at iteration: 122847 | smooth loss: 1.51758\n",
      "Best model at iteration: 122848 | smooth loss: 1.51756\n",
      "Best model at iteration: 122849 | smooth loss: 1.51727\n",
      "Best model at iteration: 122855 | smooth loss: 1.51723\n",
      "Best model at iteration: 122859 | smooth loss: 1.51696\n",
      "Best model at iteration: 122860 | smooth loss: 1.51663\n",
      "Best model at iteration: 122866 | smooth loss: 1.51646\n",
      "Best model at iteration: 122870 | smooth loss: 1.51636\n",
      "Best model at iteration: 122871 | smooth loss: 1.51616\n",
      "Best model at iteration: 122872 | smooth loss: 1.51572\n",
      "Best model at iteration: 122873 | smooth loss: 1.51529\n",
      "Best model at iteration: 122875 | smooth loss: 1.51472\n",
      "Best model at iteration: 122877 | smooth loss: 1.51467\n",
      "Best model at iteration: 122880 | smooth loss: 1.51457\n",
      "Best model at iteration: 122881 | smooth loss: 1.51448\n",
      "Best model at iteration: 122898 | smooth loss: 1.51399\n",
      "Best model at iteration: 122900 | smooth loss: 1.51355\n",
      "Best model at iteration: 122902 | smooth loss: 1.51355\n",
      "Best model at iteration: 122955 | smooth loss: 1.51300\n",
      "Best model at iteration: 122956 | smooth loss: 1.51298\n",
      "Best model at iteration: 122957 | smooth loss: 1.51292\n",
      "Best model at iteration: 122972 | smooth loss: 1.51253\n",
      "Best model at iteration: 122974 | smooth loss: 1.51236\n",
      "Best model at iteration: 122975 | smooth loss: 1.51205\n",
      "Best model at iteration: 122978 | smooth loss: 1.51205\n",
      "Best model at iteration: 122989 | smooth loss: 1.51188\n",
      "Best model at iteration: 122995 | smooth loss: 1.51125\n",
      "Best model at iteration: 122996 | smooth loss: 1.51125\n",
      "Best model at iteration: 122997 | smooth loss: 1.51115\n",
      "iter = 123000, smooth loss=1.5116208379133935\n",
      "oul.  How Ron's undered.\"\n",
      "\"Unopy.\n",
      "\"Yea.  They're all saugert. Harry.\"\n",
      "\"Jumbed.\"\" said Harry, Potter!\"\n",
      "\"Nows the lets, looking.\"\n",
      "Coormer?\"\n",
      "\"Lied, Ron?\n",
      "\"CHe look moomed tike the enecaused him, out his r\n",
      "\n",
      "Best model at iteration: 123005 | smooth loss: 1.51097\n",
      "Best model at iteration: 123024 | smooth loss: 1.51066\n",
      "Best model at iteration: 123026 | smooth loss: 1.51060\n",
      "Best model at iteration: 123028 | smooth loss: 1.51037\n",
      "Best model at iteration: 123029 | smooth loss: 1.50995\n",
      "Best model at iteration: 123030 | smooth loss: 1.50938\n",
      "Best model at iteration: 123031 | smooth loss: 1.50931\n",
      "Best model at iteration: 123174 | smooth loss: 1.50915\n",
      "Best model at iteration: 123175 | smooth loss: 1.50890\n",
      "Best model at iteration: 123177 | smooth loss: 1.50890\n",
      "Best model at iteration: 123178 | smooth loss: 1.50879\n",
      "Best model at iteration: 123179 | smooth loss: 1.50875\n",
      "Best model at iteration: 123180 | smooth loss: 1.50847\n",
      "Best model at iteration: 123182 | smooth loss: 1.50827\n",
      "iter = 124000, smooth loss=1.5264437403554743\n",
      "dory look other, and quite, jucar, sword who mage that a placts of foreetely raine of the choited to looking one other in place chan looked been the clourt of the windown all oncing him -row - wouldn'\n",
      "\n",
      "iter = 125000, smooth loss=1.5214046836128634\n",
      "ith Dumblouth him there walk?  I mising nothed as Chough gleaside mast.\n",
      "The-keptnce you knatche Mr. Crouch, door Mup cluarban for it what you plames of eaches?\"  said Dums of He.\"\n",
      "\"Quit,\" he said stil\n",
      "\n",
      "Best model at iteration: 125127 | smooth loss: 1.50819\n",
      "Best model at iteration: 125128 | smooth loss: 1.50759\n",
      "Best model at iteration: 125138 | smooth loss: 1.50736\n",
      "Best model at iteration: 125139 | smooth loss: 1.50659\n",
      "Best model at iteration: 125140 | smooth loss: 1.50653\n",
      "Best model at iteration: 125141 | smooth loss: 1.50625\n",
      "Best model at iteration: 125143 | smooth loss: 1.50621\n",
      "Best model at iteration: 125144 | smooth loss: 1.50576\n",
      "Best model at iteration: 125145 | smooth loss: 1.50541\n",
      "Best model at iteration: 125146 | smooth loss: 1.50503\n",
      "Best model at iteration: 125147 | smooth loss: 1.50497\n",
      "Best model at iteration: 125153 | smooth loss: 1.50496\n",
      "Best model at iteration: 125155 | smooth loss: 1.50473\n",
      "Best model at iteration: 125179 | smooth loss: 1.50462\n",
      "Best model at iteration: 125180 | smooth loss: 1.50446\n",
      "Best model at iteration: 125181 | smooth loss: 1.50414\n",
      "Best model at iteration: 125182 | smooth loss: 1.50383\n",
      "iter = 126000, smooth loss=1.5370012375611173\n",
      "estrely menting Hermione quite wirl, he wave pass the Milid nightout her. Buttan she's ald fey bus Weat haxing like chat,\" Harry livimal,\" said Mrsncely pother me and Mrs. Weasley, and I makne.\"\n",
      "Le se\n",
      "\n",
      "iter = 127000, smooth loss=1.541542528967339\n",
      "t the led the hardled.  Harry sand, trace' then in looking and who well.\n",
      "Harry get and the thad the pandoll walked out prepicies tuffeled cuptes.\n",
      "\n",
      "Co-Codrioky leg mames, and the turney?\"\n",
      "Himwardstiens\n",
      "\n",
      "iter = 128000, smooth loss=1.567940774800745\n",
      "well kemesking -\"\n",
      "\"I majher.\n",
      "He peace ... I lowart.  I couldn't of atercies. ... You seepless lit ferire other Tucius it chould he carrie, I weiten at and that I walked bereated in the in the might th\n",
      "\n",
      "iter = 129000, smooth loss=1.5138669473102495\n",
      " himself, from the great will sucurn mo ers, that looked them him around strabofe, scrows hes up his haid bet - reave the foll, silenty the Boum anterry - Vild hig the elmout him. \"So that he werk dar\n",
      "\n",
      "Best model at iteration: 129247 | smooth loss: 1.50378\n",
      "Best model at iteration: 129248 | smooth loss: 1.50340\n",
      "Best model at iteration: 129270 | smooth loss: 1.50302\n",
      "Best model at iteration: 129271 | smooth loss: 1.50259\n",
      "Best model at iteration: 129272 | smooth loss: 1.50243\n",
      "Best model at iteration: 129274 | smooth loss: 1.50226\n",
      "Best model at iteration: 129276 | smooth loss: 1.50191\n",
      "Best model at iteration: 129277 | smooth loss: 1.50155\n",
      "Best model at iteration: 129279 | smooth loss: 1.50149\n",
      "Best model at iteration: 129280 | smooth loss: 1.50132\n",
      "Best model at iteration: 129281 | smooth loss: 1.50101\n",
      "Best model at iteration: 129282 | smooth loss: 1.50082\n",
      "Best model at iteration: 129283 | smooth loss: 1.50002\n",
      "Best model at iteration: 129287 | smooth loss: 1.49967\n",
      "Best model at iteration: 129288 | smooth loss: 1.49922\n",
      "Best model at iteration: 129291 | smooth loss: 1.49896\n",
      "iter = 130000, smooth loss=1.5187418429038118\n",
      "ty dye gract swip do lyol.  We's never in Wimperred me to ... fries, eal's was a good, his grink, for voe.  Wey saw the wanr, peaving to say Wormmor my got a Ludned tren We kitter vicise to tasked the\n",
      "\n",
      "Best model at iteration: 130292 | smooth loss: 1.49883\n",
      "Best model at iteration: 130294 | smooth loss: 1.49867\n",
      "Best model at iteration: 130295 | smooth loss: 1.49841\n",
      "Best model at iteration: 130296 | smooth loss: 1.49792\n",
      "Best model at iteration: 130297 | smooth loss: 1.49755\n",
      "Best model at iteration: 130298 | smooth loss: 1.49735\n",
      "Best model at iteration: 130299 | smooth loss: 1.49695\n",
      "Best model at iteration: 130300 | smooth loss: 1.49653\n",
      "Best model at iteration: 130301 | smooth loss: 1.49606\n",
      "Best model at iteration: 130302 | smooth loss: 1.49548\n",
      "Best model at iteration: 130303 | smooth loss: 1.49498\n",
      "Best model at iteration: 130305 | smooth loss: 1.49495\n",
      "Best model at iteration: 130306 | smooth loss: 1.49441\n",
      "Best model at iteration: 130307 | smooth loss: 1.49406\n",
      "Best model at iteration: 130308 | smooth loss: 1.49351\n",
      "Best model at iteration: 130309 | smooth loss: 1.49295\n",
      "Best model at iteration: 130310 | smooth loss: 1.49256\n",
      "Best model at iteration: 130311 | smooth loss: 1.49233\n",
      "Best model at iteration: 130312 | smooth loss: 1.49198\n",
      "Best model at iteration: 130329 | smooth loss: 1.49182\n",
      "Best model at iteration: 130453 | smooth loss: 1.49160\n",
      "Best model at iteration: 130454 | smooth loss: 1.49128\n",
      "Best model at iteration: 130568 | smooth loss: 1.49105\n",
      "Best model at iteration: 130569 | smooth loss: 1.49060\n",
      "Best model at iteration: 130571 | smooth loss: 1.49037\n",
      "Best model at iteration: 130572 | smooth loss: 1.49030\n",
      "Best model at iteration: 130575 | smooth loss: 1.49021\n",
      "Best model at iteration: 130578 | smooth loss: 1.49004\n",
      "Best model at iteration: 130585 | smooth loss: 1.48999\n",
      "Best model at iteration: 130586 | smooth loss: 1.48988\n",
      "Best model at iteration: 130588 | smooth loss: 1.48987\n",
      "Best model at iteration: 130589 | smooth loss: 1.48965\n",
      "Best model at iteration: 130591 | smooth loss: 1.48895\n",
      "Best model at iteration: 130592 | smooth loss: 1.48874\n",
      "Best model at iteration: 130593 | smooth loss: 1.48866\n",
      "Best model at iteration: 130594 | smooth loss: 1.48852\n",
      "Best model at iteration: 130596 | smooth loss: 1.48836\n",
      "Best model at iteration: 130605 | smooth loss: 1.48818\n",
      "Best model at iteration: 130606 | smooth loss: 1.48782\n",
      "Best model at iteration: 130607 | smooth loss: 1.48739\n",
      "Best model at iteration: 130608 | smooth loss: 1.48682\n",
      "Best model at iteration: 130610 | smooth loss: 1.48676\n",
      "Best model at iteration: 130611 | smooth loss: 1.48668\n",
      "Best model at iteration: 130612 | smooth loss: 1.48631\n",
      "Best model at iteration: 130613 | smooth loss: 1.48592\n",
      "Best model at iteration: 130617 | smooth loss: 1.48573\n",
      "Best model at iteration: 130618 | smooth loss: 1.48519\n",
      "Best model at iteration: 130619 | smooth loss: 1.48476\n",
      "Best model at iteration: 130626 | smooth loss: 1.48449\n",
      "Best model at iteration: 130627 | smooth loss: 1.48389\n",
      "Best model at iteration: 130628 | smooth loss: 1.48356\n",
      "Best model at iteration: 130629 | smooth loss: 1.48298\n",
      "Best model at iteration: 130734 | smooth loss: 1.48274\n",
      "Best model at iteration: 130735 | smooth loss: 1.48240\n",
      "Best model at iteration: 130737 | smooth loss: 1.48228\n",
      "Best model at iteration: 130791 | smooth loss: 1.48193\n",
      "Best model at iteration: 130792 | smooth loss: 1.48188\n",
      "Best model at iteration: 130797 | smooth loss: 1.48171\n",
      "Best model at iteration: 130801 | smooth loss: 1.48144\n",
      "Best model at iteration: 130809 | smooth loss: 1.48105\n",
      "Best model at iteration: 130845 | smooth loss: 1.48101\n",
      "Best model at iteration: 130846 | smooth loss: 1.48002\n",
      "Best model at iteration: 130847 | smooth loss: 1.47977\n",
      "iter = 131000, smooth loss=1.4833456503354008\n",
      "ing upnaguble began desk.  \"Harry,\" said Fled a might?\"  said Dumblins tourdord stare he comenty standing proted of the vape officur, Helect what you they we gone you all ound upueferp can resired a g\n",
      "\n",
      "iter = 132000, smooth loss=1.5104732800042568\n",
      "ued.  Tes a sen't a lere to tur us an it's in a lit was feet!\"  said Doite, rown, be you conter us.  \"I'm no it onle about skill neatersa that Hag's to mise at a neor rown, I that nexkwars to surned. \n",
      "\n",
      "Completed epoch 3\n",
      "Epoch: 4\n",
      "iter = 133000, smooth loss=1.554351952511277\n",
      "d took to recudgeles, but on edridid of them,\" he thriend, when they din's back to the enor frrew didous on tile, very voice?  Criptly asmang were darrans sably - path his thing Madorone and stray ciu\n",
      "\n",
      "iter = 134000, smooth loss=1.568104118669688\n",
      "said forcumer to compuscely; the entiressive me.  The deton, the losed a floon, cut before of a fighmped, and stroanting that in a borring him something his sporming, you a sectsiust speate onnessit t\n",
      "\n",
      "iter = 135000, smooth loss=1.580356611859486\n",
      "rs words and turn Untlain.  He he hints reave their still by it.  He was duds.  Dee he cool of the last'sle he poonly, saicch.  Sirius wenared, which's got retufary something that I'll tchise stipp - \n",
      "\n",
      "iter = 136000, smooth loss=1.5644231116834661\n",
      "y,?\" said Harry!  - you,\"\n",
      "But on a mamin.\n",
      "Belloy encrelelf, all.\n",
      "\"Oo soo gaccleh.\n",
      "Whrownow of slally slowble.\n",
      "Ugribrss mystiburady him  he ovor.  Think than - it swoy a lays.\n",
      "Wort was this whathdiov U\n",
      "\n",
      "iter = 137000, smooth loss=1.5870806934837198\n",
      "At other the worry of that's an away spent her howle - Lot on all.\n",
      "\"Whospen foo sickull dower the twent-reftes?\" said Geargmin, pothed thecius of at Muggan Cursley. \"Mr. Crouch three soots his, a coul\n",
      "\n",
      "iter = 138000, smooth loss=1.6122386361365038\n",
      "der down a pacement.\n",
      "\"Krum a long his saying.  She happst of otciated ffing. . . . seem.  He was asnews, now.  He safple go, unwire mamides, more in an the was stuling pock disserpsiteled of the Fitch\n",
      "\n",
      "iter = 139000, smooth loss=1.6335286579813546\n",
      "evend mige with me. The edgemery trressed bot tacks; hin ovhorther.\n",
      "Ah, dran as they hor, somecroff tell holding.\"\n",
      "\"Harry, Harry tow, turned three serterts.\n",
      "This spacking on he' have ound to and Mr. T\n",
      "\n",
      "iter = 140000, smooth loss=1.645415540535323\n",
      "ed gup, Azgoll.  Box ona that were a ntain, the terst of shole with everyU Grotsous heard back fave into the ben bella into the Back Bolganly neck.  \"If - you his tlows as thec to sept trocy bellith t\n",
      "\n",
      "iter = 141000, smooth loss=1.577498159921856\n",
      "around do back was to,\" said Mr. Diggory wizard.\n",
      "\"She seill whispered face with while he get as no a shalf, realing far, and herpled brignation, all from she have wand teece herserged.\n",
      "\"A robhy windul\n",
      "\n",
      "iter = 142000, smooth loss=1.5645387617274127\n",
      "bous now,\" he said.  \"How, fur of them. \"There recending you're tcrished, for a gianqupiace anow for the want plan tekinted a forged goll pude Harry lutchent Harly playon oud think can there drever he\n",
      "\n",
      "iter = 143000, smooth loss=1.5889194499589268\n",
      " for raid puintiam.\n",
      "Ender some was a cang to assues up and prelves is stratther sut your dreatime in out think overy.\n",
      "\"We stalled rean be knew Gooke loigh my were cort of held toiled; he sautered geck\n",
      "\n",
      "iter = 144000, smooth loss=1.638725570428327\n",
      "asing, housenfate on the beet than.  His seaving out from the knass studelformatigalled other was out witced trase like sude deen by a dive and starnanused, the Fattor\n",
      "Harry sellles, eachelsized at th\n",
      "\n",
      "iter = 145000, smooth loss=1.6423298297370192\n",
      "it had had enjurned miveler close, affest. Man, and they jown Harry, been his riAy to hem, and her new. . . . .\"\n",
      "\"go on Owe sittion, and then yet not seemsn the cands of might out arauxed faves, Run.\n",
      "\n",
      "\n",
      "iter = 146000, smooth loss=1.5920512972154701\n",
      "cturaise.  You?  How,\" Moody.\n",
      "\"Doo.  It long his Kaugay,\" Harrion that some darrely.\n",
      "\"Gays hearssing the wantelen, clearing on the Hermiod he cut a plasts, had at of Krowll gecing want of a year, that\n",
      "\n",
      "iter = 147000, smooth loss=1.6241251932456224\n",
      "fed.  The sument handreds encecthing every starin the danked the solled.  PRof stattifnced at a suld up tole reacher coulder age haggwardly. . .\n",
      "Ron, and cloudly, studertting knombaicit he seemed to s\n",
      "\n",
      "iter = 148000, smooth loss=1.589942359034884\n",
      "alortout the gird terr eat blood.  \"Yes their table anouse. \"Fon's head to be hereplow in were said.  He lanter, furzly.  \"The barnced the before said  Oun side stiely at Hagrid, the madely have reme \n",
      "\n",
      "iter = 149000, smooth loss=1.5490067511693404\n",
      "tly, and mucckaty-side and awry of tuped otheying herselvose fook lasteltweles flit here - we'res away in the cointure, whathered be ie the ceciose the thees ort inside in to and to schoweaming feetin\n",
      "\n",
      "iter = 150000, smooth loss=1.5363854245938173\n",
      "inger tever Dumbledore's smone onlowhed Fred dinge he mutting he wanted him anow. . . how Agnering greated to chadroms, all be the exizer towardly lifely.  We gobary tore have was not his practes anto\n",
      "\n",
      "iter = 151000, smooth loss=1.5452441049757362\n",
      " load one.....\n",
      "\"Hight be set front'bl a roonlap pate ghasticed there with kighy glinge.. .\"\n",
      "Vekted hixh I warkinghed teatilly at Cupsed.\n",
      "\"Peaped reacemens. . . . He panened to troetly terticed Snoulde\n",
      "\n",
      "iter = 152000, smooth loss=1.5654154538361904\n",
      " that Dumblear hore furse.  I 'angned Ro.  \"And dnoletly.  \"Yea Retilly.  \"I wizard to got in long himb. . . . .\n",
      " \"A ey want cretting fret he happone, aft have like Harry sind tanded his varks, Ron he\n",
      "\n",
      "iter = 153000, smooth loss=1.54251561736288\n",
      "hind what was not that's air in the on his long house a book.  Betuady after.  Lhe was all right in Hogwards a tenagy what betrought?\"  - Prepkent stop!\"\n",
      "Krum watched.\n",
      "\"Gublit.\n",
      "\"\n",
      "Oy Dudont all that lo\n",
      "\n",
      "iter = 154000, smooth loss=1.5404185792036957\n",
      "suppowly said, and up on it, prosered, \"Moans - wand uaks to the first I'd and Harry frights ins light oution Was he know Harry Harry look as he jumper Hermione and the pather but noise the baggro, I'\n",
      "\n",
      "iter = 155000, smooth loss=1.5339128917658345\n",
      "Chus he had las!\"  said that be good liking fisting his looked cheessorned the doanter, he knew Lut them, and tell, them jusn who and they'?  \"Yyus I'm ghees.\"\n",
      "Se suddembers evenie sort of I't you aro\n",
      "\n",
      "iter = 156000, smooth loss=1.5426857005004446\n",
      "t the dranding kist onter,\" said Ron crowing ever thrisy ervirg to get ocks, of ghappive!\n",
      "\t\t\"Hermewhely is the Wandeds are voing me. Doonging is around it bad whatliciug thatts and owally weld toury h\n",
      "\n",
      "iter = 157000, smooth loss=1.5455388444908411\n",
      ".\"  beet of Hoully some-not painied up aroun bots to almoring to the reate!\"\n",
      "\"I'm no.  \"But woye leagh.\n",
      "\"Oh thone is me stootion Ron, you hany raugally.\n",
      "\"Not so juncely.\n",
      "\"What! Nor; staried tented ear\n",
      "\n",
      "iter = 158000, smooth loss=1.5749913980658188\n",
      "n't going forwater.  I've at he was fight was claconer.\n",
      "\"Thay is it many.\n",
      "\"As courre though Professor.\n",
      "\"I with this grond the came ouble around not it last estnors canding or a vinuticallar thing, the\n",
      "\n",
      "iter = 159000, smooth loss=1.5755784001587245\n",
      "?\"  said Ron?  Ron someho with thowe', were with Siruivase liking him.  Ron, whink, his each istant, looked russ but they he louded up the onel.  \"Oose ne on her to keematy making hut that the partles\n",
      "\n",
      "iter = 160000, smooth loss=1.5660177014075551\n",
      "dinner speer jow\"\n",
      "Professor things in? Sharktaff, but a look if yes, strabock, for cave up anything pass, laying my remely perion with a waytllt of stuped on he were . . . . Comerepened to he had the \n",
      "\n",
      "iter = 161000, smooth loss=1.5992545048621778\n",
      "to soing, Moboin to we oll the rest Seeationly.\"\n",
      "Wor vermaily.\n",
      "And to tyen.\"\n",
      "\"Well and and the roves. . . . .... of-visots on tackid, he hoop was mover the wall from undbolleg to the good, wantly who \n",
      "\n",
      "iter = 162000, smooth loss=1.5570212589130545\n",
      "ob Dark in y under he can to go was an\n",
      "Harry.\n",
      "Sidring me; Harry had go Ton; as Moody lats.  Maght turn mish of wantly.  Harry tairs, whether.  \"Momaslable was admant's flywher Hermione, of them more, \n",
      "\n",
      "iter = 163000, smooth loss=1.576652100883494\n",
      "ve yer.\n",
      "\"We're very bact around.\"\n",
      "Thanks come ust a plees.  \"Den.  She shaked as through the Cermarear of too spares. Where staremer poosbally; his voice his strilso strangs was becares a Donter a kne\n",
      "\n",
      "iter = 164000, smooth loss=1.578367871690137\n",
      "at oves from a fey was creech caur its on het time his provertione with the dorry up to sonting to pitching powle over the rabegeed had uppeg it into timay these clust preathing under, we'd foutining \n",
      "\n",
      "iter = 165000, smooth loss=1.538323280253025\n",
      "ables.  Parky frown they were nepposty. \"Evers. Dupty prite to wold fatcels, Ron, shefical's uaked in think's pace.  you knesked the engived to smill heappeys -\"\n",
      "\"Next been Dest yexter.  It more him. \n",
      "\n",
      "iter = 166000, smooth loss=1.5582271812845454\n",
      " he Potter hervertch?\" Ron could judning.  \"I's and please was the table askabout, he'd turnuts had have back other but me exvedoring begpacemors ex-lived to had find Hishy. Herm she happing on Harry \n",
      "\n",
      "iter = 167000, smooth loss=1.5011631473566382\n",
      "and Ron in that that he was said,, tile. . .\" he syene fad take,\" he was prestridly darly to still,\" said Harry, Wordemind to him; theyel.  \"He suitt his fatts,\" said Harry, slooked.\n",
      "\"He said and Herm\n",
      "\n",
      "iter = 168000, smooth loss=1.5145567116150866\n",
      "increst himseots of the corring to go trido, thad it, he panterily goined Dumbledore's press you celongs sure jusabing in,\" he did not up room.\n",
      "\"Harry, it's dissed had pllarged his night workinf, we a\n",
      "\n",
      "iter = 169000, smooth loss=1.5056747167944065\n",
      "n't say, sind back.  I have behind his owade ogeation, mind, and getmots, forriund their though Harry will haw the saymoss said, he tafins to a looking forest quiaking the meauchaird, slin fretmory sa\n",
      "\n",
      "Best model at iteration: 169442 | smooth loss: 1.47968\n",
      "Best model at iteration: 169445 | smooth loss: 1.47928\n",
      "Best model at iteration: 169446 | smooth loss: 1.47885\n",
      "Best model at iteration: 169447 | smooth loss: 1.47840\n",
      "Best model at iteration: 169448 | smooth loss: 1.47820\n",
      "Best model at iteration: 169456 | smooth loss: 1.47806\n",
      "Best model at iteration: 169479 | smooth loss: 1.47803\n",
      "Best model at iteration: 169480 | smooth loss: 1.47772\n",
      "Best model at iteration: 169481 | smooth loss: 1.47755\n",
      "Best model at iteration: 169482 | smooth loss: 1.47728\n",
      "Best model at iteration: 169483 | smooth loss: 1.47700\n",
      "iter = 170000, smooth loss=1.5222790926060226\n",
      "of mintaus me up makalk abance Dumbledore.  \"It's champer, then eving vicius again angle who said - the bruats.\n",
      "\"Dod to mageing midd the sten?\"  he trach is of the Manal Smmen been that Ron gial your \n",
      "\n",
      "iter = 171000, smooth loss=1.53385590018469\n",
      "saw was night-Next ordding seen thind UPHours?\" Wish to endark on a hings.  Harry once you. . . . . . a now?\"\n",
      "\"Misting - Mrs. Gonger - and was hind her, he red fort his diss!\"\n",
      "Exints I'd manch. Her Ha\n",
      "\n",
      "iter = 172000, smooth loss=1.5303865110392585\n",
      ".  Mr; paiting to the gated -Nave mefort on Dimbbly your forsor a broom, all the wand of the pointed forget, \"Mad I supbes pain drack or whizard.  You reterly, Vollessed to get is.  Your they . . . sh\n",
      "\n",
      "iter = 173000, smooth loss=1.50809254419796\n",
      "ow, bered to Bokany felt Kark,\" He at with insuch munse charted to sve them harden to it was sobbongs than Neviluthting atout, anlest seepling the rothom whet to dis wands with fulld as he bead what w\n",
      "\n",
      "Best model at iteration: 173571 | smooth loss: 1.47687\n",
      "Best model at iteration: 173572 | smooth loss: 1.47644\n",
      "Best model at iteration: 173573 | smooth loss: 1.47628\n",
      "Best model at iteration: 173575 | smooth loss: 1.47615\n",
      "Best model at iteration: 173577 | smooth loss: 1.47589\n",
      "Best model at iteration: 173578 | smooth loss: 1.47549\n",
      "Best model at iteration: 173580 | smooth loss: 1.47546\n",
      "Best model at iteration: 173581 | smooth loss: 1.47518\n",
      "Best model at iteration: 173582 | smooth loss: 1.47486\n",
      "Best model at iteration: 173583 | smooth loss: 1.47483\n",
      "Best model at iteration: 173584 | smooth loss: 1.47406\n",
      "Best model at iteration: 173588 | smooth loss: 1.47394\n",
      "Best model at iteration: 173589 | smooth loss: 1.47347\n",
      "Best model at iteration: 173592 | smooth loss: 1.47323\n",
      "iter = 174000, smooth loss=1.488590806745583\n",
      "comory liet, and tcos ap a Lutters - owy orded indo tiumst and the arms the maning aris, who, ins of the touth, you free mave of voing linse through the wand Mo:n life-en we help out knews with eys ha\n",
      "\n",
      "Best model at iteration: 174591 | smooth loss: 1.47323\n",
      "Best model at iteration: 174592 | smooth loss: 1.47305\n",
      "Best model at iteration: 174593 | smooth loss: 1.47266\n",
      "Best model at iteration: 174595 | smooth loss: 1.47248\n",
      "Best model at iteration: 174596 | smooth loss: 1.47218\n",
      "Best model at iteration: 174597 | smooth loss: 1.47173\n",
      "Best model at iteration: 174598 | smooth loss: 1.47134\n",
      "Best model at iteration: 174599 | smooth loss: 1.47111\n",
      "Best model at iteration: 174600 | smooth loss: 1.47064\n",
      "Best model at iteration: 174601 | smooth loss: 1.47019\n",
      "Best model at iteration: 174602 | smooth loss: 1.46979\n",
      "Best model at iteration: 174603 | smooth loss: 1.46916\n",
      "Best model at iteration: 174604 | smooth loss: 1.46865\n",
      "Best model at iteration: 174605 | smooth loss: 1.46862\n",
      "Best model at iteration: 174606 | smooth loss: 1.46841\n",
      "Best model at iteration: 174607 | smooth loss: 1.46791\n",
      "Best model at iteration: 174608 | smooth loss: 1.46757\n",
      "Best model at iteration: 174609 | smooth loss: 1.46708\n",
      "Best model at iteration: 174610 | smooth loss: 1.46656\n",
      "Best model at iteration: 174611 | smooth loss: 1.46614\n",
      "Best model at iteration: 174612 | smooth loss: 1.46588\n",
      "Best model at iteration: 174613 | smooth loss: 1.46551\n",
      "Best model at iteration: 174626 | smooth loss: 1.46550\n",
      "Best model at iteration: 174628 | smooth loss: 1.46522\n",
      "Best model at iteration: 174629 | smooth loss: 1.46515\n",
      "Best model at iteration: 174630 | smooth loss: 1.46497\n",
      "Best model at iteration: 174631 | smooth loss: 1.46497\n",
      "Best model at iteration: 174632 | smooth loss: 1.46497\n",
      "Best model at iteration: 174754 | smooth loss: 1.46440\n",
      "Best model at iteration: 174755 | smooth loss: 1.46406\n",
      "Best model at iteration: 174870 | smooth loss: 1.46369\n",
      "Best model at iteration: 174872 | smooth loss: 1.46347\n",
      "Best model at iteration: 174873 | smooth loss: 1.46343\n",
      "Best model at iteration: 174879 | smooth loss: 1.46326\n",
      "Best model at iteration: 174886 | smooth loss: 1.46309\n",
      "Best model at iteration: 174887 | smooth loss: 1.46293\n",
      "Best model at iteration: 174890 | smooth loss: 1.46269\n",
      "Best model at iteration: 174892 | smooth loss: 1.46185\n",
      "Best model at iteration: 174893 | smooth loss: 1.46169\n",
      "Best model at iteration: 174894 | smooth loss: 1.46154\n",
      "Best model at iteration: 174897 | smooth loss: 1.46140\n",
      "Best model at iteration: 174905 | smooth loss: 1.46107\n",
      "Best model at iteration: 174906 | smooth loss: 1.46080\n",
      "Best model at iteration: 174907 | smooth loss: 1.46042\n",
      "Best model at iteration: 174908 | smooth loss: 1.46007\n",
      "Best model at iteration: 174909 | smooth loss: 1.45942\n",
      "Best model at iteration: 174911 | smooth loss: 1.45930\n",
      "Best model at iteration: 174912 | smooth loss: 1.45923\n",
      "Best model at iteration: 174913 | smooth loss: 1.45895\n",
      "Best model at iteration: 174914 | smooth loss: 1.45854\n",
      "Best model at iteration: 174918 | smooth loss: 1.45831\n",
      "Best model at iteration: 174919 | smooth loss: 1.45776\n",
      "Best model at iteration: 174920 | smooth loss: 1.45739\n",
      "Best model at iteration: 174927 | smooth loss: 1.45704\n",
      "Best model at iteration: 174928 | smooth loss: 1.45646\n",
      "Best model at iteration: 174929 | smooth loss: 1.45614\n",
      "Best model at iteration: 174930 | smooth loss: 1.45557\n",
      "iter = 175000, smooth loss=1.4583138499052728\n",
      "lly what he had seep.  Voldemort sllegple of up as proping Artinnis feard Seame toward thef, rather telling focen hand another so had foot.  World put you dis this face, proused Harry.  \"g sant the de\n",
      "\n",
      "Best model at iteration: 175036 | smooth loss: 1.45535\n",
      "Best model at iteration: 175038 | smooth loss: 1.45512\n",
      "Best model at iteration: 175092 | smooth loss: 1.45477\n",
      "Best model at iteration: 175093 | smooth loss: 1.45477\n",
      "Best model at iteration: 175094 | smooth loss: 1.45467\n",
      "Best model at iteration: 175098 | smooth loss: 1.45427\n",
      "Best model at iteration: 175101 | smooth loss: 1.45425\n",
      "Best model at iteration: 175102 | smooth loss: 1.45384\n",
      "Best model at iteration: 175110 | smooth loss: 1.45358\n",
      "Best model at iteration: 175147 | smooth loss: 1.45269\n",
      "Best model at iteration: 175148 | smooth loss: 1.45246\n",
      "iter = 176000, smooth loss=1.4755822780676762\n",
      " \"I could Potter,\" said Dumbledore.  Himerous whon go you his fach aspreating for a vaked, the skum Snace bo!\"\n",
      "\"Yousht it; he trunk with angry!  I will the grout slouted azked.  Her work ask you,\" sur\n",
      "\n",
      "iter = 177000, smooth loss=1.497899007059457\n",
      "elial whother you crompering,. Where and I brien.  Cedric had say,\" said Dumbledore's found really.\n",
      "\"Who'd be acrest noor lottle had arres,\" said Snapfforned foud now, some... onlablet of touching to \n",
      "\n",
      "Completed epoch 4\n",
      "\n",
      "Best model achieved smooth loss: 1.45246 at iteration: 175148\n"
     ]
    }
   ],
   "source": [
    "eta = 0.001\n",
    "seq_length = 25\n",
    "m = 100\n",
    "K = len(unique_chars)\n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "network = initialize_network(m, K, rng)\n",
    "(\n",
    "    trained_network,\n",
    "    final_smooth_loss,\n",
    "    final_loss_history,\n",
    "    final_iter_history,\n",
    "    final_text_samples,\n",
    "    best_network,\n",
    "    best_smooth_loss,\n",
    ") = train_rnn(\n",
    "    book_data,\n",
    "    char_to_id,\n",
    "    id_to_char,\n",
    "    network,\n",
    "    rng,\n",
    "    eta=eta,\n",
    "    seq_length=seq_length,\n",
    "    num_epochs=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65bb6b76-b3e3-4ba7-9f8e-a7ba3e300a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Hersily Dumbledore told mad the membles, and Voldemy.  Hearvy arm cominnan to he will happeat I will, and with you him,  he blevany has this and can away..\\n\"You as the noved,\" though Dumbledore wanvy arming entriuted of the shoulder,\"  wall toun ristange, thought who ask oned to silent ourse begoll\\'s we nome bas queak of the possout thot Sirius.  Beh. He seer the doul?\" said Heavion\\'s were to the rese, heldem wifl arous wifl\\'s a saw.  He not ques.  Some downed.\"  wannd unabbe you his will meet turned. S it farm.\\n\"Mrsalay (Fimt dven\\'t every while them thought you knightly.\"\\nThe been Harry\\'s awayt offich, Bo.\"\\n\"Ithey.  Oh you know help of Dimblay behind whord tellin that whan ever Bell me betond each and a listen all rothing whan you hen spoke, Cedric into the helfing around, do said, he\\'s beinxy, antiede Jome,\" said Deminny agair, inains, Belix,\" sher his heard as though when as a Swelar into the flocks thas he was get him, very noisy very toldes,\" said Dumbledore-lest bus; a slowf quit'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_loss(final_loss_history, final_iter_history, \"smooth_loss_results\")\n",
    "print_text_history(final_text_samples)\n",
    "print_final_text(best_network, char_to_id, id_to_char, rng, 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
